**Tell us about your PDF experience.**

**semantic_kernel Package**

Reference

**Packages**

agents

connectors

contents

core_plugins

data

exceptions

filters

functions

memory

planners

processes

prompt_template

reliability

schema

services

template_engine

text

utils

**Modules**

ﾉ

**Expand table**

ﾉ

**Expand table**



const

kernel

kernel_pydantic

kernel_types

**Classes**

Kernel

The Kernel of Semantic Kernel.

This is the main entry point for Semantic Kernel. It provides the ability to
run functions

and manage filters, plugins, and AI services.

Initialize a new instance of the Kernel class.

ﾉ

**Expand table**



**agents Package**

Reference

**Packages**

bedrock

channels

chat_completion

group_chat

strategies

**Modules**

agent

**Classes**

Agent

Base abstraction for all Semantic Kernel agents.

An agent instance may participate in one or more conversations. A

conversation may include one or more agents. In addition to identity and

descriptive meta-data, an Agent must define its communication protocol,

or AgentChannel.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AgentChat

A base class chat interface for agents.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AgentGroupChat

An agent chat that supports multi-turn interactions.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize a new instance of AgentGroupChat.

ChatCompletionAgent

A Chat Completion Agent based on ChatCompletionClientBase.

Features marked with this status are nearing completion and are

considered stable for most purposes, but may still incur minor

refinements or optimizations before achieving full general availability.

(Version: 1.22.0-rc1)

Initialize a new instance of ChatCompletionAgent.



**azure_ai Package**

Reference

**Modules**

azure_ai_agent

ﾉ

**Expand table**



**azure_ai_agent Module**

Reference

**Classes**

AzureAIAgent

Azure AI Agent class. Provides the ability to interact with Azure AI
Assistants.

Initialize an AzureAIAgent service.

ﾉ

**Expand table**



**AzureAIAgent Class**

Reference

Azure AI Agent class. Provides the ability to interact with Azure AI
Assistants. Initialize an

AzureAIAgent service.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

The arguments to pass to the function.

`**client**`

Required*

The Azure AI Project client.

`**definition**`

Required*

The Azure AI Agent model.

`**kernel**`

The Kernel instance.

`**plugins**`

The plugins to add to the kernel. If both the plugins and the kernel

are supplied, the plugins take precedence and are added to the

kernel by default.

`**polling_options**`

The polling options.

`**prompt_template_config**`

The prompt template configuration.

`AzureAIAgent(*, arguments: KernelArguments | ``None`` = ``None``, client: `

`AIProjectClient, definition: AzureAIAgentModel, kernel: Kernel | ``None`` = `

`None``, plugins: list[KernelPlugin | object] | dict[str, KernelPlugin | `

`object] | ``None`` = ``None``, polling_options: RunPollingOptions | ``None`` = ``None``, `

`prompt_template_config: PromptTemplateConfig | ``None`` = ``None``, **kwargs: Any)`

ﾉ

**Expand table**

**Methods**



get_response

Get a response from the agent on a thread.

invoke

Invoke the agent.

invoke_stream

Invoke the agent in streaming mode.

**get_response**

Get a response from the agent on a thread.

Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The response from the agent.

**invoke**

Invoke the agent.

ﾉ

**Expand table**

`async`` get_response(thread_id: str, *, arguments: KernelArguments | ``None`` = `

`None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> ChatMessageContent`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The chat message content.

**invoke_stream**

Invoke the agent in streaming mode.

Python

**Parameters**

`async`` invoke(thread_id: str, *, arguments: KernelArguments | ``None`` = ``None``, `

`kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_stream(thread_id: str, *, arguments: KernelArguments | ``None`` `

`= ``None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[StreamingChatMessageContent]`

ﾉ

**Expand table**



**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

An async generator of StreamingChatMessageContent.

**client**

Python

**definition**

Python

**polling_options**

Python

ﾉ

**Expand table**

**Attributes**

`client: AIProjectClient`

`definition: AzureAIAgentModel`

`polling_options: RunPollingOptions`



**bedrock Package**

Reference

**Packages**

models

**Modules**

action_group_utils

bedrock_agent_settings

ﾉ

**Expand table**

ﾉ

**Expand table**



**models Package**

Reference

**Modules**

bedrock_action_group_model

bedrock_agent_event_type

bedrock_agent_model

bedrock_agent_status

ﾉ

**Expand table**



**bedrock_action_group_model Module**

Reference

**Classes**

BedrockActionGroupModel

Bedrock Action Group Model.

Model field definitions for the Amazon Bedrock Action Group Service:

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-

agent/client/create_agent_action_group.html

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BedrockActionGroupModel Class**

Reference

Bedrock Action Group Model.

Model field definitions for the Amazon Bedrock Action Group Service:

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock

-agent/client/create_agent_action_group.html

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**actionGroupId**`

Required*

`**actionGroupName**`

Required*

**action_group_id**

`BedrockActionGroupModel(*, actionGroupId: str, actionGroupName: str, `

`**extra_data: Any)`

ﾉ

**Expand table**

**Attributes**



Python

**action_group_name**

Python

**is_experimental**

Python

**stage_status**

Python

`action_group_id: str`

`action_group_name: str`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**bedrock_agent_event_type Module**

Reference

**Enums**

BedrockAgentEventType

Bedrock Agent Event Type.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**BedrockAgentEventType Enum**

Reference

Bedrock Agent Event Type.

Note: This class is marked as 'experimental' and may change in the future.

CHUNK

FILES

RETURN_CONTROL

TRACE

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**bedrock_agent_model Module**

Reference

**Classes**

BedrockAgentModel

Bedrock Agent Model.

Model field definitions for the Amazon Bedrock Agent Service:

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-

agent/client/create_agent.html

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BedrockAgentModel Class**

Reference

Bedrock Agent Model.

Model field definitions for the Amazon Bedrock Agent Service:

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock

-agent/client/create_agent.html

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**agentId**`

Required*

`**agentName**`

Required*

`**agentVersion**`

Required*

`**foundationModel**`

Required*

`BedrockAgentModel(*, agentId: str | ``None`` = ``None``, agentName: str | ``None`` = `

`None``, agentVersion: str | ``None`` = ``None``, foundationModel: str | ``None`` = ``None``, `

`agentStatus: str | ``None`` = ``None``, **extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

`**agentStatus**`

Required*

**agent_id**

Python

**agent_name**

Python

**agent_status**

Python

**agent_version**

Python

**foundation_model**

Python

**Attributes**

`agent_id: str | ``None`

`agent_name: str | ``None`

`agent_status: str | ``None`

`agent_version: str | ``None`

`foundation_model: str | ``None`



**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**bedrock_agent_status Module**

Reference

**Enums**

BedrockAgentStatus

Bedrock Agent Status.

https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PrepareAgent.html#API_agent_PrepareAgent_ResponseElements

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**BedrockAgentStatus Enum**

Reference

Bedrock Agent Status.

https://docs.aws.amazon.com/bedrock/latest/APIReference/API_agent_PrepareAgent.ht

ml#API_agent_PrepareAgent_ResponseElements

Note: This class is marked as 'experimental' and may change in the future.

CREATING

DELETING

FAILED

NOT_PREPARED

PREPARED

PREPARING

UPDATING

VERSIONING

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**action_group_utils Module**

Reference

**kernel_function_metadata_to_bedrock_function_schema**

Convert the kernel function metadata to bedrock function schema.

Python

**Parameters**

**Name**

**Description**

`**function_metadata**`

Required*

**kernel_function_parameter_to_bedrock_function_parameter**

Convert the kernel function parameters to bedrock function parameters.

Python

**Parameters**

**Name**

**Description**

`**parameter**`

Required*

**kernel_function_parameter_type_to_bedrock_function_parameter_ty**

Convert the kernel function parameter type to bedrock function parameter type.

**Functions**

`kernel_function_metadata_to_bedrock_function_schema(function_metadata: `

`KernelFunctionMetadata) -> dict[str, Any]`

ﾉ

**Expand table**

`kernel_function_parameter_to_bedrock_function_parameter(parameter: `

`KernelParameterMetadata)`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**schema_data**`

Required*

**kernel_function_to_bedrock_function_schema**

Convert the kernel function to bedrock function schema.

Python

**Parameters**

**Name**

**Description**

`**function_choice_configuration**`

Required*

**parse_function_result_contents**

Parse the function result contents to be returned to the agent in the session
state.

Python

**Parameters**

`kernel_function_parameter_type_to_bedrock_function_parameter_type(schema_data:
`

`dict[str, Any] | ``None``) -> str`

ﾉ

**Expand table**

`kernel_function_to_bedrock_function_schema(function_choice_configuration: `

`FunctionCallChoiceConfiguration) -> dict[str, Any]`

ﾉ

**Expand table**

`parse_function_result_contents(function_result_contents: `

`list[FunctionResultContent]) -> list[dict[str, Any]]`

ﾉ

**Expand table**



**Name**

**Description**

`**function_result_contents**`

Required*

**parse_return_control_payload**

Parse the return control payload to a list of function call contents for the
kernel.

Python

**Parameters**

**Name**

**Description**

`**return_control_payload**`

Required*

`parse_return_control_payload(return_control_payload: dict[str, Any]) -> `

`list[FunctionCallContent]`

ﾉ

**Expand table**



**bedrock_agent_settings Module**

Reference

**Classes**

BedrockAgentSettings

Amazon Bedrock Agent service settings.

The settings are first loaded from environment variables with the prefix

'>>BEDROCK_AGENT_<<'. If the environment variables are not found,

the settings can be loaded from a .env file with the encoding 'utf-8'. If

the settings are not found in the .env file, the settings are ignored;

however, validation will fail alerting that the settings are missing.

Optional settings for prefix '>>BEDROCK_<<' are: *

agent_resource_role_arn: str - The Amazon Bedrock agent resource role

ARN.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**

` `

`[https://docs.aws.amazon.com/bedrock/latest/userguide/`

`getting-started.html]`

`(https://docs.aws.amazon.com/bedrock/latest/userguide/`

`getting-started.html)`

` (Env var BEDROCK_AGENT_AGENT_RESOURCE_ROLE_ARN)`

` * foundation_model: str - The Amazon Bedrock `

`foundation model ID to use.`

` This is required when creating a new agent.`

` (Env var BEDROCK_AGENT_FOUNDATION_MODEL)`



**BedrockAgentSettings Class**

Reference

Amazon Bedrock Agent service settings.

The settings are first loaded from environment variables with the prefix

'>>BEDROCK_AGENT_<<'. If the environment variables are not found, the settings
can

be loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the

.env file, the settings are ignored; however, validation will fail alerting
that the settings

are missing.

Optional settings for prefix '>>BEDROCK_<<' are: * agent_resource_role_arn:
str - The

Amazon Bedrock agent resource role ARN.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

` [https://docs.aws.amazon.com/bedrock/latest/userguide/getting-`

`started.html](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-`

`started.html)`

` (Env var BEDROCK_AGENT_AGENT_RESOURCE_ROLE_ARN)`

` * foundation_model: str - The Amazon Bedrock foundation model ID to use.`

` This is required when creating a new agent.`

` (Env var BEDROCK_AGENT_FOUNDATION_MODEL)`

`BedrockAgentSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `



**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`agent_resource_role_arn: str, foundation_model: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**agent_resource_role_arn**`

Required*

`**foundation_model**`

Required*

**agent_resource_role_arn**

Python

**env_prefix**

Python

**foundation_model**

Python

**is_experimental**

Python

ﾉ

**Expand table**

**Attributes**

`agent_resource_role_arn: str`

`env_prefix: ClassVar[str] = ``'BEDROCK_AGENT_'`

`foundation_model: str | ``None`



**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**channels Package**

Reference

**Modules**

agent_channel

bedrock_agent_channel

chat_history_channel

ﾉ

**Expand table**



**agent_channel Module**

Reference

**Classes**

AgentChannel

Defines the communication protocol for a particular Agent type.

An agent provides it own AgentChannel via CreateChannel.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**AgentChannel Class**

Reference

Defines the communication protocol for a particular Agent type.

An agent provides it own AgentChannel via CreateChannel.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

get_history

Retrieve the message history specific to this channel.

invoke

Perform a discrete incremental interaction between a single Agent and

AgentChat.

invoke_stream

Perform a discrete incremental stream interaction between a single Agent and

AgentChat.

receive

Receive the conversation messages.

Used when joining a conversation and also during each agent interaction.

reset

Reset any persistent state associated with the channel.

**get_history**

Retrieve the message history specific to this channel.

Python

`AgentChannel()`

**Methods**

ﾉ

**Expand table**

`abstract get_history() -> AsyncIterable[ChatMessageContent]`



**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**invoke**

Perform a discrete incremental interaction between a single Agent and
AgentChat.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`**kwargs**`

Required*

The keyword arguments.

**Returns**

**Type**

**Description**

An async iterable of a bool, ChatMessageContent.

**invoke_stream**

Perform a discrete incremental stream interaction between a single Agent and

AgentChat.

ﾉ

**Expand table**

`abstract invoke(agent: Agent, **kwargs: Any) -> AsyncIterable[tuple[bool, `

`ChatMessageContent]]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`**messages**`

Required*

The history of messages in the conversation.

`**kwargs**`

Required*

The keyword arguments.

**Returns**

**Type**

**Description**

An async iterable ChatMessageContent.

**receive**

Receive the conversation messages.

Used when joining a conversation and also during each agent interaction.

Python

**Parameters**

`abstract invoke_stream(agent: Agent, messages: list[ChatMessageContent], `

`**kwargs: Any) -> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` receive(history: list[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**history**`

Required*

The history of messages in the conversation.

**reset**

Reset any persistent state associated with the channel.

Python

**is_experimental**

Python

**stage_status**

Python

`abstract ``async`` reset() -> ``None`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**bedrock_agent_channel Module**

Reference

**Classes**

BedrockAgentChannel

An AgentChannel for a BedrockAgent that is based on a ChatHistory.

This channel allows Bedrock agents to interact with other types of agents

in Semantic Kernel in an AgentGroupChat. However, since Bedrock

agents require the chat history to alternate between user and agent

messages, this channel will preprocess the chat history to ensure that it

meets the requirements of the Bedrock agent. When an invalid pattern is

detected, the channel will insert a placeholder user or assistant message

to ensure that the chat history alternates between user and agent

messages.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BedrockAgentChannel Class**

Reference

An AgentChannel for a BedrockAgent that is based on a ChatHistory.

This channel allows Bedrock agents to interact with other types of agents in
Semantic

Kernel in an AgentGroupChat. However, since Bedrock agents require the chat
history to

alternate between user and agent messages, this channel will preprocess the
chat

history to ensure that it meets the requirements of the Bedrock agent. When an
invalid

pattern is detected, the channel will insert a placeholder user or assistant
message to

ensure that the chat history alternates between user and agent messages.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`BedrockAgentChannel(*, messages: list[ChatMessageContent] = ``None``, `

`system_message: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**



get_history

Retrieve the message history specific to this channel.

invoke

Perform a discrete incremental interaction between a single Agent and

AgentChat.

invoke_stream

Perform a streaming interaction between a single Agent and AgentChat.

receive

Receive the conversation messages.

Bedrock requires the chat history to alternate between user and agent
messages.

Thus, when receiving the history, the message sequence will be mutated by

inserting empty agent or user messages as needed.

reset

Reset the channel state.

**get_history**

Retrieve the message history specific to this channel.

Python

**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**invoke**

Perform a discrete incremental interaction between a single Agent and
AgentChat.

Python

**Parameters**

ﾉ

**Expand table**

`async`` get_history() -> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`async`` invoke(agent: Agent, **kwargs: Any) -> AsyncIterable[tuple[bool, `

`ChatMessageContent]]`



**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`**kwargs**`

Required*

Additional keyword arguments.

**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent with a boolean indicating if the
message

should be visible external to the agent.

**invoke_stream**

Perform a streaming interaction between a single Agent and AgentChat.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`**messages**`

Required*

The history of messages in the conversation.

`**kwargs**`

Required*

Additional keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_stream(agent: Agent, messages: list[ChatMessageContent], `

`**kwargs: Any) -> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**receive**

Receive the conversation messages.

Bedrock requires the chat history to alternate between user and agent
messages.

Thus, when receiving the history, the message sequence will be mutated by
inserting

empty agent or user messages as needed.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

The history of messages in the conversation.

**reset**

Reset the channel state.

Python

**MESSAGE_PLACEHOLDER**

ﾉ

**Expand table**

`async`` receive(history: list[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`async`` reset() -> ``None`

**Attributes**



Python

**is_experimental**

Python

**stage_status**

Python

`MESSAGE_PLACEHOLDER: ClassVar[str] = ``'[SILENCE]'`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**chat_history_channel Module**

Reference

**Classes**

ChatHistoryChannel

An AgentChannel specialization for that acts upon a ChatHistoryHandler.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatHistoryChannel Class**

Reference

An AgentChannel specialization for that acts upon a ChatHistoryHandler.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

get_history

Retrieve the message history specific to this channel.

invoke

Perform a discrete incremental interaction between a single Agent and

AgentChat.

`ChatHistoryChannel(*, messages: list[ChatMessageContent] = ``None``, `

`system_message: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



invoke_stream

Perform a discrete incremental stream interaction between a single Agent and

AgentChat.

receive

Receive the conversation messages.

Do not include messages that only contain file references.

reset

Reset the channel state.

**get_history**

Retrieve the message history specific to this channel.

Python

**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**invoke**

Perform a discrete incremental interaction between a single Agent and
AgentChat.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`async`` get_history() -> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`async`` invoke(agent: Agent, **kwargs: Any) -> AsyncIterable[tuple[bool, `

`ChatMessageContent]]`

ﾉ

**Expand table**



**Name**

**Description**

`**kwargs**`

Required*

The keyword arguments.

**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**invoke_stream**

Perform a discrete incremental stream interaction between a single Agent and

AgentChat.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to interact with.

`**messages**`

Required*

The history of messages in the conversation.

`**kwargs**`

Required*

The keyword arguments

**Returns**

ﾉ

**Expand table**

`async`` invoke_stream(agent: Agent, messages: list[ChatMessageContent], `

`**kwargs: Any) -> AsyncIterable[StreamingChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

An async iterable of ChatMessageContent.

**receive**

Receive the conversation messages.

Do not include messages that only contain file references.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

The history of messages in the conversation.

**reset**

Reset the channel state.

Python

**ALLOWED_CONTENT_TYPES**

Python

`async`` receive(history: list[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`async`` reset() -> ``None`

**Attributes**

`ALLOWED_CONTENT_TYPES: ClassVar[tuple[type, ...]] = (<``class`` `

`'``semantic_kernel``.``contents``.``image_content``.``ImageContent``'>,
<``class`` `

`'``semantic_kernel``.``contents``.``function_call_content``.``FunctionCallContent``'>,
`

`<``class`` `

`'``semantic_kernel``.``contents``.``function_result_content``.``FunctionResultContent``'>`



**is_experimental**

Python

**stage_status**

Python

`, <``class`` `

`'``semantic_kernel``.``contents``.``streaming_text_content``.``StreamingTextContent``'>,
`

`<``class``
'``semantic_kernel``.``contents``.``text_content``.``TextContent``'>)`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**chat_completion Package**

Reference

**Modules**

chat_completion_agent

ﾉ

**Expand table**



**chat_completion_agent Module**

Reference

**Classes**

ChatCompletionAgent

A Chat Completion Agent based on ChatCompletionClientBase.

Features marked with this status are nearing completion and are

considered stable for most purposes, but may still incur minor

refinements or optimizations before achieving full general availability.

(Version: 1.22.0-rc1)

Initialize a new instance of ChatCompletionAgent.

ﾉ

**Expand table**



**ChatCompletionAgent Class**

Reference

A Chat Completion Agent based on ChatCompletionClientBase.

Features marked with this status are nearing completion and are considered
stable for

most purposes, but may still incur minor refinements or optimizations before
achieving

full general availability. (Version: 1.22.0-rc1)

Initialize a new instance of ChatCompletionAgent.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

The kernel arguments for the agent. Invoke method arguments

take precedence over the arguments provided here.

`**description**`

Required*

The description of the agent.

`**function_choice_behavior**`

Required*

The function choice behavior to determine how and which

plugins are advertised to the model.

`**kernel**`

Required*

The kernel instance. If both a kernel and a service are provided,

the service will take precedence if they share the same service_id

or ai_model_id. Otherwise if separate, the first AI service

registered on the kernel will be used.

`**id**`

`ChatCompletionAgent(*, arguments: KernelArguments | ``None`` = ``None``, `

`description: str | ``None`` = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, id: str | ``None`` = ``None``, instructions: `

`str | ``None`` = ``None``, kernel: Kernel | ``None`` = ``None``, name: str | ``None`` = ``None``, `

`plugins: list[KernelPlugin | object] | dict[str, KernelPlugin | object] | `

`None`` = ``None``, prompt_template_config: PromptTemplateConfig | ``None`` = ``None``, `

`service: ChatCompletionClientBase | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The unique identifier for the agent. If not provided, a unique

GUID will be generated.

`**instructions**`

Required*

The instructions for the agent.

`**name**`

Required*

The name of the agent.

`**plugins**`

Required*

The plugins for the agent. If plugins are included along with a

kernel, any plugins that already exist in the kernel will be

overwritten.

`**prompt_template_config**`

Required*

The prompt template configuration for the agent.

`**service**`

Required*

The chat completion service instance. If a kernel is provided with

the same service_id or _ai_model_id_ , the service will take

precedence.

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**description**`

Required*

`**function_choice_behavior**`

Required*

`**id**`

Required*

`**instructions**`

Required*

`**kernel**`

Required*

`**name**`

Required*

`**plugins**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**prompt_template_config**`

Required*

`**service**`

Required*

configure_service

Configure the service used by the ChatCompletionAgent.

get_response

Get a response from the agent.

invoke

Invoke the chat history handler.

invoke_stream

Invoke the chat history handler in streaming mode.

**configure_service**

Configure the service used by the ChatCompletionAgent.

Python

**get_response**

Get a response from the agent.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`configure_service() -> ChatCompletionAgent`

`async`` get_response(history: ChatHistory, arguments: KernelArguments | `

`None`` = ``None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`ChatMessageContent`

ﾉ

**Expand table**



**Name**

**Description**

`**history**`

Required*

The chat history.

`**arguments**`

Required*

The kernel arguments. (optional)

`**kernel**`

Required*

The kernel instance. (optional)

`**kwargs**`

Required*

The keyword arguments. (optional)

**Returns**

**Type**

**Description**

A chat message content.

**invoke**

Invoke the chat history handler.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

The chat history.

`**arguments**`

Required*

The kernel arguments.

`**kernel**`

The kernel instance.

ﾉ

**Expand table**

`async`` invoke(history: ChatHistory, arguments: KernelArguments | ``None`` = `

`None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**kwargs**`

Required*

The keyword arguments.

**Returns**

**Type**

**Description**

An async iterable of ChatMessageContent.

**invoke_stream**

Invoke the chat history handler in streaming mode.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

The chat history.

`**arguments**`

Required*

The kernel arguments.

`**kernel**`

Required*

The kernel instance.

`**kwargs**`

Required*

The keyword arguments.

**Returns**

ﾉ

**Expand table**

`async`` invoke_stream(history: ChatHistory, arguments: KernelArguments | `

`None`` = ``None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[StreamingChatMessageContent]`

ﾉ

**Expand table**



**Type**

**Description**

An async generator of StreamingChatMessageContent.

**arguments**

Python

**description**

Python

**function_choice_behavior**

Python

**id**

Python

**instructions**

Python

ﾉ

**Expand table**

**Attributes**

`arguments: KernelArguments | ``None`

`description: str | ``None`

`function_choice_behavior: FunctionChoiceBehavior | ``None`

`id: str`

`instructions: str | ``None`



**is_release_candidate**

Python

**kernel**

Python

**name**

Python

**prompt_template**

Python

**service**

Python

**stage_status**

Python

**stage_version**

`is_release_candidate = ``True`

`kernel: Kernel`

`name: str`

`prompt_template: PromptTemplateBase | ``None`

`service: ChatCompletionClientBase | ``None`

`stage_status = ``'release_candidate'`



Python

`stage_version = ``'1.22.0-rc1'`



**group_chat Package**

Reference

**Modules**

agent_chat

agent_chat_utils

agent_group_chat

broadcast_queue

ﾉ

**Expand table**



**agent_chat Module**

Reference

**Classes**

AgentChat

A base class chat interface for agents.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AgentChat Class**

Reference

A base class chat interface for agents.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**broadcast_queue**`

Required*

`**agent_channels**`

Required*

`**channel_map**`

Required*

`**history**`

Required*

`AgentChat(*, broadcast_queue: BroadcastQueue = ``None``, agent_channels: `

`dict[str, AgentChannel] = ``None``, channel_map: dict[Agent, str] = ``None``,
`

`history: ChatHistory = ``None``)`

ﾉ

**Expand table**

**Methods**



add_chat_message

Add a chat message.

add_chat_messages

Add chat messages.

clear_activity_signal

Clear the activity signal.

get_chat_messages

Get chat messages asynchronously.

get_messages_in_descending_order

Get messages in descending order asynchronously.

invoke

Invoke the agent asynchronously.

invoke_agent

Invoke an agent asynchronously.

invoke_agent_stream

Invoke an agent stream asynchronously.

model_post_init

This function is meant to behave like a BaseModel method

to initialise private attributes.

It takes context as an argument since that's what pydantic-

core passes when calling it.

reset

Reset the agent chat.

set_activity_or_throw

Set the activity signal or throw an exception if another

agent is active.

**add_chat_message**

Add a chat message.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

**add_chat_messages**

ﾉ

**Expand table**

`async`` add_chat_message(message: str | ChatMessageContent) -> ``None`

ﾉ

**Expand table**



Add chat messages.

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

**clear_activity_signal**

Clear the activity signal.

Python

**get_chat_messages**

Get chat messages asynchronously.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Default value: None

**get_messages_in_descending_order**

`async`` add_chat_messages(messages: list[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`clear_activity_signal()`

`async`` get_chat_messages(agent: Agent | ``None`` = ``None``) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



Get messages in descending order asynchronously.

Python

**invoke**

Invoke the agent asynchronously.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Default value: None

`**is_joining**`

Default value: True

**invoke_agent**

Invoke an agent asynchronously.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

`async`` get_messages_in_descending_order() -> `

`AsyncIterable[ChatMessageContent]`

`invoke(agent: Agent | ``None`` = ``None``, is_joining: bool = ``True``) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`async`` invoke_agent(agent: Agent) -> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



**invoke_agent_stream**

Invoke an agent stream asynchronously.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when
calling

it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**Parameters**

`async`` invoke_agent_stream(agent: Agent) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**reset**

Reset the agent chat.

Python

**set_activity_or_throw**

Set the activity signal or throw an exception if another agent is active.

Python

**is_active**

Indicates whether the agent is currently active.

**agent_channels**

Python

**broadcast_queue**

ﾉ

**Expand table**

`async`` reset() -> ``None`

`set_activity_or_throw()`

**Attributes**

`agent_channels: dict[str, AgentChannel]`



Python

**channel_map**

Python

**history**

Python

**is_experimental**

Python

**stage_status**

Python

`broadcast_queue: BroadcastQueue`

`channel_map: dict[Agent, str]`

`history: ChatHistory`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**agent_chat_utils Module**

Reference

**Classes**

KeyEncoder

A class for encoding keys.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**KeyEncoder Class**

Reference

A class for encoding keys.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

generate_hash

Generate a hash from a list of keys.

**generate_hash**

Generate a hash from a list of keys.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

A list of keys to generate the hash from.

**Returns**

`KeyEncoder()`

**Methods**

ﾉ

**Expand table**

`static generate_hash(keys: Iterable[str]) -> str`

ﾉ

**Expand table**



**Type**

**Description**

str

The generated hash.

**Exceptions**

**Type**

**Description**

AgentExecutionException

If the keys are empty

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**agent_group_chat Module**

Reference

**Classes**

AgentGroupChat

An agent chat that supports multi-turn interactions.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of AgentGroupChat.

ﾉ

**Expand table**



**AgentGroupChat Class**

Reference

An agent chat that supports multi-turn interactions.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of AgentGroupChat.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**agents**`

The agents to add to the group chat.

Default value: None

`**termination_strategy**`

The termination strategy to use.

Default value: None

`**selection_strategy**`

The selection strategy

Default value: None

`**chat_history**`

The chat history.

Default value: None

add_agent

Add an agent to the group chat.

invoke

Invoke the agent chat asynchronously.

`AgentGroupChat(agents: list[Agent] | ``None`` = ``None``, termination_strategy: `

`TerminationStrategy | ``None`` = ``None``, selection_strategy: SelectionStrategy | `

`None`` = ``None``, chat_history: ChatHistory | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Handles both group interactions and single agent interactions based

on the provided arguments.

invoke_single_turn

Invoke the agent chat for a single turn.

invoke_stream

Invoke the agent chat stream asynchronously.

Handles both group interactions and single agent interactions based

on the provided arguments.

invoke_stream_single_turn

Invoke the agent chat for a single turn.

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

reduce_history

Perform the reduction on the provided history, returning True if

reduction occurred.

**add_agent**

Add an agent to the group chat.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to add.

**invoke**

Invoke the agent chat asynchronously.

Handles both group interactions and single agent interactions based on the

provided arguments.

Python

`add_agent(agent: Agent) -> ``None`

ﾉ

**Expand table**

`async`` invoke(agent: Agent | ``None`` = ``None``, is_joining: bool = ``True``) -> `



**Parameters**

**Name**

**Description**

`**agent**`

The agent to invoke. If not provided, the method processes all agents in the

chat.

Default value: None

`**is_joining**`

Controls whether the agent joins the chat. Defaults to True.

Default value: True

**invoke_single_turn**

Invoke the agent chat for a single turn.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to invoke.

**invoke_stream**

Invoke the agent chat stream asynchronously.

Handles both group interactions and single agent interactions based on the

provided arguments.

Python

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`async`` invoke_single_turn(agent: Agent) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**agent**`

The agent to invoke. If not provided, the method processes all agents in the

chat.

Default value: None

`**is_joining**`

Controls whether the agent joins the chat. Defaults to True.

Default value: True

**invoke_stream_single_turn**

Invoke the agent chat for a single turn.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to invoke.

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

`async`` invoke_stream(agent: Agent | ``None`` = ``None``, is_joining: bool = ``True``) `

`-> AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

`async`` invoke_stream_single_turn(agent: Agent) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**



**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**reduce_history**

Perform the reduction on the provided history, returning True if reduction
occurred.

Python

**agent_channels**

Python

**agent_ids**

Python

**agents**

Python

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

`async`` reduce_history() -> bool`

**Attributes**

`agent_channels: dict[str, AgentChannel]`

`agent_ids: set[str]`

`agents: list[Agent]`



**broadcast_queue**

Python

**channel_map**

Python

**history**

Python

**is_complete**

Python

**is_experimental**

Python

**selection_strategy**

Python

**stage_status**

`broadcast_queue: BroadcastQueue`

`channel_map: dict[Agent, str]`

`history: ChatHistory`

`is_complete: bool`

`is_experimental = ``True`

`selection_strategy: SelectionStrategy`



Python

**termination_strategy**

Python

`stage_status = ``'experimental'`

`termination_strategy: TerminationStrategy`



**broadcast_queue Module**

Reference

**Classes**

BroadcastQueue

A queue for broadcasting messages to listeners.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ChannelReference

Tracks a channel along with its hashed key.

Note: This class is marked as 'experimental' and may change in the future.

QueueReference

Utility class to associate a queue with its specific lock.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BroadcastQueue Class**

Reference

A queue for broadcasting messages to listeners.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**queues**`

Required*

`**block_duration**`

Default value: 0.1

enqueue

Enqueue a set of messages for a given channel.

ensure_synchronized

Blocks until a channel-queue is not in a receive state to ensure that

channel history is complete.

`BroadcastQueue(*, queues: dict[str, QueueReference] = ``None``,
block_duration: `

`float = 0.1)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



receive

Processes the specified queue with the provided channel, until the queue

is empty.

**enqueue**

Enqueue a set of messages for a given channel.

Python

**Parameters**

**Name**

**Description**

`**channel_refs**`

Required*

The channel references.

`**messages**`

Required*

The messages to broadcast.

**ensure_synchronized**

Blocks until a channel-queue is not in a receive state to ensure that channel
history is

complete.

Python

**Parameters**

**Name**

**Description**

`**channel_ref**`

Required*

The channel reference.

`async`` enqueue(channel_refs: list[ChannelReference], messages: `

`list[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`async`` ensure_synchronized(channel_ref: ChannelReference) -> ``None`

ﾉ

**Expand table**



**receive**

Processes the specified queue with the provided channel, until the queue is
empty.

Python

**Parameters**

**Name**

**Description**

`**channel_ref**`

Required*

The channel reference.

`**queue_ref**`

Required*

The queue reference.

**block_duration**

Python

**is_experimental**

Python

**queues**

Python

`async`` receive(channel_ref: ChannelReference, queue_ref: QueueReference) -`

`> ``None`

ﾉ

**Expand table**

**Attributes**

`block_duration: float`

`is_experimental = ``True`

`queues: dict[str, QueueReference]`



**stage_status**

Python

`stage_status = ``'experimental'`



**ChannelReference Class**

Reference

Tracks a channel along with its hashed key.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**hash**`

Required*

`**channel**`

Default value: <factory>

**channel**

Python

**hash**

Python

`ChannelReference(hash: str, channel: `

`~semantic_kernel.agents.channels.agent_channel.AgentChannel = <factory>)`

ﾉ

**Expand table**

**Attributes**

`channel: AgentChannel`

`hash: str`



**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**QueueReference Class**

Reference

Utility class to associate a queue with its specific lock.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**queue**`

Required*

`**queue_lock**`

Required*

`**receive_task**`

Required*

`**receive_failure**`

Required*

`QueueReference(*, queue: deque = ``None``, queue_lock: Annotated[Lock, `

`SkipValidation()] = ``None``, receive_task: Annotated[Task | ``None``, `

`SkipValidation()] = ``None``, receive_failure: Exception | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**



validate_receive_task

Validate the receive task.

**validate_receive_task**

Validate the receive task.

Python

**Parameters**

**Name**

**Description**

`**values**`

Required*

**is_empty**

Check if the queue is empty.

**is_experimental**

Python

**queue**

Python

ﾉ

**Expand table**

`validate_receive_task(values: Any)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`queue: deque`



**queue_lock**

Python

**receive_failure**

Python

**receive_task**

Python

**stage_status**

Python

`queue_lock: Annotated[Lock, SkipValidation()]`

`receive_failure: Exception | ``None`

`receive_task: Annotated[Task | ``None``, SkipValidation()]`

`stage_status = ``'experimental'`



**open_ai Package**

Reference

**Modules**

assistant_content_generation

azure_assistant_agent

open_ai_assistant_agent

run_polling_options

ﾉ

**Expand table**



**open_ai_assistant_agent Module**

Reference

**Classes**

OpenAIAssistantAgent

OpenAI Assistant Agent class. Provides the ability to interact with OpenAI

Assistants. Initialize an OpenAIAssistant service.

ﾉ

**Expand table**



**OpenAIAssistantAgent Class**

Reference

OpenAI Assistant Agent class. Provides the ability to interact with OpenAI
Assistants.

Initialize an OpenAIAssistant service.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

The arguments to pass to the function.

`**client**`

Required*

The OpenAI client.

`**definition**`

Required*

The assistant definition.

`**kernel**`

The Kernel instance.

`**plugins**`

The plugins to add to the kernel. If both the plugins and the kernel

are supplied, the plugins take precedence and are added to the

kernel by default.

`**polling_options**`

The polling options.

`**prompt_template_config**`

The prompt template configuration.

`OpenAIAssistantAgent(*, arguments: KernelArguments | ``None`` = ``None``, client: `

`AsyncOpenAI, definition: Assistant, kernel: Kernel | ``None`` = ``None``, plugins: `

`list[KernelPlugin | object] | dict[str, KernelPlugin | object] | ``None`` = `

`None``, polling_options: RunPollingOptions | ``None`` = ``None``, `

`prompt_template_config: PromptTemplateConfig | ``None`` = ``None``, **kwargs: Any)`

ﾉ

**Expand table**

**Methods**



get_response

Get a response from the agent on a thread.

invoke

Invoke the agent.

invoke_stream

Invoke the agent in streaming mode.

**get_response**

Get a response from the agent on a thread.

Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The response from the agent.

**invoke**

Invoke the agent.

ﾉ

**Expand table**

`async`` get_response(thread_id: str, *, arguments: KernelArguments | ``None`` = `

`None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> ChatMessageContent`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The chat message content.

**invoke_stream**

Invoke the agent in streaming mode.

Python

**Parameters**

`async`` invoke(thread_id: str, *, arguments: KernelArguments | ``None`` = ``None``, `

`kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_stream(thread_id: str, *, arguments: KernelArguments | ``None`` `

`= ``None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[StreamingChatMessageContent]`

ﾉ

**Expand table**



**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

An async generator of StreamingChatMessageContent.

**client**

Python

**definition**

Python

**plugins**

Python

**polling_options**

ﾉ

**Expand table**

**Attributes**

`client: AsyncOpenAI`

`definition: Assistant`

`plugins: list[Any]`



Python

`polling_options: RunPollingOptions`



**azure_assistant_agent Module**

Reference

**Classes**

AzureAssistantAgent

Azure Assistant Agent class. Provides the ability to interact with Azure

Assistants. Initialize an AzureAssistant service.

ﾉ

**Expand table**



**AzureAssistantAgent Class**

Reference

Azure Assistant Agent class. Provides the ability to interact with Azure
Assistants.

Initialize an AzureAssistant service.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

The arguments to pass to the function.

`**client**`

Required*

The Azure OpenAI client.

`**definition**`

Required*

The assistant definition.

`**kernel**`

The Kernel instance.

`**plugins**`

The plugins to add to the kernel. If both the plugins and the kernel

are supplied, the plugins take precedence and are added to the

kernel by default.

`**polling_options**`

The polling options.

`**prompt_template_config**`

The prompt template configuration.

`AzureAssistantAgent(*, arguments: KernelArguments | ``None`` = ``None``, client: `

`AsyncAzureOpenAI, definition: Assistant, kernel: Kernel | ``None`` = ``None``, `

`plugins: list[KernelPlugin | object] | dict[str, KernelPlugin | object] | `

`None`` = ``None``, polling_options: RunPollingOptions | ``None`` = ``None``, `

`prompt_template_config: PromptTemplateConfig | ``None`` = ``None``, **kwargs: Any)`

ﾉ

**Expand table**

**Methods**



get_response

Get a response from the agent on a thread.

invoke

Invoke the agent.

invoke_stream

Invoke the agent in streaming mode.

**get_response**

Get a response from the agent on a thread.

Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The response from the agent.

**invoke**

Invoke the agent.

ﾉ

**Expand table**

`async`` get_response(thread_id: str, *, arguments: KernelArguments | ``None`` = `

`None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> ChatMessageContent`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

The chat message content.

**invoke_stream**

Invoke the agent in streaming mode.

Python

**Parameters**

`async`` invoke(thread_id: str, *, arguments: KernelArguments | ``None`` = ``None``, `

`kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[ChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_stream(thread_id: str, *, arguments: KernelArguments | ``None`` `

`= ``None``, kernel: Kernel | ``None`` = ``None``, **kwargs: Any) -> `

`AsyncIterable[StreamingChatMessageContent]`

ﾉ

**Expand table**



**Name**

**Description**

`**thread_id**`

Required*

The ID of the thread.

`**arguments**`

The kernel arguments.

`**kernel**`

The kernel.

`**kwargs**`

Additional keyword arguments.

**Returns**

**Type**

**Description**

An async generator of StreamingChatMessageContent.

**client**

Python

**definition**

Python

**plugins**

Python

**polling_options**

ﾉ

**Expand table**

**Attributes**

`client: AsyncAzure`

`definition: Assistant`

`plugins: list[Any]`



Python

`polling_options: RunPollingOptions`



**strategies Package**

Reference

**Packages**

selection

termination

**Classes**

AggregatorTerminationStrategy

A strategy that aggregates multiple termination strategies.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

DefaultTerminationStrategy

A default termination strategy that never terminates.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelFunctionSelectionStrategy

Determines agent selection based on the evaluation of a

Kernel Function.

Note: This class is marked as 'experimental' and may change

in the future.

ﾉ

**Expand table**

ﾉ

**Expand table**



Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelFunctionTerminationStrategy

A termination strategy that uses a kernel function to

determine termination.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

SequentialSelectionStrategy

Round-robin turn-taking strategy. Agent order is based on

the order in which they joined.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TerminationStrategy

A strategy for determining when an agent should terminate.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.



**selection Package**

Reference

**Modules**

kernel_function_selection_strategy

selection_strategy

sequential_selection_strategy

ﾉ

**Expand table**



**kernel_function_selection_strategy**

**Module**

Reference

**Classes**

KernelFunctionSelectionStrategy

Determines agent selection based on the evaluation of a Kernel

Function.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFunctionSelectionStrategy Class**

Reference

Determines agent selection based on the evaluation of a Kernel Function.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**has_selected**`

Required*

`**initial_agent**`

Required*

`**agent_variable_name**`

Default value: _agent_

`**history_variable_name**`

Default value: _history_

`**arguments**`

Required*

`**function**`

`KernelFunctionSelectionStrategy(*, has_selected: bool = ``False``, `

`initial_agent: Agent | ``None`` = ``None``, agent_variable_name: str | ``None`` = `

`'_agent_'``, history_variable_name: str | ``None`` = ``'_history_'``, arguments: `

`KernelArguments | ``None`` = ``None``, function: KernelFunction, kernel: Kernel, `

`result_parser: Callable[[...], str] = ``None``, history_reducer: `

`ChatHistoryReducer | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**kernel**`

Required*

`**result_parser**`

Required*

`**history_reducer**`

Required*

select_agent

Select the next agent to interact with.

**select_agent**

Select the next agent to interact with.

Python

**Parameters**

**Name**

**Description**

`**agents**`

Required*

The list of agents to select from.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Methods**

ﾉ

**Expand table**

`async`` select_agent(agents: list[Agent], history: `

`list[ChatMessageContent]) -> Agent`

ﾉ

**Expand table**



**Type**

**Description**

The next agent to interact with.

**Exceptions**

**Type**

**Description**

AgentExecutionException

If the strategy fails to execute the function or select the next

agent

**DEFAULT_AGENT_VARIABLE_NAME**

Python

**DEFAULT_HISTORY_VARIABLE_NAME**

Python

**agent_variable_name**

Python

**arguments**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`DEFAULT_AGENT_VARIABLE_NAME: ClassVar[str] = ``'_agent_'`

`DEFAULT_HISTORY_VARIABLE_NAME: ClassVar[str] = ``'_history_'`

`agent_variable_name: str | ``None`



**function**

Python

**has_selected**

Python

**history_reducer**

Python

**history_variable_name**

Python

**initial_agent**

Python

**is_experimental**

Python

`arguments: KernelArguments | ``None`

`function: KernelFunction`

`has_selected: bool`

`history_reducer: ChatHistoryReducer | ``None`

`history_variable_name: str | ``None`

`initial_agent: Agent | ``None`

`is_experimental = ``True`



**kernel**

Python

**result_parser**

Python

**stage_status**

Python

`kernel: Kernel`

`result_parser: Callable[[...], str]`

`stage_status = ``'experimental'`



**selection_strategy Module**

Reference

**Classes**

SelectionStrategy

Base strategy class for selecting the next agent in a chat.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**SelectionStrategy Class**

Reference

Base strategy class for selecting the next agent in a chat.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**has_selected**`

Required*

`**initial_agent**`

Required*

next

Select the next agent to interact with.

select_agent

Determines which agent goes next. Override for custom logic.

By default, this fallback returns the first agent in the list.

`SelectionStrategy(*, has_selected: bool = ``False``, initial_agent: Agent | ``None`` `

`= ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**next**

Select the next agent to interact with.

Python

**Parameters**

**Name**

**Description**

`**agents**`

Required*

The list of agents to select from.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

The agent who takes the next turn.

**select_agent**

Determines which agent goes next. Override for custom logic.

By default, this fallback returns the first agent in the list.

Python

**Parameters**

`async`` next(agents: list[Agent], history: list[ChatMessageContent]) -> `

`Agent`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` select_agent(agents: list[Agent], history: `

`list[ChatMessageContent]) -> Agent`



**Name**

**Description**

`**agents**`

Required*

`**history**`

Required*

**has_selected**

Python

**initial_agent**

Python

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`has_selected: bool`

`initial_agent: Agent | ``None`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**sequential_selection_strategy Module**

Reference

**Classes**

SequentialSelectionStrategy

Round-robin turn-taking strategy. Agent order is based on the

order in which they joined.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**SequentialSelectionStrategy Class**

Reference

Round-robin turn-taking strategy. Agent order is based on the order in which
they

joined.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**has_selected**`

Required*

`**initial_agent**`

Required*

model_post_init

This function is meant to behave like a BaseModel method to initialise private

attributes.

`SequentialSelectionStrategy(*, has_selected: bool = ``False``, initial_agent:
`

`Agent | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



It takes context as an argument since that's what pydantic-core passes when

calling it.

reset

Reset selection to the initial/first agent.

select_agent

Select the next agent in a round-robin fashion.

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when
calling

it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**Parameters**

**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**reset**

Reset selection to the initial/first agent.

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**select_agent**

Select the next agent in a round-robin fashion.

Python

**Parameters**

**Name**

**Description**

`**agents**`

Required*

The list of agents to select from.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

The agent who takes the next turn.

**has_selected**

Python

`reset() -> ``None`

`async`` select_agent(agents: list[Agent], history: `

`list[ChatMessageContent]) -> Agent`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`has_selected: bool`



**initial_agent**

Python

**is_experimental**

Python

**stage_status**

Python

`initial_agent: Agent | ``None`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**termination Package**

Reference

**Modules**

aggregator_termination_strategy

default_termination_strategy

kernel_function_termination_strategy

termination_strategy

ﾉ

**Expand table**



**kernel_function_termination_strategy**

**Module**

Reference

**Classes**

KernelFunctionTerminationStrategy

A termination strategy that uses a kernel function to

determine termination.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFunctionTerminationStrategy**

**Class**

Reference

A termination strategy that uses a kernel function to determine termination.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**maximum_iterations**`

Default value: 99

`**automatic_reset**`

Required*

`**agents**`

Required*

`**agent_variable_name**`

Default value: _agent_

`**history_variable_name**`

Default value: _history_

`KernelFunctionTerminationStrategy(*, maximum_iterations: int = 99, `

`automatic_reset: bool = ``False``, agents: list[Agent] = ``None``, `

`agent_variable_name: str | ``None`` = ``'_agent_'``, history_variable_name: str | `

`None`` = ``'_history_'``, arguments: KernelArguments | ``None`` = ``None``, function: `

`KernelFunction, kernel: Kernel, result_parser: Callable[[...], bool] =
``None``, `

`history_reducer: ChatHistoryReducer | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

`**function**`

Required*

`**kernel**`

Required*

`**result_parser**`

Required*

`**history_reducer**`

Required*

should_agent_terminate

Check if the agent should terminate.

**should_agent_terminate**

Check if the agent should terminate.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Methods**

ﾉ

**Expand table**

`async`` should_agent_terminate(agent: Agent, history: `

`list[ChatMessageContent]) -> bool`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**DEFAULT_AGENT_VARIABLE_NAME**

Python

**DEFAULT_HISTORY_VARIABLE_NAME**

Python

**agent_variable_name**

Python

**agents**

Python

**arguments**

Python

ﾉ

**Expand table**

**Attributes**

`DEFAULT_AGENT_VARIABLE_NAME: ClassVar[str] = ``'_agent_'`

`DEFAULT_HISTORY_VARIABLE_NAME: ClassVar[str] = ``'_history_'`

`agent_variable_name: str | ``None`

`agents: list[Agent]`



**automatic_reset**

Python

**function**

Python

**history_reducer**

Python

**history_variable_name**

Python

**is_experimental**

Python

**kernel**

Python

`arguments: KernelArguments | ``None`

`automatic_reset: bool`

`function: KernelFunction`

`history_reducer: ChatHistoryReducer | ``None`

`history_variable_name: str | ``None`

`is_experimental = ``True`

`kernel: Kernel`



**maximum_iterations**

Python

**result_parser**

Python

**stage_status**

Python

`maximum_iterations: int`

`result_parser: Callable[[...], bool]`

`stage_status = ``'experimental'`



**termination_strategy Module**

Reference

**Classes**

TerminationStrategy

A strategy for determining when an agent should terminate.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TerminationStrategy Class**

Reference

A strategy for determining when an agent should terminate.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**maximum_iterations**`

Default value: 99

`**automatic_reset**`

Required*

`**agents**`

Required*

should_agent_terminate

Check if the agent should terminate.

should_terminate

Check if the agent should terminate.

`TerminationStrategy(*, maximum_iterations: int = 99, automatic_reset: bool =
`

`False``, agents: list[Agent] = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**should_agent_terminate**

Check if the agent should terminate.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**should_terminate**

Check if the agent should terminate.

Python

**Parameters**

`async`` should_agent_terminate(agent: Agent, history: `

`list[ChatMessageContent]) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` should_terminate(agent: Agent, history: list[ChatMessageContent]) -`

`> bool`

ﾉ

**Expand table**



**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**agents**

Python

**automatic_reset**

Python

**is_experimental**

Python

**maximum_iterations**

Python

ﾉ

**Expand table**

**Attributes**

`agents: list[Agent]`

`automatic_reset: bool`

`is_experimental = ``True`



**stage_status**

Python

`maximum_iterations: int`

`stage_status = ``'experimental'`



**KernelFunctionSelectionStrategy Class**

Reference

Determines agent selection based on the evaluation of a Kernel Function.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**has_selected**`

Required*

`**initial_agent**`

Required*

`**agent_variable_name**`

Default value: _agent_

`**history_variable_name**`

Default value: _history_

`**arguments**`

Required*

`**function**`

`KernelFunctionSelectionStrategy(*, has_selected: bool = ``False``, `

`initial_agent: Agent | ``None`` = ``None``, agent_variable_name: str | ``None`` = `

`'_agent_'``, history_variable_name: str | ``None`` = ``'_history_'``, arguments: `

`KernelArguments | ``None`` = ``None``, function: KernelFunction, kernel: Kernel, `

`result_parser: Callable[[...], str] = ``None``, history_reducer: `

`ChatHistoryReducer | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**kernel**`

Required*

`**result_parser**`

Required*

`**history_reducer**`

Required*

select_agent

Select the next agent to interact with.

**select_agent**

Select the next agent to interact with.

Python

**Parameters**

**Name**

**Description**

`**agents**`

Required*

The list of agents to select from.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Methods**

ﾉ

**Expand table**

`async`` select_agent(agents: list[Agent], history: `

`list[ChatMessageContent]) -> Agent`

ﾉ

**Expand table**



**Type**

**Description**

The next agent to interact with.

**Exceptions**

**Type**

**Description**

AgentExecutionException

If the strategy fails to execute the function or select the next

agent

**DEFAULT_AGENT_VARIABLE_NAME**

Python

**DEFAULT_HISTORY_VARIABLE_NAME**

Python

**agent_variable_name**

Python

**arguments**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`DEFAULT_AGENT_VARIABLE_NAME: ClassVar[str] = ``'_agent_'`

`DEFAULT_HISTORY_VARIABLE_NAME: ClassVar[str] = ``'_history_'`

`agent_variable_name: str | ``None`



**function**

Python

**history_reducer**

Python

**history_variable_name**

Python

**is_experimental**

Python

**kernel**

Python

**result_parser**

Python

`arguments: KernelArguments | ``None`

`function: KernelFunction`

`history_reducer: ChatHistoryReducer | ``None`

`history_variable_name: str | ``None`

`is_experimental = ``True`

`kernel: Kernel`

`result_parser: Callable[[...], str]`



**stage_status**

Python

`stage_status = ``'experimental'`



**KernelFunctionTerminationStrategy**

**Class**

Reference

A termination strategy that uses a kernel function to determine termination.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**maximum_iterations**`

Default value: 99

`**automatic_reset**`

Required*

`**agents**`

Required*

`**agent_variable_name**`

Default value: _agent_

`**history_variable_name**`

Default value: _history_

`KernelFunctionTerminationStrategy(*, maximum_iterations: int = 99, `

`automatic_reset: bool = ``False``, agents: list[Agent] = ``None``, `

`agent_variable_name: str | ``None`` = ``'_agent_'``, history_variable_name: str | `

`None`` = ``'_history_'``, arguments: KernelArguments | ``None`` = ``None``, function: `

`KernelFunction, kernel: Kernel, result_parser: Callable[[...], bool] =
``None``, `

`history_reducer: ChatHistoryReducer | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

`**function**`

Required*

`**kernel**`

Required*

`**result_parser**`

Required*

`**history_reducer**`

Required*

should_agent_terminate

Check if the agent should terminate.

**should_agent_terminate**

Check if the agent should terminate.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Methods**

ﾉ

**Expand table**

`async`` should_agent_terminate(agent: Agent, history: `

`list[ChatMessageContent]) -> bool`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**DEFAULT_AGENT_VARIABLE_NAME**

Python

**DEFAULT_HISTORY_VARIABLE_NAME**

Python

**agent_variable_name**

Python

**arguments**

Python

**function**

Python

ﾉ

**Expand table**

**Attributes**

`DEFAULT_AGENT_VARIABLE_NAME: ClassVar[str] = ``'_agent_'`

`DEFAULT_HISTORY_VARIABLE_NAME: ClassVar[str] = ``'_history_'`

`agent_variable_name: str | ``None`

`arguments: KernelArguments | ``None`



**history_reducer**

Python

**history_variable_name**

Python

**is_experimental**

Python

**kernel**

Python

**result_parser**

Python

**stage_status**

Python

`function: KernelFunction`

`history_reducer: ChatHistoryReducer | ``None`

`history_variable_name: str | ``None`

`is_experimental = ``True`

`kernel: Kernel`

`result_parser: Callable[[...], bool]`

`stage_status = ``'experimental'`







**SequentialSelectionStrategy Class**

Reference

Round-robin turn-taking strategy. Agent order is based on the order in which
they

joined.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**has_selected**`

Required*

`**initial_agent**`

Required*

model_post_init

This function is meant to behave like a BaseModel method to initialise private

attributes.

`SequentialSelectionStrategy(*, has_selected: bool = ``False``, initial_agent:
`

`Agent | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



It takes context as an argument since that's what pydantic-core passes when

calling it.

reset

Reset selection to the initial/first agent.

select_agent

Select the next agent in a round-robin fashion.

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when
calling

it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**Parameters**

**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**reset**

Reset selection to the initial/first agent.

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**select_agent**

Select the next agent in a round-robin fashion.

Python

**Parameters**

**Name**

**Description**

`**agents**`

Required*

The list of agents to select from.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

The agent who takes the next turn.

**is_experimental**

Python

`reset() -> ``None`

`async`` select_agent(agents: list[Agent], history: `

`list[ChatMessageContent]) -> Agent`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`stage_status = ``'experimental'`



**TerminationStrategy Class**

Reference

A strategy for determining when an agent should terminate.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**maximum_iterations**`

Default value: 99

`**automatic_reset**`

Required*

`**agents**`

Required*

should_agent_terminate

Check if the agent should terminate.

should_terminate

Check if the agent should terminate.

`TerminationStrategy(*, maximum_iterations: int = 99, automatic_reset: bool =
`

`False``, agents: list[Agent] = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**should_agent_terminate**

Check if the agent should terminate.

Python

**Parameters**

**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**should_terminate**

Check if the agent should terminate.

Python

**Parameters**

`async`` should_agent_terminate(agent: Agent, history: `

`list[ChatMessageContent]) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` should_terminate(agent: Agent, history: list[ChatMessageContent]) -`

`> bool`

ﾉ

**Expand table**



**Name**

**Description**

`**agent**`

Required*

The agent to check.

`**history**`

Required*

The history of messages in the conversation.

**Returns**

**Type**

**Description**

True if the agent should terminate, False otherwise

**agents**

Python

**automatic_reset**

Python

**is_experimental**

Python

**maximum_iterations**

Python

ﾉ

**Expand table**

**Attributes**

`agents: list[Agent]`

`automatic_reset: bool`

`is_experimental = ``True`



**stage_status**

Python

`maximum_iterations: int`

`stage_status = ``'experimental'`



**agent Module**

Reference

**Classes**

Agent

Base abstraction for all Semantic Kernel agents.

An agent instance may participate in one or more conversations. A conversation
may

include one or more agents. In addition to identity and descriptive meta-data,
an Agent

must define its communication protocol, or AgentChannel.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**Agent Class**

Reference

Base abstraction for all Semantic Kernel agents.

An agent instance may participate in one or more conversations. A conversation
may

include one or more agents. In addition to identity and descriptive meta-data,
an Agent

must define its communication protocol, or AgentChannel.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**description**`

Required*

`**id**`

Required*

`**instructions**`

Required*

`**kernel**`

`Agent(*, arguments: KernelArguments | ``None`` = ``None``, description: str | ``None`` = `

`None``, id: str = ``None``, instructions: str | ``None`` = ``None``, kernel: Kernel = `

`None``, name: Annotated[str, _PydanticGeneralMetadata(pattern=``'^[0-9A-Za-`

`z_-]+$'``)] = ``None``, prompt_template: PromptTemplateBase | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**name**`

Required*

`**prompt_template**`

Required*

create_channel

Create a channel.

format_instructions

Format the instructions.

get_channel_keys

Get the channel keys.

get_response

Get a response from the agent.

This method returns the final result of the agent's execution as a single

ChatMessageContent object. The caller is blocked until the final result is

available.

Note: For streaming responses, use the invoke_stream method, which

returns intermediate steps and the final result as a stream of

StreamingChatMessageContent objects. Streaming only the final result is

not feasible because the timing of the final result's availability is unknown,

and blocking the caller until then is undesirable in streaming scenarios.

invoke

Invoke the agent.

This invocation method will return the intermediate steps and the final

results of the agent's execution as a stream of ChatMessageContent objects

to the caller.

Note: A ChatMessageContent object contains an entire message.

invoke_stream

Invoke the agent as a stream.

This invocation method will return the intermediate steps and final results of

the agent's execution as a stream of StreamingChatMessageContent objects

to the caller.

Note: A StreamingChatMessageContent object contains a chunk of a

message.

**Methods**

ﾉ

**Expand table**



**create_channel**

Create a channel.

Python

**Returns**

**Type**

**Description**

An instance of AgentChannel.

**format_instructions**

Format the instructions.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance.

`**arguments**`

Required*

The kernel arguments.

Default value: None

**Returns**

`async`` create_channel() -> AgentChannel`

ﾉ

**Expand table**

`async`` format_instructions(kernel: Kernel, arguments: KernelArguments | `

`None`` = ``None``) -> str | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The formatted instructions.

**get_channel_keys**

Get the channel keys.

Python

**Returns**

**Type**

**Description**

A list of channel keys.

**get_response**

Get a response from the agent.

This method returns the final result of the agent's execution as a single

ChatMessageContent object. The caller is blocked until the final result is
available.

Note: For streaming responses, use the invoke_stream method, which returns

intermediate steps and the final result as a stream of
StreamingChatMessageContent

objects. Streaming only the final result is not feasible because the timing of
the final

result's availability is unknown, and blocking the caller until then is
undesirable in

streaming scenarios.

Python

**invoke**

Invoke the agent.

`get_channel_keys() -> Iterable[str]`

ﾉ

**Expand table**

`abstract ``async`` get_response(*args, **kwargs) -> ChatMessageContent`



This invocation method will return the intermediate steps and the final
results of the

agent's execution as a stream of ChatMessageContent objects to the caller.

Note: A ChatMessageContent object contains an entire message.

Python

**invoke_stream**

Invoke the agent as a stream.

This invocation method will return the intermediate steps and final results of
the

agent's execution as a stream of StreamingChatMessageContent objects to the

caller.

Note: A StreamingChatMessageContent object contains a chunk of a message.

Python

**arguments**

The arguments for the agent

Python

**channel_type**

The type of the agent channel

Python

`abstract invoke(*args, **kwargs) -> AsyncIterable[ChatMessageContent]`

`abstract invoke_stream(*args, **kwargs) -> `

`AsyncIterable[StreamingChatMessageContent]`

**Attributes**

`arguments: KernelArguments | ``None`

`channel_type: ClassVar[type[AgentChannel] | ``None``] = ``None`



**description**

The description of the agent

Python

**id**

The unique identifier of the agent If no id is provided, a new UUID will be
generated.

Python

**instructions**

The instructions for the agent (optional)

Python

**kernel**

The kernel instance for the agent

Python

**name**

The name of the agent

Python

`description: str | ``None`

`id: str`

`instructions: str | ``None`

`kernel: Kernel`

`name: str`



**prompt_template**

The prompt template for the agent

Python

`prompt_template: PromptTemplateBase | ``None`



**connectors Package**

Reference

**Packages**

ai

memory

openapi_plugin

search

search_engine

utils

ﾉ

**Expand table**



**ai Package**

Reference

**Packages**

embeddings

google

onnx

open_ai

**Modules**

audio_to_text_client_base

chat_completion_client_base

completion_usage

function_call_choice_configuration

function_calling_utils

function_choice_behavior

function_choice_type

prompt_execution_settings

text_completion_client_base

text_to_audio_client_base

text_to_image_client_base

**Classes**

ﾉ

**Expand table**

ﾉ

**Expand table**



FunctionChoiceBehavior

Class that controls function choice behavior.

Attributes: enable_kernel_functions: Enable kernel functions.

max_auto_invoke_attempts: The maximum number of auto invoke

attempts. filters: Filters for the function choice behavior. Available

options are: excluded_plugins,

Properties: auto_invoke_kernel_functions: Check if the kernel functions

should be auto-invoked. Determined as max_auto_invoke_attempts >

0.

Methods: configure: Configures the settings for the function call

behavior, the default version in this class, does nothing, use subclasses

for different behaviors.

Class methods: Auto: Returns FunctionChoiceBehavior class with

auto_invoke enabled, and the desired functions based on either the

specified filters or the full qualified names. The model will decide

which function to use, if any.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**

` included_plugins, excluded_functions, or `

`included_functions.`

` >>type_<<: The type of function choice behavior.`

` NoneInvoke: Returns FunctionChoiceBehavior class `

`with auto_invoke disabled, and the desired `

`functions`

` based on either the specified filters or the `

`full qualified names. The model does not invoke any `

`functions,`

` but can rather describe how it would invoke a `

`function to complete a given task/query.`

` Required: Returns FunctionChoiceBehavior class `

`with auto_invoke enabled, and the desired functions`

` based on either the specified filters or the `

`full qualified names. The model is required to use `

`one of the`

` provided functions to complete a given `

`task/query.`



Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

PromptExecutionSettings

Base class for prompt execution settings.

Can be used by itself or as a base class for other prompt execution

settings. The methods are used to create specific prompt execution

settings objects based on the keys in the extension_data field, this way

you can create a generic PromptExecutionSettings object in your

application, which gets mapped into the keys of the prompt execution

settings that each services returns by using the

service.get_prompt_execution_settings() method.

Initialize the prompt execution settings.



**prompt_execution_settings Package**

Reference

**Modules**

anthropic_prompt_execution_settings

ﾉ

**Expand table**



**anthropic_prompt_execution_settings**

**Module**

Reference

**Classes**

AnthropicChatPromptExecutionSettings

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

AnthropicPromptExecutionSettings

Common request settings for Anthropic services.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**AnthropicChatPromptExecutionSettings**

**Class**

Reference

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`AnthropicChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`messages: list[dict[str, Any]] | ``None`` = ``None``, stream: bool | ``None`` = ``None``, `

`system: str | ``None`` = ``None``, max_tokens: Annotated[int, Gt(gt=0)] = 1024, `

`temperature: Annotated[float | ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, `

`stop_sequences: list[str] | ``None`` = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, top_k: Annotated[int | ``None``, Ge(ge=0)] = `

`None``, tools: list[dict[str, Any]] | ``None`` = ``None``, tool_choice: dict[str, str] `

`| ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**messages**`

Required*

`**stream**`

Required*

`**system**`

Required*

`**max_tokens**`

Default value: 1024

`**temperature**`

Required*

`**stop_sequences**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

validate_tool_choice

Validate tool choice. Anthropic doesn't support NONE tool choice.

**validate_tool_choice**

**Methods**

ﾉ

**Expand table**



Validate tool choice. Anthropic doesn't support NONE tool choice.

Python

**max_tokens**

Python

**messages**

Python

**stop_sequences**

Python

**stream**

Python

**system**

Python

`validate_tool_choice() -> AnthropicChatPromptExecutionSettings`

**Attributes**

`max_tokens: Annotated[int, FieldInfo(annotation=NoneType, required=``True``,
`

`metadata=[Gt(gt=0)])]`

`messages: list[dict[str, Any]] | ``None`

`stop_sequences: list[str] | ``None`

`stream: bool | ``None`

`system: str | ``None`



**temperature**

Python

**tool_choice**

Python

**tools**

Python

**top_k**

Python

**top_p**

Python

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`tool_choice: Annotated[dict[str, str] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`top_k: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**AnthropicPromptExecutionSettings**

**Class**

Reference

Common request settings for Anthropic services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`AnthropicPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**ai_model_id**`

Required*

**ai_model_id**

Python

**Attributes**

`ai_model_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'model'``)]`



**services Package**

Reference

**Modules**

utils

ﾉ

**Expand table**



**utils Module**

Reference

**kernel_function_metadata_to_function_call_format**

Convert the kernel function metadata to function calling format.

Python

**Parameters**

**Name**

**Description**

`**metadata**`

Required*

**update_settings_from_function_call_configuration**

Update the settings from a FunctionChoiceConfiguration.

Python

**Parameters**

**Name**

**Description**

`**function_choice_configuration**`

**Functions**

`kernel_function_metadata_to_function_call_format(metadata: `

`KernelFunctionMetadata) -> dict[str, Any]`

ﾉ

**Expand table**

`update_settings_from_function_call_configuration(function_choice_configur`

`ation: FunctionCallChoiceConfiguration, settings: `

`PromptExecutionSettings, type: FunctionChoiceType) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**settings**`

Required*

`**type**`

Required*



**services Package**

Reference



**services Package**

Reference

**Packages**

model_provider

**Modules**

bedrock_base

ﾉ

**Expand table**

ﾉ

**Expand table**



**model_provider Package**

Reference

**Modules**

bedrock_ai21_labs

bedrock_amazon_titan

bedrock_anthropic_claude

bedrock_cohere

bedrock_meta_llama

bedrock_mistralai

bedrock_model_provider

utils

ﾉ

**Expand table**



**bedrock_ai21_labs Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for AI21 Labs
models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

jamba.html

Note: We are not supporting multiple responses for now.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for AI21 Labs models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

jurassic2.html

Python

**Parameters**

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**parse_text_completion_response**

Parse the response from text completion for AI21 Labs models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`

ﾉ

**Expand table**



**bedrock_amazon_titan Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Amazon Titan

models.

Amazon Titan models do not support additional model request fields.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-

text.html

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Amazon Titan models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-

text.html

Python

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**get_text_embedding_request_body**

Get the request body for text embedding for Amazon Titan models.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**settings**`

Required*

**parse_streaming_text_completion_response**

Parse the response from streaming text completion for Amazon Titan models.

Python

**Parameters**

ﾉ

**Expand table**

`get_text_embedding_request_body(text: str, settings: `

`BedrockEmbeddingPromptExecutionSettings) -> dict[str, Any]`

ﾉ

**Expand table**

`parse_streaming_text_completion_response(chunk: dict[str, Any], model_id: `

`str) -> StreamingTextContent`



**Name**

**Description**

`**chunk**`

Required*

`**model_id**`

Required*

**parse_text_completion_response**

Parse the response from text completion for Amazon Titan models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

**parse_text_embedding_response**

Parse the response from text embedding for Amazon Titan models.

Python

**Parameters**

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`

ﾉ

**Expand table**

`parse_text_embedding_response(response: dict[str, Any]) -> list[float]`

ﾉ

**Expand table**



**Name**

**Description**

`**response**`

Required*



**bedrock_anthropic_claude Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Anthropic
Claude

models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

anthropic-claude-messages.html

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Anthropic Claude models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

anthropic-claude-text-completion.html

Python

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**parse_text_completion_response**

Parse the response from text completion for Anthropic Claude models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`

ﾉ

**Expand table**



**bedrock_cohere Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Cohere Command

models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-

command-r-plus.html

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Cohere Command models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-

command.html

Python

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**get_text_embedding_request_body**

Get the request body for text embedding for Cohere Command models.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**settings**`

Required*

**parse_text_completion_response**

Parse the response from text completion for Anthropic Claude models.

Python

**Parameters**

ﾉ

**Expand table**

`get_text_embedding_request_body(text: str, settings: `

`BedrockEmbeddingPromptExecutionSettings) -> Any`

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`



**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

**parse_text_embedding_response**

Parse the response from text embedding for Cohere Command models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

ﾉ

**Expand table**

`parse_text_embedding_response(response: dict[str, Any]) -> list[float]`

ﾉ

**Expand table**



**bedrock_meta_llama Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Meta Llama
models.

Meta Llama models do not support additional model request fields.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

meta.html

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Meta Llama models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-

meta.html

Python

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**parse_text_completion_response**

Parse the response from text completion for Meta Llama models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`

ﾉ

**Expand table**



**bedrock_mistralai Module**

Reference

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Mistral AI
models.

MMistral AI models do not support additional model request fields.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-

chat-completion.html

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Mistral AI models.

https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral-

text-completion.html

Python

**Functions**

`get_chat_completion_additional_model_request_fields(settings: `

`BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**

`get_text_completion_request_body(prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> Any`



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

`**settings**`

Required*

**parse_text_completion_response**

Parse the response from text completion for AI21 Labs models.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

`**model_id**`

Required*

ﾉ

**Expand table**

`parse_text_completion_response(response: dict[str, Any], model_id: str) -`

`> list[TextContent]`

ﾉ

**Expand table**



**bedrock_model_provider Module**

Reference

**Enums**

BedrockModelProvider

Amazon Bedrock Model Provider Enum.

This list contains the providers of all base models available on Amazon

Bedrock.

**get_chat_completion_additional_model_request_fields**

Get the additional model request fields for chat completion for Amazon Bedrock

models.

Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**settings**`

Required*

**get_text_completion_request_body**

Get the request body for text completion for Amazon Bedrock models.

ﾉ

**Expand table**

**Functions**

`get_chat_completion_additional_model_request_fields(model_id: str, `

`settings: BedrockChatPromptExecutionSettings) -> dict[str, Any] | ``None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**prompt**`

Required*

`**settings**`

Required*

**get_text_embedding_request_body**

Get the request body for text embedding for Amazon Bedrock models.

Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**text**`

Required*

`**settings**`

Required*

`get_text_completion_request_body(model_id: str, prompt: str, settings: `

`BedrockTextPromptExecutionSettings) -> dict`

ﾉ

**Expand table**

`get_text_embedding_request_body(model_id: str, text: str, settings: `

`BedrockEmbeddingPromptExecutionSettings) -> dict`

ﾉ

**Expand table**



**parse_streaming_text_completion_response**

Parse the response from streaming text completion for Amazon Bedrock models.

Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**chunk**`

Required*

**parse_text_completion_response**

Parse the response from text completion for Amazon Bedrock models.

Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**response**`

Required*

**parse_text_embedding_response**

`parse_streaming_text_completion_response(model_id: str, chunk: dict) -> `

`StreamingTextContent`

ﾉ

**Expand table**

`parse_text_completion_response(model_id: str, response: dict) -> `

`list[TextContent]`

ﾉ

**Expand table**



Parse the response from text embedding for Amazon Bedrock models.

Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

`**response**`

Required*

`parse_text_embedding_response(model_id: str, response: dict) -> `

`list[float]`

ﾉ

**Expand table**



**BedrockModelProvider Enum**

Reference

Amazon Bedrock Model Provider Enum.

This list contains the providers of all base models available on Amazon
Bedrock.

AI21LABS

AMAZON

ANTHROPIC

COHERE

META

MISTRALAI

**Fields**

ﾉ

**Expand table**



**utils Module**

Reference

**finish_reason_from_bedrock_to_semantic_kernel**

Convert a finish reason from Bedrock to Semantic Kernel.

Python

**Parameters**

**Name**

**Description**

`**finish_reason**`

Required*

**format_bedrock_function_name_to_kernel_function_fully_qualified_na**

Format the Bedrock function name to the kernel function fully qualified name.

Python

**Parameters**

**Name**

**Description**

`**bedrock_function_name**`

Required*

**remove_none_recursively**

Remove None values from a dictionary recursively.

**Functions**

`finish_reason_from_bedrock_to_semantic_kernel(finish_reason: str) -> FinishReason | `

`None`

ﾉ

**Expand table**

`format_bedrock_function_name_to_kernel_function_fully_qualified_name(bedrock_function_n`

`ame: str) -> str`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

`**max_depth**`

Required*

Default value: 5

**run_in_executor**

Run a function in an executor.

Python

**Parameters**

**Name**

**Description**

`**executor**`

Required*

`**func**`

Required*

**update_settings_from_function_choice_configuration**

Update the settings from a FunctionChoiceConfiguration.

Python

**Parameters**

`remove_none_recursively(data: dict, max_depth: int = 5) -> dict`

ﾉ

**Expand table**

`async`` run_in_executor(executor, func, *args, **kwargs)`

ﾉ

**Expand table**

`update_settings_from_function_choice_configuration(function_choice_configuration:
`

`FunctionCallChoiceConfiguration, settings: PromptExecutionSettings, type: `

`FunctionChoiceType) -> ``None`



**Name**

**Description**

`**function_choice_configuration**`

Required*

`**settings**`

Required*

`**type**`

Required*

ﾉ

**Expand table**



**bedrock_base Module**

Reference

**Classes**

BedrockBase

Amazon Bedrock Service Base Class.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BedrockBase Class**

Reference

Amazon Bedrock Service Base Class.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**bedrock_runtime_client**`

Required*

`**bedrock_client**`

Required*

get_foundation_model_info

Get the foundation model information.

**get_foundation_model_info**

Get the foundation model information.

`BedrockBase(*, bedrock_runtime_client: Any, bedrock_client: Any)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**model_id**`

Required*

**MODEL_PROVIDER_NAME**

Python

**bedrock_client**

Python

**bedrock_runtime_client**

Python

`async`` get_foundation_model_info(model_id: str) -> dict[str, Any]`

ﾉ

**Expand table**

**Attributes**

`MODEL_PROVIDER_NAME: ClassVar[str] = ``'bedrock'`

`bedrock_client: Any`

`bedrock_runtime_client: Any`



**embeddings Package**

Reference

**Modules**

embedding_generator_base

ﾉ

**Expand table**



**embedding_generator_base Module**

Reference

**Classes**

EmbeddingGeneratorBase

Base class for embedding generators.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**EmbeddingGeneratorBase Class**

Reference

Base class for embedding generators.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to form a

valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

generate_embeddings

Returns embeddings for the given texts as ndarray.

generate_raw_embeddings

Returns embeddings for the given texts in the unedited format.

This is not implemented for all embedding services, falling back to the

generate_embeddings method.

**generate_embeddings**

`EmbeddingGeneratorBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, strict=``None``, `

`min_length=1, max_length=``None``, pattern=``None``)], service_id: str =
``''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Returns embeddings for the given texts as ndarray.

Python

**Parameters**

**Name**

**Description**

`**texts**`

Required*

<xref:List>[str]

The texts to generate embeddings for.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.embeddings.embedding_generator_base.PromptExecutionSettings>

The settings to use for the request, optional.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments to pass to the request.

**generate_raw_embeddings**

Returns embeddings for the given texts in the unedited format.

This is not implemented for all embedding services, falling back to the
generate_embeddings

method.

Python

**Parameters**

**Name**

**Description**

`**texts**`

Required*

<xref:List>[str]

The texts to generate embeddings for.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.embeddings.embedding_generator_base.PromptExecutionSettings>

The settings to use for the request, optional.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments to pass to the request.

`abstract ``async`` generate_embeddings(texts: list[str], settings: `

`PromptExecutionSettings | ``None`` = ``None``, **kwargs: Any) -> ndarray`

ﾉ

**Expand table**

`async`` generate_raw_embeddings(texts: list[str], settings: PromptExecutionSettings | `

`None`` = ``None``, **kwargs: Any) -> Any`

ﾉ

**Expand table**



**ai_model_id**

Python

**is_experimental**

Python

**service_id**

Python

**stage_status**

Python

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``,
to_upper=``None``, `

`to_lower=``None``, strict=``None``, min_length=1, max_length=``None``,
pattern=``None``)]`

`is_experimental = ``True`

`service_id: str`

`stage_status = ``'experimental'`



**google Package**

Reference

**Modules**

shared_utils

ﾉ

**Expand table**



**services Package**

Reference



**services Package**

Reference



**google_ai_prompt_execution_settings**

**Module**

Reference

**Classes**

GoogleAIChatPromptExecutionSettings

Google AI Chat Prompt Execution Settings.

Initialize the prompt execution settings.

GoogleAIEmbeddingPromptExecutionSettings

Google AI Embedding Prompt Execution Settings.

Initialize the prompt execution settings.

GoogleAIPromptExecutionSettings

Google AI Prompt Execution Settings.

Initialize the prompt execution settings.

GoogleAITextPromptExecutionSettings

Google AI Text Prompt Execution Settings.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**GoogleAIChatPromptExecutionSettings**

**Class**

Reference

Google AI Chat Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`GoogleAIChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, stop_sequences: Annotated[list[str] | `

`None``, MaxLen(max_length=5)] = ``None``, response_mime_type: `

`Literal[``'text/plain'``, ``'application/json'``] | ``None`` = ``None``, response_schema: `

`Any | ``None`` = ``None``, candidate_count: Annotated[int | ``None``, Ge(ge=1)] = ``None``, `

`max_output_tokens: Annotated[int | ``None``, Ge(ge=1)] = ``None``, temperature: `

`Annotated[float | ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: float | ``None`` `

`= ``None``, top_k: int | ``None`` = ``None``, tools: list[dict[str, Any]] | ``None`` = ``None``, `

`tool_config: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**stop_sequences**`

Required*

`**response_mime_type**`

Required*

`**response_schema**`

Required*

`**candidate_count**`

Required*

`**max_output_tokens**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

`**tools**`

Required*

`**tool_config**`

Required*

prepare_settings_dict

Prepare the settings as a dictionary for sending to the AI service.

This method removes the tools and tool_choice keys from the settings

dictionary, as the Google AI service mandates these two settings to be

sent as separate parameters.

**Methods**

ﾉ

**Expand table**



**prepare_settings_dict**

Prepare the settings as a dictionary for sending to the AI service.

This method removes the tools and tool_choice keys from the settings
dictionary, as

the Google AI service mandates these two settings to be sent as separate

parameters.

Python

**tool_config**

Python

**tools**

Python

`prepare_settings_dict(**kwargs) -> dict[str, Any]`

**Attributes**

`tool_config: Annotated[dict[str, Any] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`



**GoogleAIEmbeddingPromptExecution**

**Settings Class**

Reference

Google AI Embedding Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`GoogleAIEmbeddingPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, output_dimensionality: Annotated[int | `

`None``, Le(le=768)] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**output_dimensionality**`

Required*

**output_dimensionality**

Python

**Attributes**

`output_dimensionality: Annotated[int | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=[Le(le=768)])]`



**GoogleAIPromptExecutionSettings Class**

Reference

Google AI Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`GoogleAIPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, stop_sequences: Annotated[list[str] | `

`None``, MaxLen(max_length=5)] = ``None``, response_mime_type: `

`Literal[``'text/plain'``, ``'application/json'``] | ``None`` = ``None``, response_schema: `

`Any | ``None`` = ``None``, candidate_count: Annotated[int | ``None``, Ge(ge=1)] = ``None``, `

`max_output_tokens: Annotated[int | ``None``, Ge(ge=1)] = ``None``, temperature: `

`Annotated[float | ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: float | ``None`` `

`= ``None``, top_k: int | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**stop_sequences**`

Required*

`**response_mime_type**`

Required*

`**response_schema**`

Required*

`**candidate_count**`

Required*

`**max_output_tokens**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

**candidate_count**

Python

**max_output_tokens**

Python

**Attributes**

`candidate_count: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`

`max_output_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`



**response_mime_type**

Python

**response_schema**

Python

**stop_sequences**

Python

**temperature**

Python

**top_k**

Python

**top_p**

Python

`response_mime_type: Literal[``'text/plain'``, ``'application/json'``] | ``None`

`response_schema: Any | ``None`

`stop_sequences: Annotated[list[str] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=`

`[MaxLen(max_length=5)])]`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_k: int | ``None`

`top_p: float | ``None`



**GoogleAITextPromptExecutionSettings**

**Class**

Reference

Google AI Text Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`GoogleAITextPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, stop_sequences: Annotated[list[str] | `

`None``, MaxLen(max_length=5)] = ``None``, response_mime_type: `

`Literal[``'text/plain'``, ``'application/json'``] | ``None`` = ``None``, response_schema: `

`Any | ``None`` = ``None``, candidate_count: Annotated[int | ``None``, Ge(ge=1)] = ``None``, `

`max_output_tokens: Annotated[int | ``None``, Ge(ge=1)] = ``None``, temperature: `

`Annotated[float | ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: float | ``None`` `

`= ``None``, top_k: int | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**stop_sequences**`

Required*

`**response_mime_type**`

Required*

`**response_schema**`

Required*

`**candidate_count**`

Required*

`**max_output_tokens**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

**candidate_count**

Python

**extension_data**

Python

**Attributes**

`candidate_count: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`

`extension_data: dict[str, Any]`



**function_choice_behavior**

Python

**max_output_tokens**

Python

**response_mime_type**

Python

**response_schema**

Python

**service_id**

Python

**stop_sequences**

Python

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`max_output_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`

`response_mime_type: Literal[``'text/plain'``, ``'application/json'``] | ``None`

`response_schema: Any | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`stop_sequences: Annotated[list[str] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=`

`[MaxLen(max_length=5)])]`



**temperature**

Python

**top_k**

Python

**top_p**

Python

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_k: int | ``None`

`top_p: float | ``None`



**shared_utils Module**

Reference

**filter_system_message**

Filter the first system message from the chat history.

If there are multiple system messages, raise an error. If there are no system
messages, return

None.

Python

**Parameters**

**Name**

**Description**

`**chat_history**`

Required*

**format_gemini_function_name_to_kernel_function_fully_qualified_na**

Format the Gemini function name to the kernel function fully qualified name.

Python

**Parameters**

**Name**

**Description**

`**gemini_function_name**`

Required*

**Functions**

`filter_system_message(chat_history: ChatHistory) -> str | ``None`

ﾉ

**Expand table**

`format_gemini_function_name_to_kernel_function_fully_qualified_name(gemini_function_n`

`ame: str) -> str`

ﾉ

**Expand table**



**services Package**

Reference



**prompt_execution_settings Package**

Reference



**services Package**

Reference



**onnx Package**

Reference

**Packages**

services

**Modules**

onnx_gen_ai_prompt_execution_settings

onnx_gen_ai_settings

utils

**Classes**

OnnxGenAIChatCompletion

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may

change in the future.

Initializes a new instance of the OnnxGenAITextCompletion

class.

OnnxGenAIPromptExecutionSettings

OnnxGenAI prompt execution settings.

Initialize the prompt execution settings.

OnnxGenAITextCompletion

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may

change in the future.

Initializes a new instance of the OnnxGenAITextCompletion

class.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Enums**

ONNXTemplate

ONNXTemplate is an enumeration that represents different ONNX model

templates.

ﾉ

**Expand table**



**services Package**

Reference

**Modules**

onnx_gen_ai_chat_completion

onnx_gen_ai_completion_base

onnx_gen_ai_text_completion

ﾉ

**Expand table**



**onnx_gen_ai_chat_completion Module**

Reference

**Classes**

OnnxGenAIChatCompletion

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the OnnxGenAITextCompletion class.

ﾉ

**Expand table**



**OnnxGenAIChatCompletion Class**

Reference

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OnnxGenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**template**`

Required*

The chat template configuration.

`**ai_model_path**`

Local path to the ONNX model Folder.

Default value: None

`**ai_model_id**`

The ID of the AI model. Defaults to None.

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Keyword-Only Parameters**

`OnnxGenAIChatCompletion(template: ONNXTemplate, ai_model_path: str | ``None`` = `

`None``, ai_model_id: str | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``, *, model: Any, tokenizer: Any, `

`tokenizer_stream: Any, enable_multi_modality: bool = ``False``, service_id:
str `

`= ``''``, instruction_role: str = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**model**`

Required*

`**tokenizer**`

Required*

`**tokenizer_stream**`

Required*

`**enable_multi_modality**`

Required*

`**service_id**`

Required*

`**instruction_role**`

Required*

get_prompt_execution_settings_class

Create a request settings object.

**get_prompt_execution_settings_class**

Create a request settings object.

Python

**SUPPORTS_FUNCTION_CALLING**

Python

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`SUPPORTS_FUNCTION_CALLING: ClassVar[bool] = ``False`



**ai_model_id**

Python

**enable_multi_modality**

Python

**instruction_role**

Python

**is_experimental**

Python

**model**

Python

**service_id**

Python

**stage_status**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`enable_multi_modality: bool`

`instruction_role: str`

`is_experimental = ``True`

`model: Any`

`service_id: str`



Python

**template**

Python

**tokenizer**

Python

**tokenizer_stream**

Python

`stage_status = ``'experimental'`

`template: ONNXTemplate`

`tokenizer: Any`

`tokenizer_stream: Any`



**onnx_gen_ai_completion_base Module**

Reference

**Classes**

OnnxGenAICompletionBase

Base class for OnnxGenAI Completion services.

Creates a new instance of the OnnxGenAICompletionBase class,

loads model & tokenizer.

ﾉ

**Expand table**



**OnnxGenAICompletionBase Class**

Reference

Base class for OnnxGenAI Completion services.

Creates a new instance of the OnnxGenAICompletionBase class, loads model &

tokenizer.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_path**`

Required*

Path to Onnx Model.

`****kwargs**`

Required*

Additional keyword arguments.

**Keyword-Only Parameters**

**Name**

**Description**

`**model**`

Required*

`**tokenizer**`

Required*

`**tokenizer_stream**`

Required*

`**enable_multi_modality**`

`OnnxGenAICompletionBase(ai_model_path: str, *, model: Any, tokenizer: Any, `

`tokenizer_stream: Any, enable_multi_modality: bool = ``False``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

**enable_multi_modality**

Python

**model**

Python

**tokenizer**

Python

**tokenizer_stream**

Python

**Attributes**

`enable_multi_modality: bool`

`model: Any`

`tokenizer: Any`

`tokenizer_stream: Any`



**onnx_gen_ai_text_completion Module**

Reference

**Classes**

OnnxGenAITextCompletion

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the OnnxGenAITextCompletion class.

ﾉ

**Expand table**



**OnnxGenAITextCompletion Class**

Reference

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OnnxGenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_path**`

Local path to the ONNX model Folder.

Default value: None

`**ai_model_id**`

The ID of the AI model. Defaults to None.

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file.

Default value: None

get_prompt_execution_settings_class

Create a request settings object.

`OnnxGenAITextCompletion(ai_model_path: str | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` `

`= ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**get_prompt_execution_settings_class**

Create a request settings object.

Python

**ai_model_id**

Python

**enable_multi_modality**

Python

**is_experimental**

Python

**model**

Python

**service_id**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`

`enable_multi_modality: bool`

`is_experimental = ``True`

`model: Any`



Python

**stage_status**

Python

**tokenizer**

Python

**tokenizer_stream**

Python

`service_id: str`

`stage_status = ``'experimental'`

`tokenizer: Any`

`tokenizer_stream: Any`



**onnx_gen_ai_prompt_execution_settings**

**Module**

Reference

**Classes**

OnnxGenAIPromptExecutionSettings

OnnxGenAI prompt execution settings.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**OnnxGenAIPromptExecutionSettings**

**Class**

Reference

OnnxGenAI prompt execution settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OnnxGenAIPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, diversity_penalty: Annotated[float | `

`None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, do_sample: bool = ``False``, `

`early_stopping: bool = ``True``, length_penalty: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, max_length: Annotated[int, Gt(gt=0)] =
3072, `

`min_length: Annotated[int | ``None``, Gt(gt=0)] = ``None``, no_repeat_ngram_size: `

`int = 0, num_beams: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`num_return_sequences: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`past_present_share_buffer: int = ``True``, repetition_penalty: Annotated[float | `

`None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, temperature: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=2.0)] = ``None``, top_k: Annotated[int | ``None``, Gt(gt=0)] = `

`None``, top_p: Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**diversity_penalty**`

Required*

`**do_sample**`

Required*

`**early_stopping**`

Default value: True

`**length_penalty**`

Required*

`**max_length**`

Default value: 3072

`**min_length**`

Required*

`**no_repeat_ngram_size**`

Required*

`**num_beams**`

Required*

`**num_return_sequences**`

Required*

`**past_present_share_buffer**`

Default value: True

`**repetition_penalty**`

Required*

`**temperature**`

Required*

`**top_k**`

Required*

`**top_p**`

Required*

ﾉ

**Expand table**

**Attributes**



**diversity_penalty**

Python

**do_sample**

Python

**early_stopping**

Python

**extension_data**

Python

**function_choice_behavior**

Python

**length_penalty**

Python

`diversity_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`do_sample: bool`

`early_stopping: bool`

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`length_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**max_length**

Python

**min_length**

Python

**no_repeat_ngram_size**

Python

**num_beams**

Python

**num_return_sequences**

Python

**past_present_share_buffer**

Python

`max_length: Annotated[int, FieldInfo(annotation=NoneType, required=``True``,
`

`metadata=[Gt(gt=0)])]`

`min_length: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`no_repeat_ngram_size: int`

`num_beams: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`num_return_sequences: Annotated[int | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=[Gt(gt=0)])]`

`past_present_share_buffer: int`



**repetition_penalty**

Python

**service_id**

Python

**temperature**

Python

**top_k**

Python

**top_p**

Python

`repetition_penalty: Annotated[float | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=[Ge(ge=0.0), `

`Le(le=1.0)])]`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_k: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**onnx_gen_ai_settings Module**

Reference

**Classes**

OnnxGenAISettings

Onnx Gen AI model settings.

The settings are first loaded from environment variables with the prefix

'>>ONNX_GEN_AI_<<'. If the environment variables are not found, the

settings can be loaded from a .env file with the encoding 'utf-8'. If the

settings are not found in the .env file, the settings are ignored; however,

validation will fail alerting that the settings are missing.

Optional settings for prefix '>>ONNX_GEN_AI_<<' are:

chat_model_folder: Path to the Onnx chat model folder (ENV:

ONNX_GEN_AI_CHAT_MODEL_FOLDER).

text_model_folder: Path to the Onnx text model folder (ENV:

ONNX_GEN_AI_TEXT_MODEL_FOLDER).

ﾉ

**Expand table**



**OnnxGenAISettings Class**

Reference

Onnx Gen AI model settings.

The settings are first loaded from environment variables with the prefix

'>>ONNX_GEN_AI_<<'. If the environment variables are not found, the settings
can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the .env

file, the settings are ignored; however, validation will fail alerting that
the settings are

missing.

Optional settings for prefix '>>ONNX_GEN_AI_<<' are:

chat_model_folder: Path to the Onnx chat model folder (ENV:

ONNX_GEN_AI_CHAT_MODEL_FOLDER).

text_model_folder: Path to the Onnx text model folder (ENV:

ONNX_GEN_AI_TEXT_MODEL_FOLDER).

**Constructor**

Python

**Parameters**

`OnnxGenAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`chat_model_folder: str | ``None`` = ``None``, text_model_folder: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**env_file_encoding**`

Default value: utf-8

`**chat_model_folder**`

Required*

`**text_model_folder**`

Required*

**chat_model_folder**

Python

**env_prefix**

Python

**text_model_folder**

Python

**Attributes**

`chat_model_folder: str | ``None`

`env_prefix: ClassVar[str] = ``'ONNX_GEN_AI_'`

`text_model_folder: str | ``None`



**utils Module**

Reference

**Enums**

ONNXTemplate

ONNXTemplate is an enumeration that represents different ONNX model

templates.

**apply_template**

Apply the specified ONNX template to the given chat history.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

<xref:semantic_kernel.connectors.ai.onnx.utils.ChatHistory>

The chat history to which the template will be applied.

`**template**`

Required*

ONNXTemplate

The ONNX template to apply.

**Returns**

**Type**

**Description**

str

The result of applying the template to the chat history.

ﾉ

**Expand table**

**Functions**

`apply_template(history: ChatHistory, template: ONNXTemplate) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

ServiceException

If an error occurs while applying the template.

**gemma_template**

Generates a formatted string for the Gemma model based on the provided chat

history.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

<xref:semantic_kernel.connectors.ai.onnx.utils.ChatHistory>

An object containing the chat history with messages.

**Returns**

**Type**

**Description**

str

A formatted string representing the chat history for the Gemma model.

**Exceptions**

**Type**

**Description**

ServiceInvalidRequestError

If a system message is encountered in the chat history.

ﾉ

**Expand table**

`gemma_template(history: ChatHistory) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**llama_template**

Generates a formatted string from a given chat history for use with the LLaMA

model.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

<xref:semantic_kernel.connectors.ai.onnx.utils.ChatHistory>

An object containing the chat history, which includes a list of messages.

**Returns**

**Type**

**Description**

str

A formatted string where each message is wrapped with specific header and end
tags,

and the final string ends with an assistant header tag.

**Exceptions**

**Type**

**Description**

ServiceException

If an error occurs while applying the template.

**phi3_template**

Generates a formatted string from the chat history for use with the phi3
model.

Python

`llama_template(history: ChatHistory) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**history**`

Required*

<xref:semantic_kernel.connectors.ai.onnx.utils.ChatHistory>

An object containing the chat history with a list of messages.

**Returns**

**Type**

**Description**

str

A formatted string where each message is prefixed with the role and suffixed
with an

end marker.

**Exceptions**

**Type**

**Description**

ServiceException

If an error occurs while applying the template.

**phi3v_template**

Generates a formatted string from a given chat history for use with the phi3v
model.

Python

**Parameters**

`phi3_template(history: ChatHistory) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

`phi3v_template(history: ChatHistory) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**history**`

Required*

<xref:semantic_kernel.connectors.ai.onnx.utils.ChatHistory>

An object containing the chat history with messages.

**Returns**

**Type**

**Description**

str

A formatted string representing the chat history, with special tokens
indicating the

role of each message (system, user, assistant) and the type of content (text,
image).

**Exceptions**

**Type**

**Description**

ServiceException

If an error occurs while applying the template.

ﾉ

**Expand table**

ﾉ

**Expand table**



**ONNXTemplate Enum**

Reference

ONNXTemplate is an enumeration that represents different ONNX model templates.

PHI3

PHI3V

GEMMA

LLAMA

NONE

**Fields**

ﾉ

**Expand table**



**ONNXTemplate Enum**

Reference

ONNXTemplate is an enumeration that represents different ONNX model templates.

PHI3

PHI3V

GEMMA

LLAMA

NONE

**Fields**

ﾉ

**Expand table**



**OnnxGenAIChatCompletion Class**

Reference

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OnnxGenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**template**`

Required*

The chat template configuration.

`**ai_model_path**`

Local path to the ONNX model Folder.

Default value: None

`**ai_model_id**`

The ID of the AI model. Defaults to None.

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Keyword-Only Parameters**

`OnnxGenAIChatCompletion(template: ONNXTemplate, ai_model_path: str | ``None`` = `

`None``, ai_model_id: str | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``, *, model: Any, tokenizer: Any, `

`tokenizer_stream: Any, enable_multi_modality: bool = ``False``, service_id:
str `

`= ``''``, instruction_role: str = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**model**`

Required*

`**tokenizer**`

Required*

`**tokenizer_stream**`

Required*

`**enable_multi_modality**`

Required*

`**service_id**`

Required*

`**instruction_role**`

Required*

get_prompt_execution_settings_class

Create a request settings object.

**get_prompt_execution_settings_class**

Create a request settings object.

Python

**SUPPORTS_FUNCTION_CALLING**

Python

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`SUPPORTS_FUNCTION_CALLING: ClassVar[bool] = ``False`



**is_experimental**

Python

**stage_status**

Python

**template**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`

`template: ONNXTemplate`



**OnnxGenAIPromptExecutionSettings**

**Class**

Reference

OnnxGenAI prompt execution settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OnnxGenAIPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, diversity_penalty: Annotated[float | `

`None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, do_sample: bool = ``False``, `

`early_stopping: bool = ``True``, length_penalty: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, max_length: Annotated[int, Gt(gt=0)] =
3072, `

`min_length: Annotated[int | ``None``, Gt(gt=0)] = ``None``, no_repeat_ngram_size: `

`int = 0, num_beams: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`num_return_sequences: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`past_present_share_buffer: int = ``True``, repetition_penalty: Annotated[float | `

`None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, temperature: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=2.0)] = ``None``, top_k: Annotated[int | ``None``, Gt(gt=0)] = `

`None``, top_p: Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**diversity_penalty**`

Required*

`**do_sample**`

Required*

`**early_stopping**`

Default value: True

`**length_penalty**`

Required*

`**max_length**`

Default value: 3072

`**min_length**`

Required*

`**no_repeat_ngram_size**`

Required*

`**num_beams**`

Required*

`**num_return_sequences**`

Required*

`**past_present_share_buffer**`

Default value: True

`**repetition_penalty**`

Required*

`**temperature**`

Required*

`**top_k**`

Required*

`**top_p**`

Required*

ﾉ

**Expand table**

**Attributes**



**diversity_penalty**

Python

**do_sample**

Python

**early_stopping**

Python

**length_penalty**

Python

**max_length**

Python

**min_length**

Python

`diversity_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`do_sample: bool`

`early_stopping: bool`

`length_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`max_length: Annotated[int, FieldInfo(annotation=NoneType, required=``True``,
`

`metadata=[Gt(gt=0)])]`

`min_length: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`



**no_repeat_ngram_size**

Python

**num_beams**

Python

**num_return_sequences**

Python

**past_present_share_buffer**

Python

**repetition_penalty**

Python

**temperature**

Python

`no_repeat_ngram_size: int`

`num_beams: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`num_return_sequences: Annotated[int | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=[Gt(gt=0)])]`

`past_present_share_buffer: int`

`repetition_penalty: Annotated[float | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, metadata=[Ge(ge=0.0), `

`Le(le=1.0)])]`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `



**top_k**

Python

**top_p**

Python

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_k: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**OnnxGenAITextCompletion Class**

Reference

OnnxGenAI text completion service.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OnnxGenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_path**`

Local path to the ONNX model Folder.

Default value: None

`**ai_model_id**`

The ID of the AI model. Defaults to None.

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file.

Default value: None

get_prompt_execution_settings_class

Create a request settings object.

`OnnxGenAITextCompletion(ai_model_path: str | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` `

`= ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**get_prompt_execution_settings_class**

Create a request settings object.

Python

**ai_model_id**

Python

**is_experimental**

Python

**service_id**

Python

**stage_status**

Python

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`

`is_experimental = ``True`

`service_id: str`

`stage_status = ``'experimental'`



**open_ai Package**

Reference

**Packages**

exceptions

prompt_execution_settings

services

settings

**Modules**

const

**Classes**

ApiKeyAuthentication

API key authentication.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureAISearchDataSource

Azure AI Search data source.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



AzureAISearchDataSourceParameters

Azure AI Search data source parameters.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureAudioToText

Azure audio to text service.

Initialize an AzureAudioToText service.

AzureChatCompletion

Azure Chat completion class.

Initialize an AzureChatCompletion service.

AzureChatPromptExecutionSettings

Specific settings for the Azure OpenAI Chat Completion

endpoint.

Initialize the prompt execution settings.

AzureCosmosDBDataSource

Azure Cosmos DB data source.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureCosmosDBDataSourceParameters

Azure Cosmos DB data source parameters.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureDataSourceParameters

Azure data source parameters.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureEmbeddingDependency

Azure embedding dependency.



Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AzureOpenAISettings

AzureOpenAI model settings.

The settings are first loaded from environment variables with

the prefix '>>AZURE_OPENAI_<<'. If the environment

variables are not found, the settings can be loaded from a

.env file with the encoding 'utf-8'. If the settings are not found

in the .env file, the settings are ignored; however, validation

will fail alerting that the settings are missing.

Optional settings for prefix '>>AZURE_OPENAI_<<' are:

chat_deployment_name: str - The name of the Azure

Chat deployment. This value

will correspond to the custom name you chose for your

deployment when you deployed a model. This value

can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)

text_deployment_name: str - The name of the Azure

Text deployment. This value

will correspond to the custom name you chose for your

deployment when you deployed a model. This value

can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var AZURE_OPENAI_TEXT_DEPLOYMENT_NAME)

embedding_deployment_name: str - The name of the

Azure Embedding deployment. This value

will correspond to the custom name you chose for your

deployment when you deployed a model. This value

can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var

AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME)

text_to_image_deployment_name: str - The name of the

Azure Text to Image deployment. This

value will correspond to the custom name you chose

for your deployment when you deployed a model. This



value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var

AZURE_OPENAI_TEXT_TO_IMAGE_DEPLOYMENT_NAME)

audio_to_text_deployment_name: str - The name of the

Azure Audio to Text deployment. This

value will correspond to the custom name you chose

for your deployment when you deployed a model. This

value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var

AZURE_OPENAI_AUDIO_TO_TEXT_DEPLOYMENT_NAME)

text_to_audio_deployment_name: str - The name of the

Azure Text to Audio deployment. This

value will correspond to the custom name you chose

for your deployment when you deployed a model. This

value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under

Management > Deployments in Azure OpenAI Studio.

(Env var

AZURE_OPENAI_TEXT_TO_AUDIO_DEPLOYMENT_NAME)

api_key: SecretStr - The API key for the Azure

deployment. This value can be

found in the Keys & Endpoint section when examining

your resource in the Azure portal. You can use either

KEY1 or KEY2. (Env var AZURE_OPENAI_API_KEY)

base_url: HttpsUrl | None - base_url: The url of the

Azure deployment. This value

can be found in the Keys & Endpoint section when

examining your resource from the Azure portal, the

base_url consists of the endpoint, followed by

/openai/deployments/{deployment_name}/, use

endpoint if you only want to supply the endpoint. (Env

var AZURE_OPENAI_BASE_URL)

endpoint: HttpsUrl - The endpoint of the Azure

deployment. This value

can be found in the Keys & Endpoint section when

examining your resource from the Azure portal, the

endpoint should end in openai.azure.com. If both

base_url and endpoint are supplied, base_url will be

used. (Env var AZURE_OPENAI_ENDPOINT)

api_version: str | None - The API version to use. The

default value is "2024-02-01".

(Env var AZURE_OPENAI_API_VERSION)



token_endpoint: str - The token endpoint to use to

retrieve the authentication token.

The default value is

"https://cognitiveservices.azure.com/.default

". (Env

var AZURE_OPENAI_TOKEN_ENDPOINT)

AzureTextCompletion

Azure Text Completion class.

Initialize an AzureTextCompletion service.

AzureTextEmbedding

Azure Text Embedding class.

Note: This class is marked as 'experimental' and may change

in the future.

Initialize an AzureTextEmbedding service.

service_id: The service ID. (Optional) api_key: The optional api

key. If provided, will override the value in the

env vars or .env file.

deployment_name: The optional deployment. If provided, will

override the value (text_deployment_name) in the env vars or

.env file.

endpoint: The optional deployment endpoint. If provided will

override the value in the env vars or .env file.

base_url: The optional deployment base_url. If provided will

override the value in the env vars or .env file.

api_version: The optional deployment api version. If provided

will override the value in the env vars or .env file.

ad_token: The Azure AD token for authentication. (Optional)

ad_token_provider: Whether to use Azure Active Directory

authentication.

(Optional) The default value is False.

token_endpoint: The Azure AD token endpoint. (Optional)

default_headers: The default headers mapping of string keys

to

string values for HTTP requests. (Optional)

async_client (Optional[AsyncAzureOpenAI]): An existing client

to use. (Optional) env_file_path (str | None): Use the

environment settings file as a fallback to

environment variables. (Optional)



AzureTextToAudio

Azure text to audio service.

Initialize an AzureTextToAudio service.

AzureTextToImage

Azure Text to Image service.

Initialize an AzureTextToImage service.

ConnectionStringAuthentication

Connection string authentication.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

DataSourceFieldsMapping

Data source fields mapping.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ExtraBody

Extra body for the Azure Chat Completion endpoint.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

OpenAIAudioToText

OpenAI Text to Image service.

Initializes a new instance of the OpenAIAudioToText class.

OpenAIAudioToTextExecutionSettings

Request settings for OpenAI audio to text services.

Initialize the prompt execution settings.

OpenAIChatCompletion

OpenAI Chat completion class.

Initialize an OpenAIChatCompletion service.

OpenAIChatPromptExecutionSettings

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

OpenAIEmbeddingPromptExecutionSettings

Specific settings for the text embedding endpoint.



Initialize the prompt execution settings.

OpenAIPromptExecutionSettings

Common request settings for (Azure) OpenAI services.

Initialize the prompt execution settings.

OpenAISettings

OpenAI model settings.

The settings are first loaded from environment variables with

the prefix '>>OPENAI_<<'. If the environment variables are

not found, the settings can be loaded from a .env file with the

encoding 'utf-8'. If the settings are not found in the .env file,

the settings are ignored; however, validation will fail alerting

that the settings are missing.

Optional settings for prefix '>>OPENAI_<<' are:

api_key: SecretStr - OpenAI API key, see

https://platform.openai.com/account/api-keys

(Env var OPENAI_API_KEY)

org_id: str | None - This is usually optional unless your

account belongs to multiple organizations.

(Env var OPENAI_ORG_ID)

chat_model_id: str | None - The OpenAI chat model ID

to use, for example, gpt-3.5-turbo or gpt-4.

(Env var OPENAI_CHAT_MODEL_ID)

text_model_id: str | None - The OpenAI text model ID to

use, for example, gpt-3.5-turbo-instruct.

(Env var OPENAI_TEXT_MODEL_ID)

embedding_model_id: str | None - The OpenAI

embedding model ID to use, for example, text-

embedding-ada-002.

(Env var OPENAI_EMBEDDING_MODEL_ID)

text_to_image_model_id: str | None - The OpenAI text

to image model ID to use, for example, dall-e-3.

(Env var OPENAI_TEXT_TO_IMAGE_MODEL_ID)

audio_to_text_model_id: str | None - The OpenAI audio

to text model ID to use, for example, whisper-1.

(Env var OPENAI_AUDIO_TO_TEXT_MODEL_ID)

text_to_audio_model_id: str | None - The OpenAI text to

audio model ID to use, for example, jukebox-1.

(Env var OPENAI_TEXT_TO_AUDIO_MODEL_ID)

env_file_path: str | None - if provided, the .env settings

are read from this file path location



OpenAITextCompletion

OpenAI Text Completion class.

Initialize an OpenAITextCompletion service.

OpenAITextEmbedding

OpenAI Text Embedding class.

Note: This class is marked as 'experimental' and may change

in the future.

Initializes a new instance of the OpenAITextCompletion class.

OpenAITextPromptExecutionSettings

Specific settings for the completions endpoint.

Initialize the prompt execution settings.

OpenAITextToAudio

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToAudio class.

OpenAITextToAudioExecutionSettings

Request settings for OpenAI text to audio services.

Initialize the prompt execution settings.

OpenAITextToImage

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToImage class.

OpenAITextToImageExecutionSettings

Request settings for OpenAI text to image services.

Initialize the prompt execution settings.



**exceptions Package**

Reference

**Modules**

content_filter_ai_exception

ﾉ

**Expand table**



**content_filter_ai_exception Module**

Reference

**Classes**

ContentFilterAIException

AI exception for an error from Azure OpenAI's content filter.

Initializes a new instance of the ContentFilterAIException class.

ContentFilterResult

The result of a content filter check.

**Enums**

ContentFilterCodes

Content filter codes.

ContentFilterResultSeverity

The severity of the content filter result.

ﾉ

**Expand table**

ﾉ

**Expand table**



**ContentFilterAIException Class**

Reference

AI exception for an error from Azure OpenAI's content filter.

Initializes a new instance of the ContentFilterAIException class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

str

The error message.

`**inner_exception**`

Required*

Exception

The inner exception.

**content_filter_code**

Python

**content_filter_result**

Python

`ContentFilterAIException(message: str, inner_exception: BadRequestError)`

ﾉ

**Expand table**

**Attributes**

`content_filter_code: ContentFilterCodes`

`content_filter_result: dict[str, ContentFilterResult]`



**param**

Python

`param: str | ``None`



**ContentFilterCodes Enum**

Reference

Content filter codes.

RESPONSIBLE_AI_POLICY_VIOLATION

**Fields**

ﾉ

**Expand table**



**ContentFilterResult Class**

Reference

The result of a content filter check.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**filtered**`

Default value: False

`**detected**`

Default value: False

`**severity**`

Default value: ContentFilterResultSeverity.SAFE

from_inner_error_result

Creates a ContentFilterResult from the inner error results.

**from_inner_error_result**

Creates a ContentFilterResult from the inner error results.

Python

`ContentFilterResult(filtered: bool = ``False``, detected: bool = ``False``, `

`severity: ContentFilterResultSeverity = ContentFilterResultSeverity.SAFE)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`from_inner_error_result(inner_error_results: dict[str, Any]) -> `

`ContentFilterResult`



**Parameters**

**Name**

**Description**

`**key**`

Required*

str

The key to get the inner error result from.

`**inner_error_results**`

Required*

<xref:Dict>[str,<xref: Any>]

The inner error results.

**Returns**

**Type**

**Description**

ContentFilterResult

The ContentFilterResult.

**detected**

Python

**filtered**

Python

**severity**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`detected: bool = ``False`

`filtered: bool = ``False`

`severity: ContentFilterResultSeverity = ``'safe'`



**ContentFilterResultSeverity Enum**

Reference

The severity of the content filter result.

HIGH

LOW

MEDIUM

SAFE

**Fields**

ﾉ

**Expand table**



**prompt_execution_settings Package**

Reference

**Modules**

azure_chat_prompt_execution_settings

open_ai_audio_to_text_execution_settings

open_ai_prompt_execution_settings

open_ai_text_to_audio_execution_settings

open_ai_text_to_image_execution_settings

ﾉ

**Expand table**



**azure_chat_prompt_execution_settings**

**Module**

Reference

**Classes**

AccessTokenAuthentication

Access token authentication.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

ApiKeyAuthentication

API key authentication.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureAISearchDataSource

Azure AI Search data source.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureAISearchDataSourceParameters

Azure AI Search data source parameters.

Create a new model by parsing and validating

input data from keyword arguments.

ﾉ

**Expand table**



Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureChatPromptExecutionSettings

Specific settings for the Azure OpenAI Chat

Completion endpoint.

Initialize the prompt execution settings.

AzureChatRequestBase

Base class for Azure Chat requests.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureCosmosDBDataSource

Azure Cosmos DB data source.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureCosmosDBDataSourceParameters

Azure Cosmos DB data source parameters.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureDataSourceParameters

Azure data source parameters.

Create a new model by parsing and validating

input data from keyword arguments.



Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

AzureEmbeddingDependency

Azure embedding dependency.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

ConnectionStringAuthentication

Connection string authentication.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

DataSourceFieldsMapping

Data source fields mapping.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

ExtraBody

Extra body for the Azure Chat Completion

endpoint.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.



_self_ is explicitly positional-only to allow _self_ as a

field name.

SystemAssignedManagedIdentityAuthentication

System assigned managed identity

authentication.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.

UserAssignedManagedIdentityAuthentication

User assigned managed identity authentication.

Create a new model by parsing and validating

input data from keyword arguments.

Raises [_ValidationError_]

[pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a

field name.



**AccessTokenAuthentication Class**

Reference

Access token authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: access_token

`**accessToken**`

Required*

**access_token**

Python

`AccessTokenAuthentication(*, type: Annotated[Literal[``'AccessToken'``, `

`'access_token'``], AfterValidator(func=to_snake)] = ``'access_token'``, `

`accessToken: str | ``None``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`access_token: str | ``None`



**type**

Python

`type: Annotated[Literal[``'AccessToken'``, ``'access_token'``], `

`AfterValidator(func=to_snake)]`



**ApiKeyAuthentication Class**

Reference

API key authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: api_key

`**key**`

Required*

**key**

Python

**type**

`ApiKeyAuthentication(*, type: Annotated[Literal[``'APIKey'``, ``'api_key'``],
`

`AfterValidator(func=to_snake)] = ``'api_key'``, key: str, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`key: str`



Python

`type: Annotated[Literal[``'APIKey'``, ``'api_key'``], `

`AfterValidator(func=to_snake)]`



**AzureAISearchDataSource Class**

Reference

Azure AI Search data source.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: azure_search

`**parameters**`

Required*

from_azure_ai_search_settings

Create an instance from Azure AI Search settings.

**from_azure_ai_search_settings**

Create an instance from Azure AI Search settings.

`AzureAISearchDataSource(*, type: Literal[``'azure_search'``] =
``'azure_search'``, `

`parameters: Annotated[dict, AzureAISearchDataSourceParameters], `

`**extra_data: Any)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**azure_ai_search_settings**`

Required*

**parameters**

Python

**type**

Python

`from_azure_ai_search_settings(azure_ai_search_settings: `

`AzureAISearchSettings, **kwargs: Any)`

ﾉ

**Expand table**

**Attributes**

`parameters: Annotated[dict, AzureAISearchDataSourceParameters]`

`type: Literal[``'azure_search'``]`



**AzureAISearchDataSourceParameters**

**Class**

Reference

Azure AI Search data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

`AzureAISearchDataSourceParameters(*, indexName: str, indexLanguage: str | `

`None`` = ``None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: `

`bool | ``None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: `

`str | ``None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = `

`None``, strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | `

`None`` = ``None``, endpoint: str | ``None`` = ``None``, queryType: `

`Annotated[Literal[``'simple'``, ``'semantic'``, ``'vector'``,
``'vectorSimpleHybrid'``, `

`'vectorSemanticHybrid'``], AfterValidator(func=to_snake)] = ``'simple'``, `

`authentication: ApiKeyAuthentication | `

`SystemAssignedManagedIdentityAuthentication | `

`UserAssignedManagedIdentityAuthentication | AccessTokenAuthentication | ``None`` `

`= ``None``, **extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**inScope**`

Default value: True

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

`**endpoint**`

Required*

`**queryType**`

Default value: simple

`**authentication**`

Required*

**authentication**

Python

**embedding_dependency**

Python

**Attributes**

`authentication: ApiKeyAuthentication | `

`SystemAssignedManagedIdentityAuthentication | `

`UserAssignedManagedIdentityAuthentication | AccessTokenAuthentication | `

`None`

`embedding_dependency: AzureEmbeddingDependency | ``None`



**endpoint**

Python

**fields_mapping**

Python

**filter**

Python

**in_scope**

Python

**index_language**

Python

**index_name**

Python

**query_type**

`endpoint: str | ``None`

`fields_mapping: DataSourceFieldsMapping | ``None`

`filter: str | ``None`

`in_scope: bool | ``None`

`index_language: str | ``None`

`index_name: str`



Python

**role_information**

Python

**semantic_configuration**

Python

**strictness**

Python

**top_n_documents**

Python

`query_type: Annotated[Literal[``'simple'``, ``'semantic'``, ``'vector'``, `

`'vectorSimpleHybrid'``, ``'vectorSemanticHybrid'``], `

`AfterValidator(func=to_snake)]`

`role_information: str | ``None`

`semantic_configuration: str | ``None`

`strictness: int`

`top_n_documents: int | ``None`



**AzureChatPromptExecutionSettings**

**Class**

Reference

Specific settings for the Azure OpenAI Chat Completion endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Any

`AzureChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, response_format: `

`dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | dict[str, Any] | `

`type[BaseModel] | type | ``None`` = ``None``, function_call: str | ``None`` = ``None``, `

`functions: list[dict[str, Any]] | ``None`` = ``None``, messages: list[dict[str, `

`Any]] | ``None`` = ``None``, parallel_tool_calls: bool | ``None`` = ``None``, tools: `

`list[dict[str, Any]] | ``None`` = ``None``, tool_choice: str | ``None`` = ``None``, `

`structured_json_response: bool = ``False``, stream_options: dict[str, Any] | `

`None`` = ``None``, max_completion_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`reasoning_effort: Literal[``'low'``, ``'medium'``, ``'high'``] | ``None`` = ``None``, `

`extra_body: dict[str, Any] | ExtraBody | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**store**`

Required*

`**metadata**`

Required*

`**response_format**`

Required*

`**function_call**`

Required*

`**functions**`

Required*

`**messages**`

Required*

`**parallel_tool_calls**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

`**structured_json_response**`

Required*

`**stream_options**`

Required*

`**max_completion_tokens**`

Required*

`**reasoning_effort**`

Required*

`**extra_body**`

Required*

**ai_model_id**

**Attributes**



Python

**extension_data**

Python

**extra_body**

Python

**frequency_penalty**

Python

**function_call**

Python

**function_choice_behavior**

Python

**functions**

Python

`ai_model_id: Annotated[str | ``None``, Field(serialization_alias=``'model'``)]`

`extension_data: dict[str, Any]`

`extra_body: dict[str, Any] | ExtraBody | ``None`

`frequency_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`function_call: str | ``None`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`



**logit_bias**

Python

**max_completion_tokens**

Python

**max_tokens**

Python

**messages**

Python

**metadata**

Python

**number_of_responses**

Python

`functions: list[dict[str, Any]] | ``None`

`logit_bias: dict[str | int, float] | ``None`

`max_completion_tokens: Annotated[int | ``None``, Field(gt=0, description=``'A `

`maximum limit on total tokens for completion, including both output and `

`reasoning tokens.'``)]`

`max_tokens: Annotated[int | ``None``, Field(gt=0)]`

`messages: Annotated[list[dict[str, Any]] | ``None``, Field(description=``'Do `

`not set this manually. It is set by the service.'``)]`

`metadata: dict[str, str] | ``None`



**parallel_tool_calls**

Python

**presence_penalty**

Python

**reasoning_effort**

Python

**response_format**

Python

**seed**

Python

**service_id**

`number_of_responses: Annotated[int | ``None``, Field(ge=1, le=128, `

`serialization_alias=``'n'``)]`

`parallel_tool_calls: bool | ``None`

`presence_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`reasoning_effort: Annotated[Literal[``'low'``, ``'medium'``, ``'high'``] | ``None``, `

`Field(description=``'Adjusts reasoning effort (low/medium/high). Lower `

`values reduce response time and token usage.'``)]`

`response_format: dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | `

`dict[str, Any] | type[BaseModel] | type | ``None`

`seed: int | ``None`



Python

**stop**

Python

**store**

Python

**stream**

Python

**stream_options**

Python

**structured_json_response**

Python

**temperature**

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`stop: str | list[str] | ``None`

`store: bool | ``None`

`stream: bool`

`stream_options: Annotated[dict[str, Any] | ``None``, `

`Field(description=``'Additional options to pass when streaming is used. Do `

`not set this manually.'``)]`

`structured_json_response: Annotated[bool, Field(description=``'Do not set `

`this manually. It is set by the service.'``)]`



Python

**tool_choice**

Python

**tools**

Python

**top_p**

Python

**user**

Python

`temperature: Annotated[float | ``None``, Field(ge=0.0, le=2.0)]`

`tool_choice: Annotated[str | ``None``, Field(description=``'Do not set this `

`manually. It is set by the service based on the function choice `

`configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, Field(description=``'Do not `

`set this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`top_p: Annotated[float | ``None``, Field(ge=0.0, le=1.0)]`

`user: str | ``None`



**AzureChatRequestBase Class**

Reference

Base class for Azure Chat requests.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

`AzureChatRequestBase(**extra_data: Any)`



**AzureCosmosDBDataSource Class**

Reference

Azure Cosmos DB data source.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: azure_cosmos_db

`**parameters**`

Required*

**parameters**

Python

`AzureCosmosDBDataSource(*, type: Literal[``'azure_cosmos_db'``] = `

`'azure_cosmos_db'``, parameters: AzureCosmosDBDataSourceParameters, `

`**extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`parameters: AzureCosmosDBDataSourceParameters`



**type**

Python

`type: Literal[``'azure_cosmos_db'``]`



**AzureCosmosDBDataSourceParameters**

**Class**

Reference

Azure Cosmos DB data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

Required*

`**inScope**`

Default value: True

`AzureCosmosDBDataSourceParameters(*, indexName: str, indexLanguage: str | `

`None`` = ``None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: `

`bool | ``None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: `

`str | ``None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = `

`None``, strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | `

`None`` = ``None``, authentication: ConnectionStringAuthentication | ``None`` = ``None``, `

`databaseName: str | ``None`` = ``None``, containerName: str | ``None`` = ``None``, `

`embeddingDependencyType: AzureEmbeddingDependency | ``None`` = ``None``, `

`**extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

`**authentication**`

Required*

`**databaseName**`

Required*

`**containerName**`

Required*

`**embeddingDependencyType**`

Required*

**authentication**

Python

**container_name**

Python

**database_name**

**Attributes**

`authentication: ConnectionStringAuthentication | ``None`

`container_name: str | ``None`



Python

**embedding_dependency**

Python

**embedding_dependency_type**

Python

**fields_mapping**

Python

**filter**

Python

**in_scope**

Python

**index_language**

Python

`database_name: str | ``None`

`embedding_dependency: AzureEmbeddingDependency | ``None`

`embedding_dependency_type: AzureEmbeddingDependency | ``None`

`fields_mapping: DataSourceFieldsMapping | ``None`

`filter: str | ``None`

`in_scope: bool | ``None`



**index_name**

Python

**role_information**

Python

**semantic_configuration**

Python

**strictness**

Python

**top_n_documents**

Python

`index_language: str | ``None`

`index_name: str`

`role_information: str | ``None`

`semantic_configuration: str | ``None`

`strictness: int`

`top_n_documents: int | ``None`



**AzureDataSourceParameters Class**

Reference

Azure data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

Required*

`**inScope**`

Default value: True

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`AzureDataSourceParameters(*, indexName: str, indexLanguage: str | ``None`` = `

`None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: bool | `

`None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: str | `

`None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = ``None``, `

`strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | ``None`` = `

`None``, **extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

**embedding_dependency**

Python

**fields_mapping**

Python

**filter**

Python

**in_scope**

Python

**index_language**

**Attributes**

`embedding_dependency: AzureEmbeddingDependency | ``None`

`fields_mapping: DataSourceFieldsMapping | ``None`

`filter: str | ``None`

`in_scope: bool | ``None`



Python

**index_name**

Python

**role_information**

Python

**semantic_configuration**

Python

**strictness**

Python

**top_n_documents**

Python

`index_language: str | ``None`

`index_name: str`

`role_information: str | ``None`

`semantic_configuration: str | ``None`

`strictness: int`

`top_n_documents: int | ``None`



**AzureEmbeddingDependency Class**

Reference

Azure embedding dependency.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: deployment_name

`**deploymentName**`

Required*

**deployment_name**

Python

`AzureEmbeddingDependency(*, type: Annotated[Literal[``'DeploymentName'``, `

`'deployment_name'``], AfterValidator(func=to_snake)] = ``'deployment_name'``,
`

`deploymentName: str | ``None`` = ``None``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`deployment_name: str | ``None`



**type**

Python

`type: Annotated[Literal[``'DeploymentName'``, ``'deployment_name'``], `

`AfterValidator(func=to_snake)]`



**ConnectionStringAuthentication Class**

Reference

Connection string authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: connection_string

`**connectionString**`

Required*

**connection_string**

Python

`ConnectionStringAuthentication(*, type: `

`Annotated[Literal[``'ConnectionString'``, ``'connection_string'``], `

`AfterValidator(func=to_snake)] = ``'connection_string'``, connectionString:
str `

`| ``None`` = ``None``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`connection_string: str | ``None`



**type**

Python

`type: Annotated[Literal[``'ConnectionString'``, ``'connection_string'``], `

`AfterValidator(func=to_snake)]`



**DataSourceFieldsMapping Class**

Reference

Data source fields mapping.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**titleField**`

Required*

`**urlField**`

Required*

`**filepathField**`

Required*

`**contentFields**`

Required*

`**vectorFields**`

Required*

`**contentFieldsSeparator**`

Default value:

`DataSourceFieldsMapping(*, titleField: str | ``None`` = ``None``, urlField: str | `

`None`` = ``None``, filepathField: str | ``None`` = ``None``, contentFields: list[str] | `

`None`` = ``None``, vectorFields: list[str] | ``None`` = ``None``, contentFieldsSeparator: `

`str | ``None`` = ``'\n'``, **extra_data: Any)`

ﾉ

**Expand table**



**content_fields**

Python

**content_fields_separator**

Python

**filepath_field**

Python

**title_field**

Python

**url_field**

Python

**vector_fields**

Python

**Attributes**

`content_fields: list[str] | ``None`

`content_fields_separator: str | ``None`

`filepath_field: str | ``None`

`title_field: str | ``None`

`url_field: str | ``None`

`vector_fields: list[str] | ``None`



**ExtraBody Class**

Reference

Extra body for the Azure Chat Completion endpoint.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**data_sources**`

Required*

`**input_language**`

Required*

`**output_language**`

Required*

**data_sources**

Python

`ExtraBody(*, data_sources: list[Annotated[AzureAISearchDataSource | `

`AzureCosmosDBDataSource, FieldInfo(annotation=NoneType, required=``True``, `

`discriminator=``'type'``)]] | ``None`` = ``None``, input_language: str | ``None`` = ``None``, `

`output_language: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**input_language**

Python

**output_language**

Python

`data_sources: list[Annotated[AzureAISearchDataSource | `

`AzureCosmosDBDataSource, FieldInfo(annotation=NoneType, required=``True``, `

`discriminator=``'type'``)]] | ``None`

`input_language: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2,
serialization_alias=``'inputLanguage'``)]`

`output_language: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2,
serialization_alias=``'outputLanguage'``)]`



**SystemAssignedManagedIdentity**

**Authentication Class**

Reference

System assigned managed identity authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: system_assigned_managed_identity

**type**

Python

`SystemAssignedManagedIdentityAuthentication(*, type: `

`Annotated[Literal[``'SystemAssignedManagedIdentity'``, `

`'system_assigned_managed_identity'``], AfterValidator(func=to_snake)] = `

`'system_assigned_managed_identity'``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`type: Annotated[Literal[``'SystemAssignedManagedIdentity'``, `

`'system_assigned_managed_identity'``], AfterValidator(func=to_snake)]`



**UserAssignedManagedIdentity**

**Authentication Class**

Reference

User assigned managed identity authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: user_assigned_managed_identity

`**managedIdentityResourceId**`

Required*

**managed_identity_resource_id**

Python

`UserAssignedManagedIdentityAuthentication(*, type: `

`Annotated[Literal[``'UserAssignedManagedIdentity'``, `

`'user_assigned_managed_identity'``], AfterValidator(func=to_snake)] = `

`'user_assigned_managed_identity'``, managedIdentityResourceId: str | ``None``, `

`**extra_data: Any)`

ﾉ

**Expand table**

**Attributes**



**type**

Python

`managed_identity_resource_id: str | ``None`

`type: Annotated[Literal[``'UserAssignedManagedIdentity'``, `

`'user_assigned_managed_identity'``], AfterValidator(func=to_snake)]`



**open_ai_audio_to_text_execution_settin**

**gs Module**

Reference

**Classes**

OpenAIAudioToTextExecutionSettings

Request settings for OpenAI audio to text services.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**OpenAIAudioToTextExecutionSettings**

**Class**

Reference

Request settings for OpenAI audio to text services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`OpenAIAudioToTextExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`filename: str | ``None`` = ``None``, language: str | ``None`` = ``None``, prompt: str | ``None`` `

`= ``None``, response_format: str | ``None`` = ``None``, temperature: float | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**filename**`

Required*

`**language**`

Required*

`**prompt**`

Required*

`**response_format**`

Required*

`**temperature**`

Required*

prepare_settings_dict

Prepare the settings dictionary for the OpenAI API.

**prepare_settings_dict**

Prepare the settings dictionary for the OpenAI API.

Python

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`prepare_settings_dict(**kwargs) -> dict[str, Any]`

**Attributes**



**extension_data**

Python

**filename**

Python

**function_choice_behavior**

Python

**language**

Python

**prompt**

Python

**response_format**

Python

`ai_model_id: str | ``None`

`extension_data: dict[str, Any]`

`filename: str | ``None`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`language: str | ``None`

`prompt: str | ``None`



**service_id**

Python

**temperature**

Python

`response_format: str | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`temperature: float | ``None`



**open_ai_prompt_execution_settings**

**Module**

Reference

**Classes**

OpenAIChatPromptExecutionSettings

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

OpenAIEmbeddingPromptExecutionSettings

Specific settings for the text embedding endpoint.

Initialize the prompt execution settings.

OpenAIPromptExecutionSettings

Common request settings for (Azure) OpenAI

services.

Initialize the prompt execution settings.

OpenAITextPromptExecutionSettings

Specific settings for the completions endpoint.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**OpenAIChatPromptExecutionSettings**

**Class**

Reference

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`OpenAIChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, response_format: `

`dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | dict[str, Any] | `

`type[BaseModel] | type | ``None`` = ``None``, function_call: str | ``None`` = ``None``, `

`functions: list[dict[str, Any]] | ``None`` = ``None``, messages: list[dict[str, `

`Any]] | ``None`` = ``None``, parallel_tool_calls: bool | ``None`` = ``None``, tools: `

`list[dict[str, Any]] | ``None`` = ``None``, tool_choice: str | ``None`` = ``None``, `

`structured_json_response: bool = ``False``, stream_options: dict[str, Any] | `

`None`` = ``None``, max_completion_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`reasoning_effort: Literal[``'low'``, ``'medium'``, ``'high'``] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*

`**response_format**`

Required*

`**function_call**`

Required*

`**functions**`

Required*

`**messages**`

Required*

`**parallel_tool_calls**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

`**structured_json_response**`

Required*

`**stream_options**`

Required*

`**max_completion_tokens**`

Required*

`**reasoning_effort**`

Required*

validate_function_call

Validate the function_call and functions parameters.

**Methods**

ﾉ

**Expand table**



validate_response_format_and_set_flag

Validate the response_format and set

structured_json_response accordingly.

**validate_function_call**

Validate the function_call and functions parameters.

Python

**Parameters**

**Name**

**Description**

`**v**`

Default value: None

**validate_response_format_and_set_flag**

Validate the response_format and set structured_json_response accordingly.

Python

**Parameters**

**Name**

**Description**

`**values**`

Required*

**ai_model_id**

`validate_function_call(v: str | list[dict[str, Any]] | ``None`` = ``None``)`

ﾉ

**Expand table**

`validate_response_format_and_set_flag(values: Any) -> Any`

ﾉ

**Expand table**

**Attributes**



Python

**extension_data**

Python

**frequency_penalty**

Python

**function_call**

Python

**function_choice_behavior**

Python

**functions**

Python

**logit_bias**

Python

`ai_model_id: Annotated[str | ``None``, Field(serialization_alias=``'model'``)]`

`extension_data: dict[str, Any]`

`frequency_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`function_call: str | ``None`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`functions: list[dict[str, Any]] | ``None`



**max_completion_tokens**

Python

**max_tokens**

Python

**messages**

Python

**metadata**

Python

**number_of_responses**

Python

`logit_bias: dict[str | int, float] | ``None`

`max_completion_tokens: Annotated[int | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'A maximum `

`limit on total tokens for completion, including both output and reasoning `

`tokens.'``, metadata=[Gt(gt=0)])]`

`max_tokens: Annotated[int | ``None``, Field(gt=0)]`

`messages: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service.'``)]`

`metadata: dict[str, str] | ``None`

`number_of_responses: Annotated[int | ``None``, Field(ge=1, le=128, `

`serialization_alias=``'n'``)]`



**parallel_tool_calls**

Python

**presence_penalty**

Python

**reasoning_effort**

Python

**response_format**

Python

**seed**

Python

**service_id**

Python

`parallel_tool_calls: bool | ``None`

`presence_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`reasoning_effort: Annotated[Literal[``'low'``, ``'medium'``, ``'high'``] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Adjusts `

`reasoning effort (low/medium/high). Lower values reduce response time and `

`token usage.'``)]`

`response_format: dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | `

`dict[str, Any] | type[BaseModel] | type | ``None`

`seed: int | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`



**stop**

Python

**store**

Python

**stream**

Python

**stream_options**

Python

**structured_json_response**

Python

**temperature**

Python

`stop: str | list[str] | ``None`

`store: bool | ``None`

`stream: bool`

`stream_options: Annotated[dict[str, Any] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Additional `

`options to pass when streaming is used. Do not set this manually.'``)]`

`structured_json_response: Annotated[bool, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service.'``)]`

`temperature: Annotated[float | ``None``, Field(ge=0.0, le=2.0)]`



**tool_choice**

Python

**tools**

Python

**top_p**

Python

**user**

Python

`tool_choice: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service based on the function choice configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`top_p: Annotated[float | ``None``, Field(ge=0.0, le=1.0)]`

`user: str | ``None`



**OpenAIEmbeddingPromptExecution**

**Settings Class**

Reference

Specific settings for the text embedding endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

`OpenAIEmbeddingPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, input: str | list[str] | list[int] | `

`list[list[int]] | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`encoding_format: Literal[``'float'``, ``'base64'``] | ``None`` = ``None``, user: str | ``None`` `

`= ``None``, extra_headers: dict | ``None`` = ``None``, extra_query: dict | ``None`` = ``None``, `

`extra_body: dict | ``None`` = ``None``, timeout: float | ``None`` = ``None``, dimensions: `

`Annotated[int | ``None``, Gt(gt=0), Le(le=3072)] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**function_choice_behavior**`

Required*

`**input**`

Required*

`**ai_model_id**`

Required*

`**encoding_format**`

Required*

`**user**`

Required*

`**extra_headers**`

Required*

`**extra_query**`

Required*

`**extra_body**`

Required*

`**timeout**`

Required*

`**dimensions**`

Required*

**ai_model_id**

Python

**dimensions**

Python

**Attributes**

`ai_model_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'model'``)]`



**encoding_format**

Python

**extension_data**

Python

**extra_body**

Python

**extra_headers**

Python

**extra_query**

Python

**function_choice_behavior**

Python

`dimensions: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0), Le(le=3072)])]`

`encoding_format: Literal[``'float'``, ``'base64'``] | ``None`

`extension_data: dict[str, Any]`

`extra_body: dict | ``None`

`extra_headers: dict | ``None`

`extra_query: dict | ``None`



**input**

Python

**service_id**

Python

**timeout**

Python

**user**

Python

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`input: str | list[str] | list[int] | list[list[int]] | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`timeout: float | ``None`

`user: str | ``None`



**OpenAIPromptExecutionSettings Class**

Reference

Common request settings for (Azure) OpenAI services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OpenAIPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*



**ai_model_id**

Python

**extension_data**

Python

**frequency_penalty**

Python

**function_choice_behavior**

Python

**logit_bias**

Python

**max_tokens**

Python

**Attributes**

`ai_model_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'model'``)]`

`extension_data: dict[str, Any]`

`frequency_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`logit_bias: dict[str | int, float] | ``None`



**metadata**

Python

**number_of_responses**

Python

**presence_penalty**

Python

**seed**

Python

**service_id**

Python

**stop**

`max_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`metadata: dict[str, str] | ``None`

`number_of_responses: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'n'``, metadata=`

`[Ge(ge=1), Le(le=128)])]`

`presence_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`seed: int | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`



Python

**store**

Python

**stream**

Python

**temperature**

Python

**top_p**

Python

**user**

Python

`stop: str | list[str] | ``None`

`store: bool | ``None`

`stream: bool`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`user: str | ``None`



**OpenAITextPromptExecutionSettings**

**Class**

Reference

Specific settings for the completions endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OpenAITextPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, prompt: str | ``None`` = ``None``, `

`best_of: Annotated[int | ``None``, Ge(ge=1)] = ``None``, echo: bool = ``False``, `

`logprobs: Annotated[int | ``None``, Ge(ge=0), Le(le=5)] = ``None``, suffix: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**prompt**`

Required*

`**best_of**`

Required*

`**echo**`

Required*

`**logprobs**`

Required*

`**suffix**`

Required*

check_best_of_and_n

Check that the best_of parameter is not greater than the

number_of_responses parameter.

**check_best_of_and_n**

Check that the best_of parameter is not greater than the number_of_responses

parameter.

Python

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`check_best_of_and_n() -> OpenAITextPromptExecutionSettings`

**Attributes**

`ai_model_id: Annotated[str | ``None``, Field(serialization_alias=``'model'``)]`



**best_of**

Python

**echo**

Python

**extension_data**

Python

**frequency_penalty**

Python

**function_choice_behavior**

Python

**logit_bias**

Python

`best_of: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`

`echo: bool`

`extension_data: dict[str, Any]`

`frequency_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`logit_bias: dict[str | int, float] | ``None`



**logprobs**

Python

**max_tokens**

Python

**metadata**

Python

**number_of_responses**

Python

**presence_penalty**

Python

**prompt**

Python

`logprobs: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0), Le(le=5)])]`

`max_tokens: Annotated[int | ``None``, Field(gt=0)]`

`metadata: dict[str, str] | ``None`

`number_of_responses: Annotated[int | ``None``, Field(ge=1, le=128, `

`serialization_alias=``'n'``)]`

`presence_penalty: Annotated[float | ``None``, Field(ge=-2.0, le=2.0)]`

`prompt: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service based on the text content.'``)]`



**seed**

Python

**service_id**

Python

**stop**

Python

**store**

Python

**stream**

Python

**suffix**

Python

**temperature**

`seed: int | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`stop: str | list[str] | ``None`

`store: bool | ``None`

`stream: bool`

`suffix: str | ``None`



Python

**top_p**

Python

**user**

Python

`temperature: Annotated[float | ``None``, Field(ge=0.0, le=2.0)]`

`top_p: Annotated[float | ``None``, Field(ge=0.0, le=1.0)]`

`user: str | ``None`



**open_ai_text_to_audio_execution_settin**

**gs Module**

Reference

**Classes**

OpenAITextToAudioExecutionSettings

Request settings for OpenAI text to audio services.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**OpenAITextToAudioExecutionSettings**

**Class**

Reference

Request settings for OpenAI text to audio services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`OpenAITextToAudioExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, input: `

`str | ``None`` = ``None``, voice: Literal[``'alloy'``, ``'echo'``, ``'fable'``, ``'onyx'``, ``'nova'``, `

`'shimmer'``] = ``'alloy'``, response_format: Literal[``'mp3'``, ``'opus'``,
``'aac'``, ``'flac'``, `

`'wav'``, ``'pcm'``] | ``None`` = ``None``, speed: float | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**input**`

Required*

`**voice**`

Default value: alloy

`**response_format**`

Required*

`**speed**`

Required*

validate_speed

Validate the speed parameter.

**validate_speed**

Validate the speed parameter.

Python

**ai_model_id**

Python

**extension_data**

**Methods**

ﾉ

**Expand table**

`validate_speed() -> OpenAITextToAudioExecutionSettings`

**Attributes**

`ai_model_id: str | ``None`



Python

**function_choice_behavior**

Python

**input**

Python

**response_format**

Python

**service_id**

Python

**speed**

Python

**voice**

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`input: str | ``None`

`response_format: Literal[``'mp3'``, ``'opus'``, ``'aac'``, ``'flac'``, ``'wav'``, ``'pcm'``] | `

`None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`speed: float | ``None`



Python

`voice: Literal[``'alloy'``, ``'echo'``, ``'fable'``, ``'onyx'``, ``'nova'``,
``'shimmer'``]`



**open_ai_text_to_image_execution_settin**

**gs Module**

Reference

**Classes**

ImageSize

Image size.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

OpenAITextToImageExecutionSettings

Request settings for OpenAI text to image services.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**ImageSize Class**

Reference

Image size.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**width**`

Required*

`**height**`

Required*

**height**

Python

**width**

`ImageSize(*, width: int, height: int)`

ﾉ

**Expand table**

**Attributes**

`height: int`



Python

`width: int`



**OpenAITextToImageExecutionSettings**

**Class**

Reference

Request settings for OpenAI text to image services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`OpenAITextToImageExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, prompt: str | ``None`` = ``None``, `

`ai_model_id: str | ``None`` = ``None``, size: ImageSize | ``None`` = ``None``, quality: str `

`| ``None`` = ``None``, style: str | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**prompt**`

Required*

`**ai_model_id**`

Required*

`**size**`

Required*

`**quality**`

Required*

`**style**`

Required*

check_prompt

Check that the prompt is not empty.

check_size

Check that the requested image size is valid.

prepare_settings_dict

Prepare the settings dictionary for the OpenAI API.

**check_prompt**

Check that the prompt is not empty.

Python

**check_size**

Check that the requested image size is valid.

Python

**Methods**

ﾉ

**Expand table**

`check_prompt() -> OpenAITextToImageExecutionSettings`

`check_size() -> OpenAITextToImageExecutionSettings`



**prepare_settings_dict**

Prepare the settings dictionary for the OpenAI API.

Python

**ai_model_id**

Python

**extension_data**

Python

**function_choice_behavior**

Python

**prompt**

Python

**quality**

Python

`prepare_settings_dict(**kwargs) -> dict[str, Any]`

**Attributes**

`ai_model_id: str | ``None`

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`prompt: str | ``None`



**service_id**

Python

**size**

Python

**style**

Python

`quality: str | ``None`

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`size: ImageSize | ``None`

`style: str | ``None`



**services Package**

Reference

**Modules**

azure_audio_to_text

azure_chat_completion

azure_config_base

azure_text_completion

azure_text_embedding

azure_text_to_audio

azure_text_to_image

open_ai_audio_to_text

open_ai_audio_to_text_base

open_ai_chat_completion

open_ai_chat_completion_base

open_ai_config_base

open_ai_handler

open_ai_model_types

open_ai_text_completion

open_ai_text_completion_base

open_ai_text_embedding

open_ai_text_embedding_base

open_ai_text_to_audio

open_ai_text_to_audio_base

open_ai_text_to_image

ﾉ

**Expand table**



open_ai_text_to_image_base



**azure_audio_to_text Module**

Reference

**Classes**

AzureAudioToText

Azure audio to text service.

Initialize an AzureAudioToText service.

ﾉ

**Expand table**



**AzureAudioToText Class**

Reference

Azure audio to text service.

Initialize an AzureAudioToText service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(audio_to_text_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

`AzureAudioToText(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | ``None`` = `

`None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = ``None``, `

`token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` `

`= ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: str | `

`None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**service_id**

Python

**total_tokens**

Python

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**azure_chat_completion Module**

Reference

**Classes**

AzureChatCompletion

Azure Chat completion class.

Initialize an AzureChatCompletion service.

ﾉ

**Expand table**



**AzureChatCompletion Class**

Reference

Azure Chat completion class.

Initialize an AzureChatCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The service ID for the Azure deployment. (Optional)

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The optional api key. If provided, will override the value in the env vars or
.env file.

Default value: None

`**deployment_name**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The optional deployment. If provided, will override the value
(chat_deployment_name) in the env vars or

.env file.

Default value: None

`**endpoint**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The optional deployment endpoint. If provided will override the value in the
env vars or .env file.

Default value: None

`**base_url**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The optional deployment base_url. If provided will override the value in the
env vars or .env file.

Default value: None

`**api_version**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The optional deployment api version. If provided will override the value in
the env vars or .env file.

Default value: None

`**ad_token**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The Azure Active Directory token. (Optional)

Default value: None

`**ad_token_provider**`

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AsyncAzureADTokenProvider>

The Azure Active Directory token provider. (Optional)

`AzureChatCompletion(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, deployment_name: `

`str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: str | ``None`` = ``None``, api_version: str | `

`None`` = ``None``, ad_token: str | ``None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] `

`| ``None`` = ``None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` = `

`None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``, instruction_role: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

`**token_endpoint**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The token endpoint to request an Azure token. (Optional)

Default value: None

`**default_headers**`

<xref:Mapping>[str,str]

The default headers mapping of string keys to string values for HTTP requests.
(Optional)

Default value: None

`**async_client**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AsyncAzureOpenAI
|

None>>

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

Use the environment settings file as a fallback to using env vars.

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The encoding of the environment settings file, defaults to 'utf-8'.

Default value: None

`**instruction_role**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.str | None>>

The role to use for 'instruction' messages, for example, summarization prompts
could use _developer_ or

_system_. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

get_prompt_execution_settings_class

Create a request settings object.

split_message

Split an Azure On Your Data response into separate ChatMessageContents.

If the message does not have three contents, and those three are one each of:

FunctionCallContent, FunctionResultContent, and TextContent, it will not
return

three messages, potentially only one or two.

The order of the returned messages is as expected by OpenAI.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> AzureChatCompletion`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys: service_id, and
optionally:

ad_auth, ad_token_provider, default_headers

**get_prompt_execution_settings_class**

Create a request settings object.

Python

**split_message**

Split an Azure On Your Data response into separate ChatMessageContents.

If the message does not have three contents, and those three are one each of:
FunctionCallContent,

FunctionResultContent, and TextContent, it will not return three messages,
potentially only one or two.

The order of the returned messages is as expected by OpenAI.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

**ai_model_id**

Python

**ai_model_type**

Python

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

`static split_message(message: ChatMessageContent) ->
list[ChatMessageContent]`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``,
min_length=1)]`



**client**

Python

**completion_tokens**

Python

**instruction_role**

Python

**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`instruction_role: str`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**azure_config_base Module**

Reference

**Classes**

AzureOpenAIConfigBase

Internal class for configuring a connection to an Azure OpenAI service.

Internal class for configuring a connection to an Azure OpenAI service.

The _validate_call_ decorator is used with a configuration that allows

arbitrary types. This is necessary for types like _HttpsUrl_ and

_OpenAIModelTypes_.

ﾉ

**Expand table**



**AzureOpenAIConfigBase Class**

Reference

Internal class for configuring a connection to an Azure OpenAI service.

Internal class for configuring a connection to an Azure OpenAI service.

The _validate_call_ decorator is used with a configuration that allows
arbitrary types. This is

necessary for types like _HttpsUrl_ and _OpenAIModelTypes_.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**deployment_name**`

Required*

str

Name of the deployment.

`**ai_model_type**`

Required*

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_config_base.OpenAIModelTypes>

The type of OpenAI model to deploy.

`**endpoint**`

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_config_base.HttpsUrl>

The specific endpoint URL for the deployment. (Optional)

Default value: None

`**base_url**`

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_config_base.HttpsUrl>

The base URL for Azure services. (Optional)

Default value: None

`**api_version**`

str

Azure API version. Defaults to the defined DEFAULT_AZURE_API_VERSION.

Default value: 2024-10-21

`AzureOpenAIConfigBase(deployment_name: str, ai_model_type: OpenAIModelTypes,
`

`endpoint: Annotated[Url, UrlConstraints(max_length=2083,
allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, default_path=``None``)] | ``None`` `

`= ``None``, base_url: Annotated[Url, UrlConstraints(max_length=2083,
allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)] | ``None`` = ``None``, api_version: str = ``'2024-10-21'``, service_id: str `

`| ``None`` = ``None``, api_key: str | ``None`` = ``None``, ad_token: str | ``None`` = ``None``, `

`ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = ``None``, token_endpoint: `

`str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` = ``None``, client: `

`AsyncAzureOpenAI | ``None`` = ``None``, instruction_role: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**service_id**`

str

Service ID for the deployment. (Optional)

Default value: None

`**api_key**`

str

API key for Azure services. (Optional)

Default value: None

`**ad_token**`

str

Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

<xref:Callable>[[],<xref: Union>[str,<xref: Awaitable>[str]]]

A callable or coroutine function providing Azure AD tokens. (Optional)

Default value: None

`**token_endpoint**`

str

Azure AD token endpoint use to get the token. (Optional)

Default value: None

`**default_headers**`

<xref:Union>[<xref:Mapping>[str,str],None]

Default headers for HTTP requests. (Optional)

Default value: None

`**client**`

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_config_base.AsyncAzureOpenAI>

An existing client to use. (Optional)

Default value: None

`**instruction_role**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.services.azure_config_base.str | None>>

The role to use for 'instruction' messages, for example, summarization prompts
could use

_developer_ or _system_. (Optional)

Default value: None

to_dict

Convert the configuration to a dictionary.

**to_dict**

Convert the configuration to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`to_dict() -> dict[str, str]`



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**azure_text_completion Module**

Reference

**Classes**

AzureTextCompletion

Azure Text Completion class.

Initialize an AzureTextCompletion service.

ﾉ

**Expand table**



**AzureTextCompletion Class**

Reference

Azure Text Completion class.

Initialize an AzureTextCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID for the Azure deployment. (Optional)

Default value: None

`**api_key**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

The optional api key. If provided, will override the value in the env vars or
.env

file.

Default value: None

`**deployment_name**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

The optional deployment. If provided, will override the value

(text_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

`AzureTextCompletion(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, `

`base_url: str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | `

`None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = `

`None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] `

`| ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment endpoint. If provided will override the value in the

env vars or .env file.

Default value: None

`**base_url**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

The optional deployment base_url. If provided will override the value in the

env vars or .env file.

Default value: None

`**api_version**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

The optional deployment api version. If provided will override the value in
the

env vars or .env file.

Default value: None

`**ad_token**`

The Azure Active Directory token. (Optional)

Default value: None

`**ad_token_provider**`

The Azure Active Directory token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure Active Directory token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncAzureOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.azure_text_completion.str

| None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**Methods**

ﾉ

**Expand table**



**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

`from_dict(settings: dict[str, Any]) -> AzureTextCompletion`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`



Python

**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`completion_tokens: int`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**azure_text_embedding Module**

Reference

**Classes**

AzureTextEmbedding

Azure Text Embedding class.

Note: This class is marked as 'experimental' and may change in the future.

Initialize an AzureTextEmbedding service.

service_id: The service ID. (Optional) api_key: The optional api key. If

provided, will override the value in the

env vars or .env file.

deployment_name: The optional deployment. If provided, will override the

value (text_deployment_name) in the env vars or .env file.

endpoint: The optional deployment endpoint. If provided will override the

value in the env vars or .env file.

base_url: The optional deployment base_url. If provided will override the

value in the env vars or .env file.

api_version: The optional deployment api version. If provided will override

the value in the env vars or .env file.

ad_token: The Azure AD token for authentication. (Optional)

ad_token_provider: Whether to use Azure Active Directory authentication.

(Optional) The default value is False.

token_endpoint: The Azure AD token endpoint. (Optional)

default_headers: The default headers mapping of string keys to

string values for HTTP requests. (Optional)

async_client (Optional[AsyncAzureOpenAI]): An existing client to use.

(Optional) env_file_path (str | None): Use the environment settings file as a

fallback to

environment variables. (Optional)

ﾉ

**Expand table**



**AzureTextEmbedding Class**

Reference

Azure Text Embedding class.

Note: This class is marked as 'experimental' and may change in the future.

Initialize an AzureTextEmbedding service.

service_id: The service ID. (Optional) api_key: The optional api key. If
provided, will

override the value in the

env vars or .env file.

deployment_name: The optional deployment. If provided, will override the value

(text_deployment_name) in the env vars or .env file.

endpoint: The optional deployment endpoint. If provided will override the
value in the

env vars or .env file.

base_url: The optional deployment base_url. If provided will override the
value in the env

vars or .env file.

api_version: The optional deployment api version. If provided will override
the value in

the env vars or .env file.

ad_token: The Azure AD token for authentication. (Optional) ad_token_provider:

Whether to use Azure Active Directory authentication.

(Optional) The default value is False.

token_endpoint: The Azure AD token endpoint. (Optional) default_headers: The
default

headers mapping of string keys to

string values for HTTP requests. (Optional)

async_client (Optional[AsyncAzureOpenAI]): An existing client to use.
(Optional)

env_file_path (str | None): Use the environment settings file as a fallback to

environment variables. (Optional)

**Constructor**

Python



**Parameters**

**Name**

**Description**

`**service_id**`

Default value: None

`**api_key**`

Default value: None

`**deployment_name**`

Default value: None

`**endpoint**`

Default value: None

`**base_url**`

Default value: None

`**api_version**`

Default value: None

`**ad_token**`

Default value: None

`**ad_token_provider**`

Default value: None

`**token_endpoint**`

Default value: None

`**default_headers**`

Default value: None

`**async_client**`

Default value: None

`**env_file_path**`

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

`AzureTextEmbedding(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, `

`base_url: str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | `

`None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = `

`None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] `

`| ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

`from_dict(settings: dict[str, Any]) -> AzureTextEmbedding`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`



**is_experimental**

Python

**prompt_tokens**

Python

**service_id**

Python

**stage_status**

Python

**total_tokens**

Python

`completion_tokens: int`

`is_experimental = ``True`

`prompt_tokens: int`

`service_id: str`

`stage_status = ``'experimental'`

`total_tokens: int`



**azure_text_to_audio Module**

Reference

**Classes**

AzureTextToAudio

Azure text to audio service.

Initialize an AzureTextToAudio service.

ﾉ

**Expand table**



**AzureTextToAudio Class**

Reference

Azure text to audio service.

Initialize an AzureTextToAudio service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(text_to_audio_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`AzureTextToAudio(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``'2024-10-01-preview'``, ad_token: `

`str | ``None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | `

`None`` = ``None``, token_endpoint: str | ``None`` = ``None``, default_headers: `

`Mapping[str, str] | ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**api_version**`

The optional deployment api version. If provided will override the value in

the env vars or .env file. Default is "2024-10-01-preview".

Default value: 2024-10-01-preview

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**service_id**

Python

**total_tokens**

Python

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**azure_text_to_image Module**

Reference

**Classes**

AzureTextToImage

Azure Text to Image service.

Initialize an AzureTextToImage service.

ﾉ

**Expand table**



**AzureTextToImage Class**

Reference

Azure Text to Image service.

Initialize an AzureTextToImage service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(text_to_image_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

`AzureTextToImage(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | ``None`` = `

`None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = ``None``, `

`token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` `

`= ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: str | `

`None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**service_id**

Python

**total_tokens**

Python

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**open_ai_audio_to_text Module**

Reference

**Classes**

OpenAIAudioToText

OpenAI Text to Image service.

Initializes a new instance of the OpenAIAudioToText class.

ﾉ

**Expand table**



**OpenAIAudioToText Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAIAudioToText class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAIAudioToText(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**open_ai_audio_to_text_base Module**

Reference

**Classes**

OpenAIAudioToTextBase

OpenAI audio to text client.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAIAudioToTextBase Class**

Reference

OpenAI audio to text client.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`OpenAIAudioToTextBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``, client: AsyncOpenAI, ai_model_type: OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0, `

`total_tokens: int = 0)`

ﾉ

**Expand table**



**Name**

**Description**

`**total_tokens**`

Required*

get_prompt_execution_settings_class

Get the request settings class.

get_text_contents

**get_prompt_execution_settings_class**

Get the request settings class.

Python

**get_text_contents**

Python

**Parameters**

**Name**

**Description**

`**audio_content**`

Required*

`**settings**`

Required*

Default value: None

**Methods**

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

`async`` get_text_contents(audio_content: AudioContent, settings: `

`PromptExecutionSettings | ``None`` = ``None``, **kwargs: Any) -> `

`list[TextContent]`

ﾉ

**Expand table**



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**open_ai_chat_completion Module**

Reference

**Classes**

OpenAIChatCompletion

OpenAI Chat completion class.

Initialize an OpenAIChatCompletion service.

ﾉ

**Expand table**



**OpenAIChatCompletion Class**

Reference

OpenAI Chat completion class.

Initialize an OpenAIChatCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

str

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

The optional API key to use. If provided will override, the env vars or .env
file

value.

Default value: None

`**org_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

The optional org ID to use. If provided will override, the env vars or .env
file

value.

`OpenAIChatCompletion(ai_model_id: str | ``None`` = ``None``, service_id: str | ``None`` `

`= ``None``, api_key: str | ``None`` = ``None``, org_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``, instruction_role: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP requests.

(Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

The encoding of the environment settings file. (Optional)

Default value: None

`**instruction_role**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_chat_completion.str

| None>>

The role to use for 'instruction' messages, for example,

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> OpenAIChatCompletion`



**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**instruction_role**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`instruction_role: str`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**open_ai_chat_completion_base Module**

Reference

**Classes**

OpenAIChatCompletionBase

OpenAI Chat completion class.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAIChatCompletionBase Class**

Reference

OpenAI Chat completion class.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**instruction_role**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`OpenAIChatCompletionBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``, instruction_role: str = ``None``, client: AsyncOpenAI,
ai_model_type: `

`OpenAIModelTypes = OpenAIModelTypes.CHAT, prompt_tokens: int = 0, `

`completion_tokens: int = 0, total_tokens: int = 0)`

ﾉ

**Expand table**



**Name**

**Description**

`**completion_tokens**`

Required*

`**total_tokens**`

Required*

get_prompt_execution_settings_class

service_url

**get_prompt_execution_settings_class**

Python

**service_url**

Python

**MODEL_PROVIDER_NAME**

Python

**SUPPORTS_FUNCTION_CALLING**

Python

**Methods**

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

`service_url() -> str | ``None`

**Attributes**

`MODEL_PROVIDER_NAME: ClassVar[str] = ``'openai'`



`SUPPORTS_FUNCTION_CALLING: ClassVar[bool] = ``True`



**open_ai_config_base Module**

Reference

**Classes**

OpenAIConfigBase

Internal class for configuring a connection to an OpenAI service.

Initialize a client for OpenAI services.

This constructor sets up a client to interact with OpenAI's API, allowing for

different types of AI model interactions, like chat or text completion.

ﾉ

**Expand table**



**OpenAIConfigBase Class**

Reference

Internal class for configuring a connection to an OpenAI service.

Initialize a client for OpenAI services.

This constructor sets up a client to interact with OpenAI's API, allowing for
different types of AI

model interactions, like chat or text completion.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

str

OpenAI model identifier. Must be non-empty. Default to a preset value.

Default value: annotation=str required=True metadata=[MinLen(min_length=1)]

`**api_key**`

str

OpenAI API key for authentication. Must be non-empty. (Optional)

Default value: annotation=Union[str, NoneType] required=True metadata=

[MinLen(min_length=1)]

`**ai_model_type**`

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_config_base.OpenAIModelTypes>

The type of OpenAI model to interact with. Defaults to CHAT.

Default value: OpenAIModelTypes.CHAT

`**org_id**`

str

OpenAI organization ID. This is optional unless the account belongs to
multiple organizations.

Default value: None

`**service_id**`

str

OpenAI service ID. This is optional.

Default value: None

`OpenAIConfigBase(ai_model_id: str = FieldInfo(annotation=str,
required=``True``, `

`metadata=[MinLen(min_length=1)]), api_key: str | ``None`` = `

`FieldInfo(annotation=Union[str, NoneType], required=``True``, metadata=`

`[MinLen(min_length=1)]), ai_model_type: OpenAIModelTypes | ``None`` = `

`OpenAIModelTypes.CHAT, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, client: AsyncOpenAI | ``None`` = ``None``, `

`instruction_role: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**default_headers**`

<xref:Mapping>[str,str]

Default headers for HTTP requests. (Optional)

Default value: None

`**client**`

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_config_base.AsyncOpenAI>

An existing OpenAI client, optional.

Default value: None

`**instruction_role**`

str

The role to use for 'instruction' messages, for example, summarization prompts
could use

_developer_ or _system_. (Optional)

Default value: None

to_dict

Create a dict of the service settings.

**to_dict**

Create a dict of the service settings.

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

**Methods**

ﾉ

**Expand table**

`to_dict() -> dict[str, str]`

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`



Python

**prompt_tokens**

Python

**total_tokens**

Python

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**open_ai_handler Module**

Reference

**Classes**

OpenAIHandler

Internal class for calls to OpenAI API's.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAIHandler Class**

Reference

Internal class for calls to OpenAI API's.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`**total_tokens**`

Required*

`OpenAIHandler(*, client: AsyncOpenAI, ai_model_type: OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0, `

`total_tokens: int = 0)`

ﾉ

**Expand table**

**Methods**



store_usage

Store the usage information from the response.

**store_usage**

Store the usage information from the response.

Python

**Parameters**

**Name**

**Description**

`**response**`

Required*

**ai_model_type**

Python

**client**

Python

**completion_tokens**

ﾉ

**Expand table**

`store_usage(response: ChatCompletion | Completion | `

`AsyncStream[ChatCompletionChunk] | AsyncStream[Completion] | `

`CreateEmbeddingResponse)`

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`



Python

**prompt_tokens**

Python

**total_tokens**

Python

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**open_ai_model_types Module**

Reference

**Enums**

OpenAIModelTypes

OpenAI model types, can be text, chat or embedding.

ﾉ

**Expand table**



**OpenAIModelTypes Enum**

Reference

OpenAI model types, can be text, chat or embedding.

AUDIO_TO_TEXT

CHAT

EMBEDDING

TEXT

TEXT_TO_AUDIO

TEXT_TO_IMAGE

**Fields**

ﾉ

**Expand table**



**open_ai_text_completion Module**

Reference

**Classes**

OpenAITextCompletion

OpenAI Text Completion class.

Initialize an OpenAITextCompletion service.

ﾉ

**Expand table**



**OpenAITextCompletion Class**

Reference

OpenAI Text Completion class.

Initialize an OpenAITextCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

The optional API key to use. If provided will override, the env vars or .env
file

value.

Default value: None

`**org_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

`OpenAITextCompletion(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional org ID to use. If provided will override, the env vars or .env
file

value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP requests.

(Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_completion.str

| None>>

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> OpenAITextCompletion`

ﾉ

**Expand table**



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`



**service_id**

Python

**total_tokens**

Python

`service_id: str`

`total_tokens: int`



**open_ai_text_completion_base Module**

Reference

**Classes**

OpenAITextCompletionBase

Base class for OpenAI text completion services.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAITextCompletionBase Class**

Reference

Base class for OpenAI text completion services.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`OpenAITextCompletionBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``, client: AsyncOpenAI, ai_model_type: OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0, `

`total_tokens: int = 0)`

ﾉ

**Expand table**



**Name**

**Description**

`**total_tokens**`

Required*

get_prompt_execution_settings_class

service_url

**get_prompt_execution_settings_class**

Python

**service_url**

Python

**MODEL_PROVIDER_NAME**

Python

**Methods**

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

`service_url() -> str | ``None`

**Attributes**

`MODEL_PROVIDER_NAME: ClassVar[str] = ``'openai'`



**open_ai_text_embedding Module**

Reference

**Classes**

OpenAITextEmbedding

OpenAI Text Embedding class.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the OpenAITextCompletion class.

ﾉ

**Expand table**



**OpenAITextEmbedding Class**

Reference

OpenAI Text Embedding class.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OpenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

str

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.str

| None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.str

| None>>

The optional API key to use. If provided will override, the env vars or .env
file

value.

Default value: None

`**org_id**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.str

| None>>

`OpenAITextEmbedding(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional org ID to use. If provided will override, the env vars or .env
file

value.

Default value: None

`**default_headers**`

<xref:Mapping>[str,str]<xref: | None>

The default headers mapping of string keys to string values for HTTP requests.

(Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.str

| None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding.str

| None>>

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**is_experimental**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**prompt_tokens**

Python

**service_id**

Python

**stage_status**

Python

**total_tokens**

Python

`is_experimental = ``True`

`prompt_tokens: int`

`service_id: str`

`stage_status = ``'experimental'`

`total_tokens: int`



**open_ai_text_embedding_base Module**

Reference

**Classes**

OpenAITextEmbeddingBase

Base class for OpenAI text embedding services.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAITextEmbeddingBase Class**

Reference

Base class for OpenAI text embedding services.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`**total_tokens**`

Required*

`OpenAITextEmbeddingBase(*, ai_model_id: Annotated[str,
StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, pattern=``None``)], `

`service_id: str = ``''``, client: AsyncOpenAI, ai_model_type:
OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0,
total_tokens: int = `

`0)`

ﾉ

**Expand table**

**Methods**



generate_embeddings

generate_raw_embeddings

Returns embeddings for the given texts in the unedited format.

get_prompt_execution_settings_class

Get the request settings class.

**generate_embeddings**

Python

**Parameters**

**Name**

**Description**

`**texts**`

Required*

`**settings**`

Required*

Default value: None

`**batch_size**`

Required*

Default value: None

**generate_raw_embeddings**

Returns embeddings for the given texts in the unedited format.

Python

**Parameters**

**Name**

**Description**

`**texts**`

Required*

<xref:List>[str]

The texts to generate embeddings for.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.open_ai.services.open_ai_text_embedding_base.PromptExecutionSettings>

The settings to use for the request.

Default value: None

`**batch_size**`

int

ﾉ

**Expand table**

`async`` generate_embeddings(texts: list[str], settings: PromptExecutionSettings | ``None`` = `

`None``, batch_size: int | ``None`` = ``None``, **kwargs: Any) -> ndarray`

ﾉ

**Expand table**

`async`` generate_raw_embeddings(texts: list[str], settings: PromptExecutionSettings | ``None`` = `

`None``, batch_size: int | ``None`` = ``None``, **kwargs: Any) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The batch size to use for the request.

Default value: None

`**kwargs**`

Required*

<xref:Dict>[str,<xref: Any>]

Additional arguments to pass to the request.

**get_prompt_execution_settings_class**

Get the request settings class.

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**is_experimental**

Python

**prompt_tokens**

Python

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`is_experimental = ``True`



**stage_status**

Python

**total_tokens**

Python

`prompt_tokens: int`

`stage_status = ``'experimental'`

`total_tokens: int`



**open_ai_text_to_audio Module**

Reference

**Classes**

OpenAITextToAudio

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToAudio class.

ﾉ

**Expand table**



**OpenAITextToAudio Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToAudio class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAITextToAudio(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**open_ai_text_to_audio_base Module**

Reference

**Classes**

OpenAITextToAudioBase

OpenAI text to audio client base class.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAITextToAudioBase Class**

Reference

OpenAI text to audio client base class.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`OpenAITextToAudioBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``, client: AsyncOpenAI, ai_model_type: OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0, `

`total_tokens: int = 0)`

ﾉ

**Expand table**



**Name**

**Description**

`**total_tokens**`

Required*

get_audio_contents

get_prompt_execution_settings_class

Get the request settings class.

**get_audio_contents**

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**settings**`

Required*

Default value: None

**get_prompt_execution_settings_class**

Get the request settings class.

Python

**Methods**

ﾉ

**Expand table**

`async`` get_audio_contents(text: str, settings: PromptExecutionSettings | `

`None`` = ``None``, **kwargs: Any) -> list[AudioContent]`

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**open_ai_text_to_image Module**

Reference

**Classes**

OpenAITextToImage

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToImage class.

ﾉ

**Expand table**



**OpenAITextToImage Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToImage class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAITextToImage(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`min_length=1)]`



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**service_id**

Python

**total_tokens**

Python

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`service_id: str`

`total_tokens: int`



**open_ai_text_to_image_base Module**

Reference

**Classes**

OpenAITextToImageBase

OpenAI text to image client.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**OpenAITextToImageBase Class**

Reference

OpenAI text to image client.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**client**`

Required*

`**ai_model_type**`

Default value: OpenAIModelTypes.CHAT

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

`OpenAITextToImageBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``, client: AsyncOpenAI, ai_model_type: OpenAIModelTypes = `

`OpenAIModelTypes.CHAT, prompt_tokens: int = 0, completion_tokens: int = 0, `

`total_tokens: int = 0)`

ﾉ

**Expand table**



**Name**

**Description**

`**total_tokens**`

Required*

generate_image

Generate image from text.

get_prompt_execution_settings_class

Get the request settings class.

**generate_image**

Generate image from text.

Python

**Parameters**

**Name**

**Description**

`**description**`

Required*

Description of the image.

`**width**`

Required*

Width of the image, check the openai documentation for the supported sizes.

`**height**`

Required*

Height of the image, check the openai documentation for the supported

sizes.

`**kwargs**`

Required*

Additional arguments, check the openai images.generate documentation for

the supported arguments.

**Returns**

**Methods**

ﾉ

**Expand table**

`async`` generate_image(description: str, width: int, height: int, **kwargs: `

`Any) -> bytes | str`

ﾉ

**Expand table**



**Type**

**Description**

bytes | str

Image bytes or image URL.

**get_prompt_execution_settings_class**

Get the request settings class.

Python

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**total_tokens**

Python

`prompt_tokens: int`

`total_tokens: int`



**settings Package**

Reference

**Modules**

azure_open_ai_settings

open_ai_settings

ﾉ

**Expand table**



**azure_open_ai_settings Module**

Reference

**Classes**

AzureOpenAISettings

AzureOpenAI model settings.

The settings are first loaded from environment variables with the prefix

'>>AZURE_OPENAI_<<'. If the environment variables are not found, the

settings can be loaded from a .env file with the encoding 'utf-8'. If the

settings are not found in the .env file, the settings are ignored; however,

validation will fail alerting that the settings are missing.

Optional settings for prefix '>>AZURE_OPENAI_<<' are:

chat_deployment_name: str - The name of the Azure Chat

deployment. This value

will correspond to the custom name you chose for your deployment

when you deployed a model. This value can be found under

Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)

text_deployment_name: str - The name of the Azure Text

deployment. This value

will correspond to the custom name you chose for your deployment

when you deployed a model. This value can be found under

Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var AZURE_OPENAI_TEXT_DEPLOYMENT_NAME)

embedding_deployment_name: str - The name of the Azure

Embedding deployment. This value

will correspond to the custom name you chose for your deployment

when you deployed a model. This value can be found under

Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var

AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME)

text_to_image_deployment_name: str - The name of the Azure Text

to Image deployment. This

value will correspond to the custom name you chose for your

deployment when you deployed a model. This value can be found

ﾉ

**Expand table**



under Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var

AZURE_OPENAI_TEXT_TO_IMAGE_DEPLOYMENT_NAME)

audio_to_text_deployment_name: str - The name of the Azure Audio

to Text deployment. This

value will correspond to the custom name you chose for your

deployment when you deployed a model. This value can be found

under Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var

AZURE_OPENAI_AUDIO_TO_TEXT_DEPLOYMENT_NAME)

text_to_audio_deployment_name: str - The name of the Azure Text

to Audio deployment. This

value will correspond to the custom name you chose for your

deployment when you deployed a model. This value can be found

under Resource Management > Deployments in the Azure portal or,

alternatively, under Management > Deployments in Azure OpenAI

Studio. (Env var

AZURE_OPENAI_TEXT_TO_AUDIO_DEPLOYMENT_NAME)

api_key: SecretStr - The API key for the Azure deployment. This

value can be

found in the Keys & Endpoint section when examining your

resource in the Azure portal. You can use either KEY1 or KEY2. (Env

var AZURE_OPENAI_API_KEY)

base_url: HttpsUrl | None - base_url: The url of the Azure

deployment. This value

can be found in the Keys & Endpoint section when examining your

resource from the Azure portal, the base_url consists of the

endpoint, followed by /openai/deployments/{deployment_name}/,

use endpoint if you only want to supply the endpoint. (Env var

AZURE_OPENAI_BASE_URL)

endpoint: HttpsUrl - The endpoint of the Azure deployment. This

value

can be found in the Keys & Endpoint section when examining your

resource from the Azure portal, the endpoint should end in

openai.azure.com. If both base_url and endpoint are supplied,

base_url will be used. (Env var AZURE_OPENAI_ENDPOINT)

api_version: str | None - The API version to use. The default value is

"2024-02-01".

(Env var AZURE_OPENAI_API_VERSION)

token_endpoint: str - The token endpoint to use to retrieve the

authentication token.



The default value is "https://cognitiveservices.azure.com/.default

".

(Env var AZURE_OPENAI_TOKEN_ENDPOINT)



**AzureOpenAISettings Class**

Reference

AzureOpenAI model settings.

The settings are first loaded from environment variables with the prefix

'>>AZURE_OPENAI_<<'. If the environment variables are not found, the settings
can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the .env

file, the settings are ignored; however, validation will fail alerting that
the settings are

missing.

Optional settings for prefix '>>AZURE_OPENAI_<<' are:

chat_deployment_name: str - The name of the Azure Chat deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)

text_deployment_name: str - The name of the Azure Text deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_DEPLOYMENT_NAME)

embedding_deployment_name: str - The name of the Azure Embedding

deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME)

text_to_image_deployment_name: str - The name of the Azure Text to Image

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >



Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_TO_IMAGE_DEPLOYMENT_NAME)

audio_to_text_deployment_name: str - The name of the Azure Audio to Text

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_AUDIO_TO_TEXT_DEPLOYMENT_NAME)

text_to_audio_deployment_name: str - The name of the Azure Text to Audio

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_TO_AUDIO_DEPLOYMENT_NAME)

api_key: SecretStr - The API key for the Azure deployment. This value can be

found in the Keys & Endpoint section when examining your resource in the Azure

portal. You can use either KEY1 or KEY2. (Env var AZURE_OPENAI_API_KEY)

base_url: HttpsUrl | None - base_url: The url of the Azure deployment. This value

can be found in the Keys & Endpoint section when examining your resource from

the Azure portal, the base_url consists of the endpoint, followed by

/openai/deployments/{deployment_name}/, use endpoint if you only want to

supply the endpoint. (Env var AZURE_OPENAI_BASE_URL)

endpoint: HttpsUrl - The endpoint of the Azure deployment. This value

can be found in the Keys & Endpoint section when examining your resource from

the Azure portal, the endpoint should end in openai.azure.com. If both
base_url

and endpoint are supplied, base_url will be used. (Env var

AZURE_OPENAI_ENDPOINT)

api_version: str | None - The API version to use. The default value is "2024-02-01".

(Env var AZURE_OPENAI_API_VERSION)

token_endpoint: str - The token endpoint to use to retrieve the authentication

token.

The default value is "https://cognitiveservices.azure.com/.default

". (Env var

AZURE_OPENAI_TOKEN_ENDPOINT)



**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`AzureOpenAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`chat_deployment_name: str | ``None`` = ``None``, text_deployment_name: str | ``None`` = `

`None``, embedding_deployment_name: str | ``None`` = ``None``, `

`text_to_image_deployment_name: str | ``None`` = ``None``, `

`audio_to_text_deployment_name: str | ``None`` = ``None``, `

`text_to_audio_deployment_name: str | ``None`` = ``None``, endpoint: Annotated[Url, `

`UrlConstraints(max_length=2083, allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)] | ``None`` = ``None``, base_url: Annotated[Url, `

`UrlConstraints(max_length=2083, allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)] | ``None`` = ``None``, api_key: SecretStr | ``None`` = ``None``, `

`api_version: str = ``'2024-10-21'``, token_endpoint: str = `

`'https://cognitiveservices.azure.com/.default'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**chat_deployment_name**`

Required*

`**text_deployment_name**`

Required*

`**embedding_deployment_name**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**text_to_image_deployment_name**`

Required*

`**audio_to_text_deployment_name**`

Required*

`**text_to_audio_deployment_name**`

Required*

`**endpoint**`

Required*

`**base_url**`

Required*

`**api_key**`

Required*

`**api_version**`

Default value: 2024-10-21

`**token_endpoint**`

Default value: https://cognitiveservices.azure.com/.default

get_azure_openai_auth_token

Retrieve a Microsoft Entra Auth Token for a given token endpoint

for the use with Azure OpenAI.

The required role for the token is _Cognitive Services OpenAI_

_Contributor_. The token endpoint may be specified as an

environment variable, via the .env file or as an argument. If the

token endpoint is not provided, the default is None. The

_token_endpoint_ argument takes precedence over the

_token_endpoint_ attribute.

**get_azure_openai_auth_token**

Retrieve a Microsoft Entra Auth Token for a given token endpoint for the use
with

Azure OpenAI.

The required role for the token is _Cognitive Services OpenAI Contributor_.
The token

endpoint may be specified as an environment variable, via the .env file or as
an

**Methods**

ﾉ

**Expand table**



argument. If the token endpoint is not provided, the default is None. The

_token_endpoint_ argument takes precedence over the _token_endpoint_
attribute.

Python

**Parameters**

**Name**

**Description**

`**token_endpoint**`

The token endpoint to use. Defaults to

_https://cognitiveservices.azure.com/.default_

.

Default value: None

**Returns**

**Type**

**Description**

The Azure token or None if the token could not be retrieved.

**Exceptions**

**Type**

**Description**

ServiceInitializationError

If the token endpoint is not provided.

**api_key**

Python

`get_azure_openai_auth_token(token_endpoint: str | ``None`` = ``None``) -> str | `

`None`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**api_version**

Python

**audio_to_text_deployment_name**

Python

**base_url**

Python

**chat_deployment_name**

Python

**embedding_deployment_name**

Python

**endpoint**

Python

`api_key: SecretStr | ``None`

`api_version: str`

`audio_to_text_deployment_name: str | ``None`

`base_url: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)] | ``None`

`chat_deployment_name: str | ``None`

`embedding_deployment_name: str | ``None`



**env_file_encoding**

Python

**env_file_path**

Python

**env_prefix**

Python

**text_deployment_name**

Python

**text_to_audio_deployment_name**

Python

**text_to_image_deployment_name**

Python

`endpoint: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)] | ``None`

`env_file_encoding: str`

`env_file_path: str | ``None`

`env_prefix: ClassVar[str] = ``'AZURE_OPENAI_'`

`text_deployment_name: str | ``None`

`text_to_audio_deployment_name: str | ``None`



**token_endpoint**

Python

`text_to_image_deployment_name: str | ``None`

`token_endpoint: str`



**open_ai_settings Module**

Reference

**Classes**

OpenAISettings

OpenAI model settings.

The settings are first loaded from environment variables with the prefix

'>>OPENAI_<<'. If the environment variables are not found, the settings can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in

the .env file, the settings are ignored; however, validation will fail
alerting that

the settings are missing.

Optional settings for prefix '>>OPENAI_<<' are:

api_key: SecretStr - OpenAI API key, see

https://platform.openai.com/account/api-keys

(Env var OPENAI_API_KEY)

org_id: str | None - This is usually optional unless your account belongs to

multiple organizations.

(Env var OPENAI_ORG_ID)

chat_model_id: str | None - The OpenAI chat model ID to use, for

example, gpt-3.5-turbo or gpt-4.

(Env var OPENAI_CHAT_MODEL_ID)

text_model_id: str | None - The OpenAI text model ID to use, for example,

gpt-3.5-turbo-instruct.

(Env var OPENAI_TEXT_MODEL_ID)

embedding_model_id: str | None - The OpenAI embedding model ID to

use, for example, text-embedding-ada-002.

(Env var OPENAI_EMBEDDING_MODEL_ID)

text_to_image_model_id: str | None - The OpenAI text to image model ID

to use, for example, dall-e-3.

(Env var OPENAI_TEXT_TO_IMAGE_MODEL_ID)

audio_to_text_model_id: str | None - The OpenAI audio to text model ID

to use, for example, whisper-1.

(Env var OPENAI_AUDIO_TO_TEXT_MODEL_ID)

text_to_audio_model_id: str | None - The OpenAI text to audio model ID

to use, for example, jukebox-1.

ﾉ

**Expand table**



(Env var OPENAI_TEXT_TO_AUDIO_MODEL_ID)

env_file_path: str | None - if provided, the .env settings are read from this

file path location



**OpenAISettings Class**

Reference

OpenAI model settings.

The settings are first loaded from environment variables with the prefix
'>>OPENAI_<<'.

If the environment variables are not found, the settings can be loaded from a
.env file

with the encoding 'utf-8'. If the settings are not found in the .env file, the
settings are

ignored; however, validation will fail alerting that the settings are missing.

Optional settings for prefix '>>OPENAI_<<' are:

api_key: SecretStr - OpenAI API key, see
https://platform.openai.com/account/api-

keys

(Env var OPENAI_API_KEY)

org_id: str | None - This is usually optional unless your account belongs to multiple

organizations.

(Env var OPENAI_ORG_ID)

chat_model_id: str | None - The OpenAI chat model ID to use, for example, gpt-3.5-

turbo or gpt-4.

(Env var OPENAI_CHAT_MODEL_ID)

text_model_id: str | None - The OpenAI text model ID to use, for example, gpt-3.5-

turbo-instruct.

(Env var OPENAI_TEXT_MODEL_ID)

embedding_model_id: str | None - The OpenAI embedding model ID to use, for

example, text-embedding-ada-002.

(Env var OPENAI_EMBEDDING_MODEL_ID)

text_to_image_model_id: str | None - The OpenAI text to image model ID to use,

for example, dall-e-3.

(Env var OPENAI_TEXT_TO_IMAGE_MODEL_ID)

audio_to_text_model_id: str | None - The OpenAI audio to text model ID to use, for

example, whisper-1.

(Env var OPENAI_AUDIO_TO_TEXT_MODEL_ID)

text_to_audio_model_id: str | None - The OpenAI text to audio model ID to use, for

example, jukebox-1.



(Env var OPENAI_TEXT_TO_AUDIO_MODEL_ID)

env_file_path: str | None - if provided, the .env settings are read from this file path

location

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`OpenAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr | ``None`` = ``None``, org_id: str | ``None`` = ``None``, chat_model_id: `

`str | ``None`` = ``None``, text_model_id: str | ``None`` = ``None``, embedding_model_id: str `

`| ``None`` = ``None``, text_to_image_model_id: str | ``None`` = ``None``, `

`audio_to_text_model_id: str | ``None`` = ``None``, text_to_audio_model_id: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**org_id**`

Required*

`**chat_model_id**`

Required*

`**text_model_id**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**embedding_model_id**`

Required*

`**text_to_image_model_id**`

Required*

`**audio_to_text_model_id**`

Required*

`**text_to_audio_model_id**`

Required*

**api_key**

Python

**audio_to_text_model_id**

Python

**chat_model_id**

Python

**embedding_model_id**

Python

**Attributes**

`api_key: SecretStr | ``None`

`audio_to_text_model_id: str | ``None`

`chat_model_id: str | ``None`

`embedding_model_id: str | ``None`



**env_file_encoding**

Python

**env_file_path**

Python

**env_prefix**

Python

**org_id**

Python

**text_model_id**

Python

**text_to_audio_model_id**

Python

**text_to_image_model_id**

`env_file_encoding: str`

`env_file_path: str | ``None`

`env_prefix: ClassVar[str] = ``'OPENAI_'`

`org_id: str | ``None`

`text_model_id: str | ``None`

`text_to_audio_model_id: str | ``None`



Python

`text_to_image_model_id: str | ``None`



**const Module**

Reference



**ApiKeyAuthentication Class**

Reference

API key authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: api_key

`**key**`

Required*

**key**

Python

**type**

`ApiKeyAuthentication(*, type: Annotated[Literal[``'APIKey'``, ``'api_key'``],
`

`AfterValidator(func=to_snake)] = ``'api_key'``, key: str, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`key: str`



Python

`type: Annotated[Literal[``'APIKey'``, ``'api_key'``], `

`AfterValidator(func=to_snake)]`



**AzureAISearchDataSource Class**

Reference

Azure AI Search data source.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: azure_search

`**parameters**`

Required*

from_azure_ai_search_settings

Create an instance from Azure AI Search settings.

**from_azure_ai_search_settings**

Create an instance from Azure AI Search settings.

`AzureAISearchDataSource(*, type: Literal[``'azure_search'``] =
``'azure_search'``, `

`parameters: Annotated[dict, AzureAISearchDataSourceParameters], `

`**extra_data: Any)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**azure_ai_search_settings**`

Required*

**parameters**

Python

**type**

Python

`from_azure_ai_search_settings(azure_ai_search_settings: `

`AzureAISearchSettings, **kwargs: Any)`

ﾉ

**Expand table**

**Attributes**

`parameters: Annotated[dict, AzureAISearchDataSourceParameters]`

`type: Literal[``'azure_search'``]`



**AzureAISearchDataSourceParameters**

**Class**

Reference

Azure AI Search data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

`AzureAISearchDataSourceParameters(*, indexName: str, indexLanguage: str | `

`None`` = ``None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: `

`bool | ``None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: `

`str | ``None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = `

`None``, strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | `

`None`` = ``None``, endpoint: str | ``None`` = ``None``, queryType: `

`Annotated[Literal[``'simple'``, ``'semantic'``, ``'vector'``,
``'vectorSimpleHybrid'``, `

`'vectorSemanticHybrid'``], AfterValidator(func=to_snake)] = ``'simple'``, `

`authentication: ApiKeyAuthentication | `

`SystemAssignedManagedIdentityAuthentication | `

`UserAssignedManagedIdentityAuthentication | AccessTokenAuthentication | ``None`` `

`= ``None``, **extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**inScope**`

Default value: True

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

`**endpoint**`

Required*

`**queryType**`

Default value: simple

`**authentication**`

Required*

**authentication**

Python

**endpoint**

Python

**Attributes**

`authentication: ApiKeyAuthentication | `

`SystemAssignedManagedIdentityAuthentication | `

`UserAssignedManagedIdentityAuthentication | AccessTokenAuthentication | `

`None`

`endpoint: str | ``None`



**query_type**

Python

`query_type: Annotated[Literal[``'simple'``, ``'semantic'``, ``'vector'``, `

`'vectorSimpleHybrid'``, ``'vectorSemanticHybrid'``], `

`AfterValidator(func=to_snake)]`



**AzureAudioToText Class**

Reference

Azure audio to text service.

Initialize an AzureAudioToText service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(audio_to_text_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

`AzureAudioToText(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | ``None`` = `

`None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = ``None``, `

`token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` `

`= ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: str | `

`None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**AzureChatCompletion Class**

Reference

Azure Chat completion class.

Initialize an AzureChatCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The service ID for the Azure deployment. (Optional)

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment. If provided, will override the value

(chat_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`AzureChatCompletion(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, `

`base_url: str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | `

`None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = `

`None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] `

`| ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: `

`str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``, instruction_role: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**base_url**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The Azure Active Directory token. (Optional)

Default value: None

`**ad_token_provider**`

<xref:semantic_kernel.connectors.ai.open_ai.AsyncAzureADTokenProvider>

The Azure Active Directory token provider. (Optional)

Default value: None

`**token_endpoint**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The token endpoint to request an Azure token. (Optional)

Default value: None

`**default_headers**`

<xref:Mapping>[str,str]

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.AsyncAzureOpenAI |

None>>

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Use the environment settings file as a fallback to using env vars.

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The encoding of the environment settings file, defaults to 'utf-8'.

Default value: None

`**instruction_role**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The role to use for 'instruction' messages, for example, summarization

prompts could use _developer_ or _system_. (Optional)

Default value: None

**Methods**



from_dict

Initialize an Azure OpenAI service from a dictionary of

settings.

get_prompt_execution_settings_class

Create a request settings object.

split_message

Split an Azure On Your Data response into separate

ChatMessageContents.

If the message does not have three contents, and those

three are one each of: FunctionCallContent,

FunctionResultContent, and TextContent, it will not return

three messages, potentially only one or two.

The order of the returned messages is as expected by

OpenAI.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys: service_id, and

optionally:

ad_auth, ad_token_provider, default_headers

**get_prompt_execution_settings_class**

Create a request settings object.

Python

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> AzureChatCompletion`

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`



**split_message**

Split an Azure On Your Data response into separate ChatMessageContents.

If the message does not have three contents, and those three are one each of:

FunctionCallContent, FunctionResultContent, and TextContent, it will not
return three

messages, potentially only one or two.

The order of the returned messages is as expected by OpenAI.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

`static split_message(message: ChatMessageContent) -> `

`list[ChatMessageContent]`

ﾉ

**Expand table**



**AzureChatPromptExecutionSettings**

**Class**

Reference

Specific settings for the Azure OpenAI Chat Completion endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Any

`AzureChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, response_format: `

`dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | dict[str, Any] | `

`type[BaseModel] | type | ``None`` = ``None``, function_call: str | ``None`` = ``None``, `

`functions: list[dict[str, Any]] | ``None`` = ``None``, messages: list[dict[str, `

`Any]] | ``None`` = ``None``, parallel_tool_calls: bool | ``None`` = ``None``, tools: `

`list[dict[str, Any]] | ``None`` = ``None``, tool_choice: str | ``None`` = ``None``, `

`structured_json_response: bool = ``False``, stream_options: dict[str, Any] | `

`None`` = ``None``, max_completion_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`reasoning_effort: Literal[``'low'``, ``'medium'``, ``'high'``] | ``None`` = ``None``, `

`extra_body: dict[str, Any] | ExtraBody | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**store**`

Required*

`**metadata**`

Required*

`**response_format**`

Required*

`**function_call**`

Required*

`**functions**`

Required*

`**messages**`

Required*

`**parallel_tool_calls**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

`**structured_json_response**`

Required*

`**stream_options**`

Required*

`**max_completion_tokens**`

Required*

`**reasoning_effort**`

Required*

`**extra_body**`

Required*

**extra_body**

**Attributes**



Python

`extra_body: dict[str, Any] | ExtraBody | ``None`



**AzureCosmosDBDataSource Class**

Reference

Azure Cosmos DB data source.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: azure_cosmos_db

`**parameters**`

Required*

**parameters**

Python

`AzureCosmosDBDataSource(*, type: Literal[``'azure_cosmos_db'``] = `

`'azure_cosmos_db'``, parameters: AzureCosmosDBDataSourceParameters, `

`**extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`parameters: AzureCosmosDBDataSourceParameters`



**type**

Python

`type: Literal[``'azure_cosmos_db'``]`



**AzureCosmosDBDataSourceParameters**

**Class**

Reference

Azure Cosmos DB data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

Required*

`**inScope**`

Default value: True

`AzureCosmosDBDataSourceParameters(*, indexName: str, indexLanguage: str | `

`None`` = ``None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: `

`bool | ``None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: `

`str | ``None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = `

`None``, strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | `

`None`` = ``None``, authentication: ConnectionStringAuthentication | ``None`` = ``None``, `

`databaseName: str | ``None`` = ``None``, containerName: str | ``None`` = ``None``, `

`embeddingDependencyType: AzureEmbeddingDependency | ``None`` = ``None``, `

`**extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

`**authentication**`

Required*

`**databaseName**`

Required*

`**containerName**`

Required*

`**embeddingDependencyType**`

Required*

**authentication**

Python

**container_name**

Python

**database_name**

**Attributes**

`authentication: ConnectionStringAuthentication | ``None`

`container_name: str | ``None`



Python

**embedding_dependency_type**

Python

`database_name: str | ``None`

`embedding_dependency_type: AzureEmbeddingDependency | ``None`



**AzureDataSourceParameters Class**

Reference

Azure data source parameters.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**indexName**`

Required*

`**indexLanguage**`

Required*

`**fieldsMapping**`

Required*

`**inScope**`

Default value: True

`**topNDocuments**`

Default value: 5

`**semanticConfiguration**`

Required*

`AzureDataSourceParameters(*, indexName: str, indexLanguage: str | ``None`` = `

`None``, fieldsMapping: DataSourceFieldsMapping | ``None`` = ``None``, inScope: bool | `

`None`` = ``True``, topNDocuments: int | ``None`` = 5, semanticConfiguration: str | `

`None`` = ``None``, roleInformation: str | ``None`` = ``None``, filter: str | ``None`` = ``None``, `

`strictness: int = 3, embeddingDependency: AzureEmbeddingDependency | ``None`` = `

`None``, **extra_data: Any)`

ﾉ

**Expand table**



**Name**

**Description**

`**roleInformation**`

Required*

`**filter**`

Required*

`**strictness**`

Default value: 3

`**embeddingDependency**`

Required*

**embedding_dependency**

Python

**fields_mapping**

Python

**filter**

Python

**in_scope**

Python

**index_language**

**Attributes**

`embedding_dependency: AzureEmbeddingDependency | ``None`

`fields_mapping: DataSourceFieldsMapping | ``None`

`filter: str | ``None`

`in_scope: bool | ``None`



Python

**index_name**

Python

**role_information**

Python

**semantic_configuration**

Python

**strictness**

Python

**top_n_documents**

Python

`index_language: str | ``None`

`index_name: str`

`role_information: str | ``None`

`semantic_configuration: str | ``None`

`strictness: int`

`top_n_documents: int | ``None`



**AzureEmbeddingDependency Class**

Reference

Azure embedding dependency.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: deployment_name

`**deploymentName**`

Required*

**deployment_name**

Python

`AzureEmbeddingDependency(*, type: Annotated[Literal[``'DeploymentName'``, `

`'deployment_name'``], AfterValidator(func=to_snake)] = ``'deployment_name'``,
`

`deploymentName: str | ``None`` = ``None``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`deployment_name: str | ``None`



**type**

Python

`type: Annotated[Literal[``'DeploymentName'``, ``'deployment_name'``], `

`AfterValidator(func=to_snake)]`



**AzureOpenAISettings Class**

Reference

AzureOpenAI model settings.

The settings are first loaded from environment variables with the prefix

'>>AZURE_OPENAI_<<'. If the environment variables are not found, the settings
can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the .env

file, the settings are ignored; however, validation will fail alerting that
the settings are

missing.

Optional settings for prefix '>>AZURE_OPENAI_<<' are:

chat_deployment_name: str - The name of the Azure Chat deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)

text_deployment_name: str - The name of the Azure Text deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_DEPLOYMENT_NAME)

embedding_deployment_name: str - The name of the Azure Embedding

deployment. This value

will correspond to the custom name you chose for your deployment when you

deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME)

text_to_image_deployment_name: str - The name of the Azure Text to Image

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >



Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_TO_IMAGE_DEPLOYMENT_NAME)

audio_to_text_deployment_name: str - The name of the Azure Audio to Text

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_AUDIO_TO_TEXT_DEPLOYMENT_NAME)

text_to_audio_deployment_name: str - The name of the Azure Text to Audio

deployment. This

value will correspond to the custom name you chose for your deployment when

you deployed a model. This value can be found under Resource Management >

Deployments in the Azure portal or, alternatively, under Management >

Deployments in Azure OpenAI Studio. (Env var

AZURE_OPENAI_TEXT_TO_AUDIO_DEPLOYMENT_NAME)

api_key: SecretStr - The API key for the Azure deployment. This value can be

found in the Keys & Endpoint section when examining your resource in the Azure

portal. You can use either KEY1 or KEY2. (Env var AZURE_OPENAI_API_KEY)

base_url: HttpsUrl | None - base_url: The url of the Azure deployment. This value

can be found in the Keys & Endpoint section when examining your resource from

the Azure portal, the base_url consists of the endpoint, followed by

/openai/deployments/{deployment_name}/, use endpoint if you only want to

supply the endpoint. (Env var AZURE_OPENAI_BASE_URL)

endpoint: HttpsUrl - The endpoint of the Azure deployment. This value

can be found in the Keys & Endpoint section when examining your resource from

the Azure portal, the endpoint should end in openai.azure.com. If both
base_url

and endpoint are supplied, base_url will be used. (Env var

AZURE_OPENAI_ENDPOINT)

api_version: str | None - The API version to use. The default value is "2024-02-01".

(Env var AZURE_OPENAI_API_VERSION)

token_endpoint: str - The token endpoint to use to retrieve the authentication

token.

The default value is "https://cognitiveservices.azure.com/.default

". (Env var

AZURE_OPENAI_TOKEN_ENDPOINT)



**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`AzureOpenAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`chat_deployment_name: str | ``None`` = ``None``, text_deployment_name: str | ``None`` = `

`None``, embedding_deployment_name: str | ``None`` = ``None``, `

`text_to_image_deployment_name: str | ``None`` = ``None``, `

`audio_to_text_deployment_name: str | ``None`` = ``None``, `

`text_to_audio_deployment_name: str | ``None`` = ``None``, endpoint: Annotated[Url, `

`UrlConstraints(max_length=2083, allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)] | ``None`` = ``None``, base_url: Annotated[Url, `

`UrlConstraints(max_length=2083, allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)] | ``None`` = ``None``, api_key: SecretStr | ``None`` = ``None``, `

`api_version: str = ``'2024-10-21'``, token_endpoint: str = `

`'https://cognitiveservices.azure.com/.default'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**chat_deployment_name**`

Required*

`**text_deployment_name**`

Required*

`**embedding_deployment_name**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**text_to_image_deployment_name**`

Required*

`**audio_to_text_deployment_name**`

Required*

`**text_to_audio_deployment_name**`

Required*

`**endpoint**`

Required*

`**base_url**`

Required*

`**api_key**`

Required*

`**api_version**`

Default value: 2024-10-21

`**token_endpoint**`

Default value: https://cognitiveservices.azure.com/.default

get_azure_openai_auth_token

Retrieve a Microsoft Entra Auth Token for a given token endpoint

for the use with Azure OpenAI.

The required role for the token is _Cognitive Services OpenAI_

_Contributor_. The token endpoint may be specified as an

environment variable, via the .env file or as an argument. If the

token endpoint is not provided, the default is None. The

_token_endpoint_ argument takes precedence over the

_token_endpoint_ attribute.

**get_azure_openai_auth_token**

Retrieve a Microsoft Entra Auth Token for a given token endpoint for the use
with

Azure OpenAI.

The required role for the token is _Cognitive Services OpenAI Contributor_.
The token

endpoint may be specified as an environment variable, via the .env file or as
an

**Methods**

ﾉ

**Expand table**



argument. If the token endpoint is not provided, the default is None. The

_token_endpoint_ argument takes precedence over the _token_endpoint_
attribute.

Python

**Parameters**

**Name**

**Description**

`**token_endpoint**`

The token endpoint to use. Defaults to

_https://cognitiveservices.azure.com/.default_

.

Default value: None

**Returns**

**Type**

**Description**

The Azure token or None if the token could not be retrieved.

**Exceptions**

**Type**

**Description**

ServiceInitializationError

If the token endpoint is not provided.

**api_key**

Python

`get_azure_openai_auth_token(token_endpoint: str | ``None`` = ``None``) -> str | `

`None`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**api_version**

Python

**audio_to_text_deployment_name**

Python

**base_url**

Python

**chat_deployment_name**

Python

**embedding_deployment_name**

Python

**endpoint**

Python

`api_key: SecretStr | ``None`

`api_version: str`

`audio_to_text_deployment_name: str | ``None`

`base_url: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)] | ``None`

`chat_deployment_name: str | ``None`

`embedding_deployment_name: str | ``None`



**env_prefix**

Python

**text_deployment_name**

Python

**text_to_audio_deployment_name**

Python

**text_to_image_deployment_name**

Python

**token_endpoint**

Python

`endpoint: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)] | ``None`

`env_prefix: ClassVar[str] = ``'AZURE_OPENAI_'`

`text_deployment_name: str | ``None`

`text_to_audio_deployment_name: str | ``None`

`text_to_image_deployment_name: str | ``None`

`token_endpoint: str`



**AzureTextCompletion Class**

Reference

Azure Text Completion class.

Initialize an AzureTextCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID for the Azure deployment. (Optional)

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment. If provided, will override the value

(text_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

`AzureTextCompletion(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, `

`base_url: str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | `

`None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = `

`None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] `

`| ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

The Azure Active Directory token. (Optional)

Default value: None

`**ad_token_provider**`

The Azure Active Directory token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure Active Directory token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncAzureOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> AzureTextCompletion`



**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

ﾉ

**Expand table**



**AzureTextEmbedding Class**

Reference

Azure Text Embedding class.

Note: This class is marked as 'experimental' and may change in the future.

Initialize an AzureTextEmbedding service.

service_id: The service ID. (Optional) api_key: The optional api key. If
provided, will

override the value in the

env vars or .env file.

deployment_name: The optional deployment. If provided, will override the value

(text_deployment_name) in the env vars or .env file.

endpoint: The optional deployment endpoint. If provided will override the
value in the

env vars or .env file.

base_url: The optional deployment base_url. If provided will override the
value in the env

vars or .env file.

api_version: The optional deployment api version. If provided will override
the value in

the env vars or .env file.

ad_token: The Azure AD token for authentication. (Optional) ad_token_provider:

Whether to use Azure Active Directory authentication.

(Optional) The default value is False.

token_endpoint: The Azure AD token endpoint. (Optional) default_headers: The
default

headers mapping of string keys to

string values for HTTP requests. (Optional)

async_client (Optional[AsyncAzureOpenAI]): An existing client to use.
(Optional)

env_file_path (str | None): Use the environment settings file as a fallback to

environment variables. (Optional)

**Constructor**

Python



**Parameters**

**Name**

**Description**

`**service_id**`

Default value: None

`**api_key**`

Default value: None

`**deployment_name**`

Default value: None

`**endpoint**`

Default value: None

`**base_url**`

Default value: None

`**api_version**`

Default value: None

`**ad_token**`

Default value: None

`**ad_token_provider**`

Default value: None

`**token_endpoint**`

Default value: None

`**default_headers**`

Default value: None

`**async_client**`

Default value: None

`**env_file_path**`

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

`AzureTextEmbedding(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, `

`base_url: str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | `

`None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = `

`None``, token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] `

`| ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**is_experimental**

Python

`from_dict(settings: dict[str, Any]) -> AzureTextEmbedding`

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`



**prompt_tokens**

Python

**stage_status**

Python

**total_tokens**

Python

`is_experimental = ``True`

`prompt_tokens: int`

`stage_status = ``'experimental'`

`total_tokens: int`



**AzureTextToAudio Class**

Reference

Azure text to audio service.

Initialize an AzureTextToAudio service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(text_to_audio_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`AzureTextToAudio(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``'2024-10-01-preview'``, ad_token: `

`str | ``None`` = ``None``, ad_token_provider: Callable[[], str | Awaitable[str]] | `

`None`` = ``None``, token_endpoint: str | ``None`` = ``None``, default_headers: `

`Mapping[str, str] | ``None`` = ``None``, async_client: AsyncAzureOpenAI | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**api_version**`

The optional deployment api version. If provided will override the value in

the env vars or .env file. Default is "2024-10-01-preview".

Default value: 2024-10-01-preview

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**AzureTextToImage Class**

Reference

Azure Text to Image service.

Initialize an AzureTextToImage service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

The service ID. (Optional)

Default value: None

`**api_key**`

The optional api key. If provided, will override the value in the env vars or

.env file.

Default value: None

`**deployment_name**`

The optional deployment. If provided, will override the value

(text_to_image_deployment_name) in the env vars or .env file.

Default value: None

`**endpoint**`

The optional deployment endpoint. If provided will override the value in

the env vars or .env file.

Default value: None

`**base_url**`

The optional deployment base_url. If provided will override the value in

the env vars or .env file.

Default value: None

`**api_version**`

`AzureTextToImage(service_id: str | ``None`` = ``None``, api_key: str | ``None`` = ``None``, `

`deployment_name: str | ``None`` = ``None``, endpoint: str | ``None`` = ``None``, base_url: `

`str | ``None`` = ``None``, api_version: str | ``None`` = ``None``, ad_token: str | ``None`` = `

`None``, ad_token_provider: Callable[[], str | Awaitable[str]] | ``None`` = ``None``, `

`token_endpoint: str | ``None`` = ``None``, default_headers: Mapping[str, str] | ``None`` `

`= ``None``, async_client: AsyncAzureOpenAI | ``None`` = ``None``, env_file_path: str | `

`None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The optional deployment api version. If provided will override the value in

the env vars or .env file.

Default value: None

`**ad_token**`

The Azure AD token for authentication. (Optional)

Default value: None

`**ad_token_provider**`

Azure AD Token provider. (Optional)

Default value: None

`**token_endpoint**`

The Azure AD token endpoint. (Optional)

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Azure OpenAI service from a dictionary of settings.

**from_dict**

Initialize an Azure OpenAI service from a dictionary of settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`



**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service. should contain keys:
deployment_name,

endpoint, api_key and optionally: api_version, ad_auth

**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**ConnectionStringAuthentication Class**

Reference

Connection string authentication.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: connection_string

`**connectionString**`

Required*

**connection_string**

Python

`ConnectionStringAuthentication(*, type: `

`Annotated[Literal[``'ConnectionString'``, ``'connection_string'``], `

`AfterValidator(func=to_snake)] = ``'connection_string'``, connectionString:
str `

`| ``None`` = ``None``, **extra_data: Any)`

ﾉ

**Expand table**

**Attributes**

`connection_string: str | ``None`



**type**

Python

`type: Annotated[Literal[``'ConnectionString'``, ``'connection_string'``], `

`AfterValidator(func=to_snake)]`



**DataSourceFieldsMapping Class**

Reference

Data source fields mapping.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**titleField**`

Required*

`**urlField**`

Required*

`**filepathField**`

Required*

`**contentFields**`

Required*

`**vectorFields**`

Required*

`**contentFieldsSeparator**`

Default value:

`DataSourceFieldsMapping(*, titleField: str | ``None`` = ``None``, urlField: str | `

`None`` = ``None``, filepathField: str | ``None`` = ``None``, contentFields: list[str] | `

`None`` = ``None``, vectorFields: list[str] | ``None`` = ``None``, contentFieldsSeparator: `

`str | ``None`` = ``'\n'``, **extra_data: Any)`

ﾉ

**Expand table**



**content_fields**

Python

**content_fields_separator**

Python

**filepath_field**

Python

**title_field**

Python

**url_field**

Python

**vector_fields**

Python

**Attributes**

`content_fields: list[str] | ``None`

`content_fields_separator: str | ``None`

`filepath_field: str | ``None`

`title_field: str | ``None`

`url_field: str | ``None`

`vector_fields: list[str] | ``None`



**ExtraBody Class**

Reference

Extra body for the Azure Chat Completion endpoint.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**data_sources**`

Required*

`**input_language**`

Required*

`**output_language**`

Required*

**data_sources**

Python

`ExtraBody(*, data_sources: list[Annotated[AzureAISearchDataSource | `

`AzureCosmosDBDataSource, FieldInfo(annotation=NoneType, required=``True``, `

`discriminator=``'type'``)]] | ``None`` = ``None``, input_language: str | ``None`` = ``None``, `

`output_language: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**input_language**

Python

**output_language**

Python

`data_sources: list[Annotated[AzureAISearchDataSource | `

`AzureCosmosDBDataSource, FieldInfo(annotation=NoneType, required=``True``, `

`discriminator=``'type'``)]] | ``None`

`input_language: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2,
serialization_alias=``'inputLanguage'``)]`

`output_language: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2,
serialization_alias=``'outputLanguage'``)]`



**OpenAIAudioToText Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAIAudioToText class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAIAudioToText(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_type**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`



**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**OpenAIAudioToTextExecutionSettings**

**Class**

Reference

Request settings for OpenAI audio to text services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`OpenAIAudioToTextExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`filename: str | ``None`` = ``None``, language: str | ``None`` = ``None``, prompt: str | ``None`` `

`= ``None``, response_format: str | ``None`` = ``None``, temperature: float | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**filename**`

Required*

`**language**`

Required*

`**prompt**`

Required*

`**response_format**`

Required*

`**temperature**`

Required*

prepare_settings_dict

Prepare the settings dictionary for the OpenAI API.

**prepare_settings_dict**

Prepare the settings dictionary for the OpenAI API.

Python

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`prepare_settings_dict(**kwargs) -> dict[str, Any]`

**Attributes**



**filename**

Python

**language**

Python

**prompt**

Python

**response_format**

Python

**temperature**

Python

`ai_model_id: str | ``None`

`filename: str | ``None`

`language: str | ``None`

`prompt: str | ``None`

`response_format: str | ``None`

`temperature: float | ``None`



**OpenAIChatCompletion Class**

Reference

OpenAI Chat completion class.

Initialize an OpenAIChatCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

str

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`OpenAIChatCompletion(ai_model_id: str | ``None`` = ``None``, service_id: str | ``None`` `

`= ``None``, api_key: str | ``None`` = ``None``, org_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``, instruction_role: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The encoding of the environment settings file. (Optional)

Default value: None

`**instruction_role**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The role to use for 'instruction' messages, for example,

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> OpenAIChatCompletion`

ﾉ

**Expand table**



**OpenAIChatPromptExecutionSettings**

**Class**

Reference

Specific settings for the Chat Completion endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`OpenAIChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, response_format: `

`dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | dict[str, Any] | `

`type[BaseModel] | type | ``None`` = ``None``, function_call: str | ``None`` = ``None``, `

`functions: list[dict[str, Any]] | ``None`` = ``None``, messages: list[dict[str, `

`Any]] | ``None`` = ``None``, parallel_tool_calls: bool | ``None`` = ``None``, tools: `

`list[dict[str, Any]] | ``None`` = ``None``, tool_choice: str | ``None`` = ``None``, `

`structured_json_response: bool = ``False``, stream_options: dict[str, Any] | `

`None`` = ``None``, max_completion_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`reasoning_effort: Literal[``'low'``, ``'medium'``, ``'high'``] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*

`**response_format**`

Required*

`**function_call**`

Required*

`**functions**`

Required*

`**messages**`

Required*

`**parallel_tool_calls**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

`**structured_json_response**`

Required*

`**stream_options**`

Required*

`**max_completion_tokens**`

Required*

`**reasoning_effort**`

Required*

validate_function_call

Validate the function_call and functions parameters.

**Methods**

ﾉ

**Expand table**



validate_response_format_and_set_flag

Validate the response_format and set

structured_json_response accordingly.

**validate_function_call**

Validate the function_call and functions parameters.

Python

**Parameters**

**Name**

**Description**

`**v**`

Default value: None

**validate_response_format_and_set_flag**

Validate the response_format and set structured_json_response accordingly.

Python

**Parameters**

**Name**

**Description**

`**values**`

Required*

**function_call**

`validate_function_call(v: str | list[dict[str, Any]] | ``None`` = ``None``)`

ﾉ

**Expand table**

`validate_response_format_and_set_flag(values: Any) -> Any`

ﾉ

**Expand table**

**Attributes**



Python

**functions**

Python

**max_completion_tokens**

Python

**messages**

Python

**parallel_tool_calls**

Python

**reasoning_effort**

Python

`function_call: str | ``None`

`functions: list[dict[str, Any]] | ``None`

`max_completion_tokens: Annotated[int | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'A maximum `

`limit on total tokens for completion, including both output and reasoning `

`tokens.'``, metadata=[Gt(gt=0)])]`

`messages: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service.'``)]`

`parallel_tool_calls: bool | ``None`

`reasoning_effort: Annotated[Literal[``'low'``, ``'medium'``, ``'high'``] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Adjusts `



**response_format**

Python

**stream_options**

Python

**structured_json_response**

Python

**tool_choice**

Python

**tools**

Python

`reasoning effort (low/medium/high). Lower values reduce response time and `

`token usage.'``)]`

`response_format: dict[Literal[``'type'``], Literal[``'text'``, ``'json_object'``]] | `

`dict[str, Any] | type[BaseModel] | type | ``None`

`stream_options: Annotated[dict[str, Any] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Additional `

`options to pass when streaming is used. Do not set this manually.'``)]`

`structured_json_response: Annotated[bool, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service.'``)]`

`tool_choice: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service based on the function choice configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `



`this manually. It is set by the service based on the function choice `

`configuration.'``)]`



**OpenAIEmbeddingPromptExecution**

**Settings Class**

Reference

Specific settings for the text embedding endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

`OpenAIEmbeddingPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, input: str | list[str] | list[int] | `

`list[list[int]] | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`encoding_format: Literal[``'float'``, ``'base64'``] | ``None`` = ``None``, user: str | ``None`` `

`= ``None``, extra_headers: dict | ``None`` = ``None``, extra_query: dict | ``None`` = ``None``, `

`extra_body: dict | ``None`` = ``None``, timeout: float | ``None`` = ``None``, dimensions: `

`Annotated[int | ``None``, Gt(gt=0), Le(le=3072)] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**function_choice_behavior**`

Required*

`**input**`

Required*

`**ai_model_id**`

Required*

`**encoding_format**`

Required*

`**user**`

Required*

`**extra_headers**`

Required*

`**extra_query**`

Required*

`**extra_body**`

Required*

`**timeout**`

Required*

`**dimensions**`

Required*

**ai_model_id**

Python

**dimensions**

Python

**Attributes**

`ai_model_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'model'``)]`



**encoding_format**

Python

**extra_body**

Python

**extra_headers**

Python

**extra_query**

Python

**input**

Python

**timeout**

Python

`dimensions: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0), Le(le=3072)])]`

`encoding_format: Literal[``'float'``, ``'base64'``] | ``None`

`extra_body: dict | ``None`

`extra_headers: dict | ``None`

`extra_query: dict | ``None`

`input: str | list[str] | list[int] | list[list[int]] | ``None`



**user**

Python

`timeout: float | ``None`

`user: str | ``None`



**OpenAIPromptExecutionSettings Class**

Reference

Common request settings for (Azure) OpenAI services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OpenAIPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*



**ai_model_id**

Python

**frequency_penalty**

Python

**logit_bias**

Python

**max_tokens**

Python

**metadata**

Python

**number_of_responses**

Python

**Attributes**

`ai_model_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'model'``)]`

`frequency_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`logit_bias: dict[str | int, float] | ``None`

`max_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`metadata: dict[str, str] | ``None`



**presence_penalty**

Python

**seed**

Python

**stop**

Python

**store**

Python

**stream**

Python

**temperature**

Python

`number_of_responses: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, alias_priority=2, serialization_alias=``'n'``, metadata=`

`[Ge(ge=1), Le(le=128)])]`

`presence_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`seed: int | ``None`

`stop: str | list[str] | ``None`

`store: bool | ``None`

`stream: bool`



**top_p**

Python

**user**

Python

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=2.0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`user: str | ``None`



**OpenAISettings Class**

Reference

OpenAI model settings.

The settings are first loaded from environment variables with the prefix
'>>OPENAI_<<'.

If the environment variables are not found, the settings can be loaded from a
.env file

with the encoding 'utf-8'. If the settings are not found in the .env file, the
settings are

ignored; however, validation will fail alerting that the settings are missing.

Optional settings for prefix '>>OPENAI_<<' are:

api_key: SecretStr - OpenAI API key, see
https://platform.openai.com/account/api-

keys

(Env var OPENAI_API_KEY)

org_id: str | None - This is usually optional unless your account belongs to multiple

organizations.

(Env var OPENAI_ORG_ID)

chat_model_id: str | None - The OpenAI chat model ID to use, for example, gpt-3.5-

turbo or gpt-4.

(Env var OPENAI_CHAT_MODEL_ID)

text_model_id: str | None - The OpenAI text model ID to use, for example, gpt-3.5-

turbo-instruct.

(Env var OPENAI_TEXT_MODEL_ID)

embedding_model_id: str | None - The OpenAI embedding model ID to use, for

example, text-embedding-ada-002.

(Env var OPENAI_EMBEDDING_MODEL_ID)

text_to_image_model_id: str | None - The OpenAI text to image model ID to use,

for example, dall-e-3.

(Env var OPENAI_TEXT_TO_IMAGE_MODEL_ID)

audio_to_text_model_id: str | None - The OpenAI audio to text model ID to use, for

example, whisper-1.

(Env var OPENAI_AUDIO_TO_TEXT_MODEL_ID)

text_to_audio_model_id: str | None - The OpenAI text to audio model ID to use, for

example, jukebox-1.



(Env var OPENAI_TEXT_TO_AUDIO_MODEL_ID)

env_file_path: str | None - if provided, the .env settings are read from this file path

location

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`OpenAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr | ``None`` = ``None``, org_id: str | ``None`` = ``None``, chat_model_id: `

`str | ``None`` = ``None``, text_model_id: str | ``None`` = ``None``, embedding_model_id: str `

`| ``None`` = ``None``, text_to_image_model_id: str | ``None`` = ``None``, `

`audio_to_text_model_id: str | ``None`` = ``None``, text_to_audio_model_id: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**org_id**`

Required*

`**chat_model_id**`

Required*

`**text_model_id**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**embedding_model_id**`

Required*

`**text_to_image_model_id**`

Required*

`**audio_to_text_model_id**`

Required*

`**text_to_audio_model_id**`

Required*

**api_key**

Python

**audio_to_text_model_id**

Python

**chat_model_id**

Python

**embedding_model_id**

Python

**Attributes**

`api_key: SecretStr | ``None`

`audio_to_text_model_id: str | ``None`

`chat_model_id: str | ``None`

`embedding_model_id: str | ``None`



**env_prefix**

Python

**org_id**

Python

**text_model_id**

Python

**text_to_audio_model_id**

Python

**text_to_image_model_id**

Python

`env_prefix: ClassVar[str] = ``'OPENAI_'`

`org_id: str | ``None`

`text_model_id: str | ``None`

`text_to_audio_model_id: str | ``None`

`text_to_image_model_id: str | ``None`



**OpenAITextCompletion Class**

Reference

OpenAI Text Completion class.

Initialize an OpenAITextCompletion service.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`OpenAITextCompletion(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> OpenAITextCompletion`

ﾉ

**Expand table**



**OpenAITextEmbedding Class**

Reference

OpenAI Text Embedding class.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the OpenAITextCompletion class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

str

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Service ID tied to the execution settings.

Default value: None

`**api_key**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

<xref:Mapping>[str,str]<xref: | None>

`OpenAITextEmbedding(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

<xref:Optional>[<xref:AsyncOpenAI>]

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.ai.open_ai.str | None>>

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**



**ai_model_type**

Python

**client**

Python

**completion_tokens**

Python

**is_experimental**

Python

**prompt_tokens**

Python

**stage_status**

Python

**total_tokens**

`ai_model_type: OpenAIModelTypes`

`client: AsyncOpenAI`

`completion_tokens: int`

`is_experimental = ``True`

`prompt_tokens: int`

`stage_status = ``'experimental'`



Python

`total_tokens: int`



**OpenAITextPromptExecutionSettings**

**Class**

Reference

Specific settings for the completions endpoint.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`OpenAITextPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, `

`frequency_penalty: Annotated[float | ``None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, `

`logit_bias: dict[str | int, float] | ``None`` = ``None``, max_tokens: Annotated[int `

`| ``None``, Gt(gt=0)] = ``None``, number_of_responses: Annotated[int | ``None``, `

`Ge(ge=1), Le(le=128)] = ``None``, presence_penalty: Annotated[float | ``None``, `

`Ge(ge=-2.0), Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | `

`list[str] | ``None`` = ``None``, stream: bool = ``False``, temperature: Annotated[float `

`| ``None``, Ge(ge=0.0), Le(le=2.0)] = ``None``, top_p: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, user: str | ``None`` = ``None``, store: bool | ``None`` `

`= ``None``, metadata: dict[str, str] | ``None`` = ``None``, prompt: str | ``None`` = ``None``, `

`best_of: Annotated[int | ``None``, Ge(ge=1)] = ``None``, echo: bool = ``False``, `

`logprobs: Annotated[int | ``None``, Ge(ge=0), Le(le=5)] = ``None``, suffix: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**frequency_penalty**`

Required*

`**logit_bias**`

Required*

`**max_tokens**`

Required*

`**number_of_responses**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**stream**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**user**`

Required*

`**store**`

Required*

`**metadata**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**prompt**`

Required*

`**best_of**`

Required*

`**echo**`

Required*

`**logprobs**`

Required*

`**suffix**`

Required*

check_best_of_and_n

Check that the best_of parameter is not greater than the

number_of_responses parameter.

**check_best_of_and_n**

Check that the best_of parameter is not greater than the number_of_responses

parameter.

Python

**best_of**

Python

**Methods**

ﾉ

**Expand table**

`check_best_of_and_n() -> OpenAITextPromptExecutionSettings`

**Attributes**

`best_of: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=1)])]`



**echo**

Python

**logprobs**

Python

**prompt**

Python

**suffix**

Python

`echo: bool`

`logprobs: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0), Le(le=5)])]`

`prompt: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service based on the text content.'``)]`

`suffix: str | ``None`



**OpenAITextToAudio Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToAudio class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAITextToAudio(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_type**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`



**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**OpenAITextToAudioExecutionSettings**

**Class**

Reference

Request settings for OpenAI text to audio services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`OpenAITextToAudioExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, input: `

`str | ``None`` = ``None``, voice: Literal[``'alloy'``, ``'echo'``, ``'fable'``, ``'onyx'``, ``'nova'``, `

`'shimmer'``] = ``'alloy'``, response_format: Literal[``'mp3'``, ``'opus'``,
``'aac'``, ``'flac'``, `

`'wav'``, ``'pcm'``] | ``None`` = ``None``, speed: float | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**ai_model_id**`

Required*

`**input**`

Required*

`**voice**`

Default value: alloy

`**response_format**`

Required*

`**speed**`

Required*

validate_speed

Validate the speed parameter.

**validate_speed**

Validate the speed parameter.

Python

**ai_model_id**

Python

**input**

**Methods**

ﾉ

**Expand table**

`validate_speed() -> OpenAITextToAudioExecutionSettings`

**Attributes**

`ai_model_id: str | ``None`



Python

**response_format**

Python

**speed**

Python

**voice**

Python

`input: str | ``None`

`response_format: Literal[``'mp3'``, ``'opus'``, ``'aac'``, ``'flac'``, ``'wav'``, ``'pcm'``] | `

`None`

`speed: float | ``None`

`voice: Literal[``'alloy'``, ``'echo'``, ``'fable'``, ``'onyx'``, ``'nova'``,
``'shimmer'``]`



**OpenAITextToImage Class**

Reference

OpenAI Text to Image service.

Initializes a new instance of the OpenAITextToImage class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**ai_model_id**`

OpenAI model name, see https://platform.openai.com/docs/models

Default value: None

`**service_id**`

Service ID tied to the execution settings.

Default value: None

`**api_key**`

The optional API key to use. If provided will override, the env vars or .env

file value.

Default value: None

`**org_id**`

The optional org ID to use. If provided will override, the env vars or .env

file value.

Default value: None

`**default_headers**`

The default headers mapping of string keys to string values for HTTP

requests. (Optional)

Default value: None

`**async_client**`

An existing client to use. (Optional)

Default value: None

`**env_file_path**`

`OpenAITextToImage(ai_model_id: str | ``None`` = ``None``, api_key: str | ``None`` = `

`None``, org_id: str | ``None`` = ``None``, service_id: str | ``None`` = ``None``, `

`default_headers: Mapping[str, str] | ``None`` = ``None``, async_client: AsyncOpenAI `

`| ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

The encoding of the environment settings file. (Optional)

Default value: None

from_dict

Initialize an Open AI service from a dictionary of settings.

**from_dict**

Initialize an Open AI service from a dictionary of settings.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

A dictionary of settings for the service.

**ai_model_type**

Python

**Methods**

ﾉ

**Expand table**

`from_dict(settings: dict[str, Any]) -> T_`

ﾉ

**Expand table**

**Attributes**

`ai_model_type: OpenAIModelTypes`



**client**

Python

**completion_tokens**

Python

**prompt_tokens**

Python

**total_tokens**

Python

`client: AsyncOpenAI`

`completion_tokens: int`

`prompt_tokens: int`

`total_tokens: int`



**OpenAITextToImageExecutionSettings**

**Class**

Reference

Request settings for OpenAI text to image services.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`OpenAITextToImageExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, prompt: str | ``None`` = ``None``, `

`ai_model_id: str | ``None`` = ``None``, size: ImageSize | ``None`` = ``None``, quality: str `

`| ``None`` = ``None``, style: str | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**prompt**`

Required*

`**ai_model_id**`

Required*

`**size**`

Required*

`**quality**`

Required*

`**style**`

Required*

check_prompt

Check that the prompt is not empty.

check_size

Check that the requested image size is valid.

prepare_settings_dict

Prepare the settings dictionary for the OpenAI API.

**check_prompt**

Check that the prompt is not empty.

Python

**check_size**

Check that the requested image size is valid.

Python

**Methods**

ﾉ

**Expand table**

`check_prompt() -> OpenAITextToImageExecutionSettings`

`check_size() -> OpenAITextToImageExecutionSettings`



**prepare_settings_dict**

Prepare the settings dictionary for the OpenAI API.

Python

**ai_model_id**

Python

**prompt**

Python

**quality**

Python

**size**

Python

**style**

Python

`prepare_settings_dict(**kwargs) -> dict[str, Any]`

**Attributes**

`ai_model_id: str | ``None`

`prompt: str | ``None`

`quality: str | ``None`

`size: ImageSize | ``None`



`style: str | ``None`



**audio_to_text_client_base Module**

Reference

**Classes**

AudioToTextClientBase

Base class for audio to text client.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AudioToTextClientBase Class**

Reference

Base class for audio to text client.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

get_text_content

Get text content from audio.

get_text_contents

Get text contents from audio.

`AudioToTextClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**get_text_content**

Get text content from audio.

Python

**Parameters**

**Name**

**Description**

`**audio_content**`

Required*

Audio content.

`**settings**`

Required*

Prompt execution settings.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

TextContent

Text content.

**get_text_contents**

Get text contents from audio.

Python

**Parameters**

`async`` get_text_content(audio_content: AudioContent, settings: `

`PromptExecutionSettings | ``None`` = ``None``, **kwargs: Any) -> TextContent`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` get_text_contents(audio_content: AudioContent, settings: `

`PromptExecutionSettings | ``None`` = ``None``, **kwargs: Any) -> `

`list[TextContent]`



**Name**

**Description**

`**audio_content**`

Required*

Audio content.

`**settings**`

Required*

Prompt execution settings.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

list[TextContent]

Text contents.

**ai_model_id**

Python

**service_id**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`

`service_id: str`



**azure_ai_inference_prompt_execution_se**

**ttings Module**

Reference

**Classes**

AzureAIInferenceChatPromptExecutionSettings

Azure AI Inference Chat Prompt Execution

Settings.

Note: This class is marked as

'experimental' and may change in the

future.

Initialize the prompt execution settings.

AzureAIInferenceEmbeddingPromptExecutionSettings

Azure AI Inference Embedding Prompt

Execution Settings.

Note: _extra_parameters_ is a dictionary to

pass additional model-specific

parameters to the model.

Note: This class is marked as

'experimental' and may change in the

future.

Initialize the prompt execution settings.

AzureAIInferencePromptExecutionSettings

Azure AI Inference Prompt Execution

Settings.

Note: _extra_parameters_ is a dictionary to

pass additional model-specific

parameters to the model.

Note: This class is marked as

'experimental' and may change in the

future.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**AzureAIInferenceChatPromptExecution**

**Settings Class**

Reference

Azure AI Inference Chat Prompt Execution Settings.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`AzureAIInferenceChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, `

`*, extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, frequency_penalty: Annotated[float | `

`None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, max_tokens: Annotated[int | ``None``, `

`Gt(gt=0)] = ``None``, presence_penalty: Annotated[float | ``None``, Ge(ge=-2.0), `

`Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | ``None`` = ``None``, `

`temperature: Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, top_p: `

`Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, extra_parameters: `

`dict[str, Any] | ``None`` = ``None``, tools: list[dict[str, Any]] | ``None`` = ``None``, `

`tool_choice: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**frequency_penalty**`

Required*

`**max_tokens**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**extra_parameters**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

**is_experimental**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

**tool_choice**

Python

**tools**

Python

`stage_status = ``'experimental'`

`tool_choice: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, description=``'Do not set this manually. It is set by the
`

`service based on the function choice configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`



**AzureAIInferenceEmbeddingPrompt**

**ExecutionSettings Class**

Reference

Azure AI Inference Embedding Prompt Execution Settings.

Note: _extra_parameters_ is a dictionary to pass additional model-specific
parameters to

the model.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`AzureAIInferenceEmbeddingPromptExecutionSettings(service_id: str | ``None`` = `

`None``, *, extension_data: dict[str, Any] = ``None``,
function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, dimensions: Annotated[int | ``None``, `

`Gt(gt=0)] = ``None``, encoding_format: Literal[``'base64'``, ``'binary'``,
``'float'``, `

`'int8'``, ``'ubinary'``, ``'uint8'``] | ``None`` = ``None``, input_type: Literal[``'text'``, `

`'query'``, ``'document'``] | ``None`` = ``None``, extra_parameters: dict[str, str] | ``None`` `

`= ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**dimensions**`

Required*

`**encoding_format**`

Required*

`**input_type**`

Required*

`**extra_parameters**`

Required*

**dimensions**

Python

**encoding_format**

Python

**extra_parameters**

Python

ﾉ

**Expand table**

**Attributes**

`dimensions: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`encoding_format: Literal[``'base64'``, ``'binary'``, ``'float'``, ``'int8'``,
``'ubinary'``, `

`'uint8'``] | ``None`

`extra_parameters: dict[str, str] | ``None`



**input_type**

Python

**is_experimental**

Python

**stage_status**

Python

`input_type: Literal[``'text'``, ``'query'``, ``'document'``] | ``None`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**AzureAIInferencePromptExecution**

**Settings Class**

Reference

Azure AI Inference Prompt Execution Settings.

Note: _extra_parameters_ is a dictionary to pass additional model-specific
parameters to

the model.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`AzureAIInferencePromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, frequency_penalty: Annotated[float | `

`None``, Ge(ge=-2.0), Le(le=2.0)] = ``None``, max_tokens: Annotated[int | ``None``, `

`Gt(gt=0)] = ``None``, presence_penalty: Annotated[float | ``None``, Ge(ge=-2.0), `

`Le(le=2.0)] = ``None``, seed: int | ``None`` = ``None``, stop: str | ``None`` = ``None``, `

`temperature: Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, top_p: `

`Annotated[float | ``None``, Ge(ge=0.0), Le(le=1.0)] = ``None``, extra_parameters: `

`dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**frequency_penalty**`

Required*

`**max_tokens**`

Required*

`**presence_penalty**`

Required*

`**seed**`

Required*

`**stop**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**extra_parameters**`

Required*

**extra_parameters**

Python

**frequency_penalty**

Python

ﾉ

**Expand table**

**Attributes**

`extra_parameters: dict[str, Any] | ``None`



**is_experimental**

Python

**max_tokens**

Python

**presence_penalty**

Python

**seed**

Python

**stage_status**

Python

**stop**

Python

`frequency_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`is_experimental = ``True`

`max_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`presence_penalty: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=-2.0), Le(le=2.0)])]`

`seed: int | ``None`

`stage_status = ``'experimental'`



**temperature**

Python

**top_p**

Python

`stop: str | ``None`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**azure_ai_inference_settings Module**

Reference

**Classes**

AzureAIInferenceSettings

Azure AI Inference settings.

The settings are first loaded from environment variables with the

prefix '>>AZURE_AI_INFERENCE_<<'. If the environment variables are

not found, the settings can be loaded from a .env file with the

encoding 'utf-8'. If the settings are not found in the .env file, the

settings are ignored; however, validation will fail alerting that the

settings are missing.

Required settings for prefix '>>AZURE_AI_INFERENCE_<<' are:

endpoint: HttpsUrl - The endpoint of the Azure AI Inference

service deployment.

This value can be found in the Keys & Endpoint section when

examining your resource from the Azure portal. (Env var

AZURE_AI_INFERENCE_ENDPOINT)

api_key: SecretStr - The API key for the Azure AI Inference

service deployment.

This value can be found in the Keys & Endpoint section when

examining your resource from the Azure portal. You can use

either KEY1 or KEY2. (Env var AZURE_AI_INFERENCE_API_KEY)

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**AzureAIInferenceSettings Class**

Reference

Azure AI Inference settings.

The settings are first loaded from environment variables with the prefix

'>>AZURE_AI_INFERENCE_<<'. If the environment variables are not found, the
settings

can be loaded from a .env file with the encoding 'utf-8'. If the settings are
not found in

the .env file, the settings are ignored; however, validation will fail
alerting that the

settings are missing.

Required settings for prefix '>>AZURE_AI_INFERENCE_<<' are:

endpoint: HttpsUrl - The endpoint of the Azure AI Inference service
deployment.

This value can be found in the Keys & Endpoint section when examining your

resource from the Azure portal. (Env var AZURE_AI_INFERENCE_ENDPOINT)

api_key: SecretStr - The API key for the Azure AI Inference service
deployment.

This value can be found in the Keys & Endpoint section when examining your

resource from the Azure portal. You can use either KEY1 or KEY2. (Env var

AZURE_AI_INFERENCE_API_KEY)

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

`AzureAIInferenceSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`endpoint: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`



**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)], api_key: SecretStr | ``None`` = ``None``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**endpoint**`

Required*

`**api_key**`

Required*

**api_key**

Python

**endpoint**

Python

**env_prefix**

Python

**is_experimental**

ﾉ

**Expand table**

**Attributes**

`api_key: SecretStr | ``None`

`endpoint: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)]`

`env_prefix: ClassVar[str] = ``'AZURE_AI_INFERENCE_'`



Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**bedrock_prompt_execution_settings**

**Module**

Reference

**Classes**

BedrockChatPromptExecutionSettings

Bedrock Chat Prompt Execution Settings.

Initialize the prompt execution settings.

BedrockEmbeddingPromptExecutionSettings

Bedrock Embedding Prompt Execution Settings.

Initialize the prompt execution settings.

BedrockPromptExecutionSettings

Bedrock Prompt Execution Settings.

Initialize the prompt execution settings.

BedrockTextPromptExecutionSettings

Bedrock Text Prompt Execution Settings.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**BedrockChatPromptExecutionSettings**

**Class**

Reference

Bedrock Chat Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

`BedrockChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, temperature: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, top_p: Annotated[float | ``None``, Ge(ge=0.0), `

`Le(le=1.0)] = ``None``, top_k: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`max_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, stop: list[str] = ``None``, `

`tools: list[dict[str, Any]] | ``None`` = ``None``, tool_choice: dict[str, Any] | `

`None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**function_choice_behavior**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

`**max_tokens**`

Required*

`**stop**`

Required*

`**tools**`

Required*

`**tool_choice**`

Required*

**tool_choice**

Python

**tools**

Python

**Attributes**

`tool_choice: Annotated[dict[str, Any] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `



`this manually. It is set by the service based on the function choice `

`configuration.'``)]`



**BedrockEmbeddingPromptExecution**

**Settings Class**

Reference

Bedrock Embedding Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`BedrockEmbeddingPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**extension_data**

Python

**function_choice_behavior**

Python

**service_id**

Python

**Attributes**

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, exclude=``True``)]`

`service_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[MinLen(min_length=1)])]`



**BedrockPromptExecutionSettings Class**

Reference

Bedrock Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`BedrockPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, temperature: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, top_p: Annotated[float | ``None``, Ge(ge=0.0), `

`Le(le=1.0)] = ``None``, top_k: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`max_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, stop: list[str] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

`**max_tokens**`

Required*

`**stop**`

Required*

**max_tokens**

Python

**stop**

Python

**temperature**

Python

**top_k**

Python

**Attributes**

`max_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`stop: list[str]`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**top_p**

Python

`top_k: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**BedrockTextPromptExecutionSettings**

**Class**

Reference

Bedrock Text Prompt Execution Settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`BedrockTextPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, temperature: Annotated[float | ``None``, `

`Ge(ge=0.0), Le(le=1.0)] = ``None``, top_p: Annotated[float | ``None``, Ge(ge=0.0), `

`Le(le=1.0)] = ``None``, top_k: Annotated[int | ``None``, Gt(gt=0)] = ``None``, `

`max_tokens: Annotated[int | ``None``, Gt(gt=0)] = ``None``, stop: list[str] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**temperature**`

Required*

`**top_p**`

Required*

`**top_k**`

Required*

`**max_tokens**`

Required*

`**stop**`

Required*

**extension_data**

Python

**function_choice_behavior**

Python

**max_tokens**

Python

**Attributes**

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`Field(exclude=``True``)]`

`max_tokens: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`



**service_id**

Python

**stop**

Python

**temperature**

Python

**top_k**

Python

**top_p**

Python

`service_id: Annotated[str | ``None``, Field(min_length=1)]`

`stop: list[str]`

`temperature: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`

`top_k: Annotated[int | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Gt(gt=0)])]`

`top_p: Annotated[float | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[Ge(ge=0.0), Le(le=1.0)])]`



**bedrock_settings Module**

Reference

**Classes**

BedrockSettings

Amazon Bedrock service settings.

The settings are first loaded from environment variables with the prefix

'>>BEDROCK_<<'. If the environment variables are not found, the settings can

be loaded from a .env file with the encoding 'utf-8'. If the settings are not

found in the .env file, the settings are ignored; however, validation will
fail

alerting that the settings are missing.

Optional settings for prefix '>>BEDROCK_<<' are: * chat_model_id: str | None -

The Amazon Bedrock chat model ID to use.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**

` (Env var BEDROCK_CHAT_MODEL_ID)`

` * text_model_id: str | None - The Amazon Bedrock text `

`model ID to use.`

` (Env var BEDROCK_TEXT_MODEL_ID)`

` * embedding_model_id: str | None - The Amazon Bedrock `

`embedding model ID to use.`

` (Env var BEDROCK_EMBEDDING_MODEL_ID)`



**BedrockSettings Class**

Reference

Amazon Bedrock service settings.

The settings are first loaded from environment variables with the prefix

'>>BEDROCK_<<'. If the environment variables are not found, the settings can
be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the .env

file, the settings are ignored; however, validation will fail alerting that
the settings are

missing.

Optional settings for prefix '>>BEDROCK_<<' are: * chat_model_id: str | None - The

Amazon Bedrock chat model ID to use.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

` (Env var BEDROCK_CHAT_MODEL_ID)`

` * text_model_id: str | None - The Amazon Bedrock text model ID to use.`

` (Env var BEDROCK_TEXT_MODEL_ID)`

` * embedding_model_id: str | None - The Amazon Bedrock embedding model ID `

`to use.`

` (Env var BEDROCK_EMBEDDING_MODEL_ID)`

`BedrockSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `



**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`chat_model_id: str | ``None`` = ``None``, text_model_id: str | ``None`` = ``None``, `

`embedding_model_id: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**chat_model_id**`

Required*

`**text_model_id**`

Required*

`**embedding_model_id**`

Required*

**chat_model_id**

Python

**embedding_model_id**

Python

**env_prefix**

Python

**is_experimental**

ﾉ

**Expand table**

**Attributes**

`chat_model_id: str | ``None`

`embedding_model_id: str | ``None`

`env_prefix: ClassVar[str] = ``'BEDROCK_'`



Python

**stage_status**

Python

**text_model_id**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`

`text_model_id: str | ``None`



**chat_completion_client_base Module**

Reference

**Classes**

ChatCompletionClientBase

Base class for chat completion AI services.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatCompletionClientBase Class**

Reference

Base class for chat completion AI services.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to

form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`**instruction_role**`

Required*

get_chat_message_content

This is the method that is called from the kernel to get a

response from a chat-optimized LLM.

get_chat_message_contents

Create chat message contents, in the number specified by the

settings.

`ChatCompletionClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str = ``''``, `

`instruction_role: str = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_streaming_chat_message_content

This is the method that is called from the kernel to get a stream

response from a chat-optimized LLM.

get_streaming_chat_message_contents

Create streaming chat message contents, in the number

specified by the settings.

**get_chat_message_content**

This is the method that is called from the kernel to get a response from a
chat-optimized

LLM.

Python

**Parameters**

**Name**

**Description**

`**chat_history**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.ChatHistory>

A list of chat chat_history, that can be rendered into a set of chat_history,
from system, user,

assistant and function.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.PromptExecutionSettings>

Settings for the request.

`**kwargs**`

Required*

<xref:Dict>[str,<xref: Any>]

The optional arguments.

**Returns**

**Type**

**Description**

A string representing the response from the LLM.

**get_chat_message_contents**

Create chat message contents, in the number specified by the settings.

Python

`async`` get_chat_message_content(chat_history: ChatHistory, settings: `

`PromptExecutionSettings, **kwargs: Any) -> ChatMessageContent | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**chat_history**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.ChatHistory>

A list of chats in a chat_history object, that can be rendered into messages
from system,

user, assistant and tools.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.PromptExecutionSettings>

Settings for the request.

`****kwargs**`

Required*

Any

The optional arguments.

**Returns**

**Type**

**Description**

A list of chat message contents representing the response(s) from the LLM.

**get_streaming_chat_message_content**

This is the method that is called from the kernel to get a stream response
from a chat-

optimized LLM.

Python

**Parameters**

**Name**

**Description**

`**chat_history**`

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.ChatHistory>

`async`` get_chat_message_contents(chat_history: ChatHistory, settings: `

`PromptExecutionSettings, **kwargs: Any) -> list[ChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_streaming_chat_message_content(chat_history: ChatHistory,
settings: `

`PromptExecutionSettings, **kwargs: Any) -> `

`AsyncGenerator[StreamingChatMessageContent | ``None``, Any]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

A list of chat chat_history, that can be rendered into a set of chat_history,
from system, user,

assistant and function.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.PromptExecutionSettings>

Settings for the request.

`**kwargs**`

Required*

<xref:Dict>[str,<xref: Any>]

The optional arguments.

**get_streaming_chat_message_contents**

Create streaming chat message contents, in the number specified by the
settings.

Python

**Parameters**

**Name**

**Description**

`**chat_history**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.ChatHistory>

A list of chat chat_history, that can be rendered into a set of chat_history,
from system, user,

assistant and function.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.chat_completion_client_base.PromptExecutionSettings>

Settings for the request.

`**kwargs**`

Required*

<xref:Dict>[str,<xref: Any>]

The optional arguments.

**SUPPORTS_FUNCTION_CALLING**

Python

**instruction_role**

`async`` get_streaming_chat_message_contents(chat_history: ChatHistory,
settings: `

`PromptExecutionSettings, **kwargs: Any) -> `

`AsyncGenerator[list[StreamingChatMessageContent], Any]`

ﾉ

**Expand table**

**Attributes**

`SUPPORTS_FUNCTION_CALLING: ClassVar[bool] = ``False`



Python

`instruction_role: str`



**completion_usage Module**

Reference

**Classes**

CompletionUsage

Completion usage information.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**CompletionUsage Class**

Reference

Completion usage information.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_tokens**`

Required*

`**completion_tokens**`

Required*

from_openai

Create a CompletionUsage object from an OpenAI response.

**from_openai**

Create a CompletionUsage object from an OpenAI response.

`CompletionUsage(*, prompt_tokens: int | ``None`` = ``None``, completion_tokens: int `

`| ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**openai_completion_usage**`

Required*

**completion_tokens**

Python

**prompt_tokens**

Python

`from_openai(openai_completion_usage: CompletionUsage)`

ﾉ

**Expand table**

**Attributes**

`completion_tokens: int | ``None`

`prompt_tokens: int | ``None`



**function_call_choice_configuration**

**Module**

Reference

**Classes**

FunctionCallChoiceConfiguration

Configuration for function call choice.

Note: This class is marked as 'experimental' and may change in

the future.

ﾉ

**Expand table**



**FunctionCallChoiceConfiguration Class**

Reference

Configuration for function call choice.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**available_functions**`

Default value: None

**available_functions**

Python

**is_experimental**

Python

**stage_status**

`FunctionCallChoiceConfiguration(available_functions: `

`list[KernelFunctionMetadata] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`available_functions: list[KernelFunctionMetadata] | ``None`` = ``None`

`is_experimental = ``True`



Python

`stage_status = ``'experimental'`



**function_calling_utils Module**

Reference

**kernel_function_metadata_to_function_call_format**

Convert the kernel function metadata to function calling format.

Python

**Parameters**

**Name**

**Description**

`**metadata**`

Required*

**merge_function_results**

Combine multiple function result content types to one chat message content
type.

This method combines the FunctionResultContent items from separate

ChatMessageContent messages, and is used in the event that the
_context.terminate =_

_True_ condition is met.

Python

**Parameters**

**Functions**

`kernel_function_metadata_to_function_call_format(metadata: `

`KernelFunctionMetadata) -> dict[str, Any]`

ﾉ

**Expand table**

`merge_function_results(messages: list[ChatMessageContent]) -> `

`list[ChatMessageContent]`

ﾉ

**Expand table**



**Name**

**Description**

`**messages**`

Required*

**merge_streaming_function_results**

Combine multiple streaming function result content types to one streaming chat

message content type.

This method combines the FunctionResultContent items from separate

StreamingChatMessageContent messages, and is used in the event that the

_context.terminate = True_ condition is met.

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

The list of streaming chat message content types.

`**ai_model_id**`

Required*

The AI model ID.

Default value: None

`**function_invoke_attempt**`

Required*

The function invoke attempt.

Default value: None

**Returns**

**Type**

**Description**

The combined streaming chat message content type.

`merge_streaming_function_results(messages: list[ChatMessageContent | `

`StreamingChatMessageContent], ai_model_id: str | ``None`` = ``None``, `

`function_invoke_attempt: int | ``None`` = ``None``) -> `

`list[StreamingChatMessageContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**update_settings_from_function_call_configuration**

Update the settings from a FunctionChoiceConfiguration.

Python

**Parameters**

**Name**

**Description**

`**function_choice_configuration**`

Required*

`**settings**`

Required*

`**type**`

Required*

`update_settings_from_function_call_configuration(function_choice_configur`

`ation: FunctionCallChoiceConfiguration, settings: `

`PromptExecutionSettings, type: FunctionChoiceType) -> ``None`

ﾉ

**Expand table**



**function_choice_behavior Module**

Reference

**Classes**

FunctionChoiceBehavior

Class that controls function choice behavior.

Attributes: enable_kernel_functions: Enable kernel functions.

max_auto_invoke_attempts: The maximum number of auto invoke

attempts. filters: Filters for the function choice behavior. Available

options are: excluded_plugins,

Properties: auto_invoke_kernel_functions: Check if the kernel functions

should be auto-invoked. Determined as max_auto_invoke_attempts >

0.

Methods: configure: Configures the settings for the function call

behavior, the default version in this class, does nothing, use subclasses

for different behaviors.

Class methods: Auto: Returns FunctionChoiceBehavior class with

auto_invoke enabled, and the desired functions based on either the

specified filters or the full qualified names. The model will decide which

function to use, if any.

ﾉ

**Expand table**

` included_plugins, excluded_functions, or `

`included_functions.`

` >>type_<<: The type of function choice behavior.`

` NoneInvoke: Returns FunctionChoiceBehavior class `

`with auto_invoke disabled, and the desired functions`

` based on either the specified filters or the `

`full qualified names. The model does not invoke any `

`functions,`

` but can rather describe how it would invoke a `

`function to complete a given task/query.`

` Required: Returns FunctionChoiceBehavior class `

`with auto_invoke enabled, and the desired functions`

` based on either the specified filters or the `



Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

`full qualified names. The model is required to use `

`one of the`

` provided functions to complete a given `

`task/query.`



**FunctionChoiceBehavior Class**

Reference

Class that controls function choice behavior.

Attributes: enable_kernel_functions: Enable kernel functions.
max_auto_invoke_attempts:

The maximum number of auto invoke attempts. filters: Filters for the function
choice

behavior. Available options are: excluded_plugins,

Properties: auto_invoke_kernel_functions: Check if the kernel functions should
be auto-

invoked. Determined as max_auto_invoke_attempts > 0.

Methods: configure: Configures the settings for the function call behavior,
the default

version in this class, does nothing, use subclasses for different behaviors.

Class methods: Auto: Returns FunctionChoiceBehavior class with auto_invoke
enabled,

and the desired functions based on either the specified filters or the full
qualified names.

The model will decide which function to use, if any.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

` included_plugins, excluded_functions, or included_functions.`

` >>type_<<: The type of function choice behavior.`

` NoneInvoke: Returns FunctionChoiceBehavior class with auto_invoke `

`disabled, and the desired functions`

` based on either the specified filters or the full qualified names. The `

`model does not invoke any functions,`

` but can rather describe how it would invoke a function to complete a `

`given task/query.`

` Required: Returns FunctionChoiceBehavior class with auto_invoke enabled, `

`and the desired functions`

` based on either the specified filters or the full qualified names. The `

`model is required to use one of the`

` provided functions to complete a given task/query.`



Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**enable_kernel_functions**`

Default value: True

`**maximum_auto_invoke_attempts**`

Default value: 5

`**filters**`

Required*

`**type_**`

Required*

Auto

Creates a FunctionChoiceBehavior with type AUTO.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

will decide which function to use, if any.

NoneInvoke

Creates a FunctionChoiceBehavior with type NONE.

Returns FunctionChoiceBehavior class with auto_invoke disabled, and the
desired

functions based on either the specified filters or the full qualified names.
The model

`FunctionChoiceBehavior(*, enable_kernel_functions: bool = ``True``, `

`maximum_auto_invoke_attempts: int = 5, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``,
``'excluded_functions'``, `

`'included_functions'``], list[str]] | ``None`` = ``None``, type_: FunctionChoiceType | `

`None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



does not invoke any functions, but can rather describe how it would invoke a

function to complete a given task/query.

Required

Creates a FunctionChoiceBehavior with type REQUIRED.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

is required to use one of the provided functions to complete a given
task/query.

configure

Configure the function choice behavior.

from_dict

Create a FunctionChoiceBehavior from a dictionary.

from_string

Create a FunctionChoiceBehavior from a string.

This method converts the provided string to a FunctionChoiceBehavior object
for

the specified type.

get_config

Get the function call choice configuration based on the type.

**Auto**

Creates a FunctionChoiceBehavior with type AUTO.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

will decide which function to use, if any.

Python

**Parameters**

**Name**

**Description**

`**auto_invoke**`

Default value: True

**Keyword-Only Parameters**

`Auto(auto_invoke: bool = ``True``, *, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`` = ``None``, `

`**kwargs) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**filters**`

Required*

**NoneInvoke**

Creates a FunctionChoiceBehavior with type NONE.

Returns FunctionChoiceBehavior class with auto_invoke disabled, and the
desired

functions based on either the specified filters or the full qualified names.
The model

does not invoke any functions, but can rather describe how it would invoke a

function to complete a given task/query.

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filters**`

Required*

**Required**

Creates a FunctionChoiceBehavior with type REQUIRED.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

is required to use one of the provided functions to complete a given
task/query.

Python

ﾉ

**Expand table**

`NoneInvoke(*, filters: dict[Literal[``'excluded_plugins'``, `

`'included_plugins'``, ``'excluded_functions'``, ``'included_functions'``], `

`list[str]] | ``None`` = ``None``, **kwargs) -> _T`

ﾉ

**Expand table**

`Required(auto_invoke: bool = ``True``, *, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `



**Parameters**

**Name**

**Description**

`**auto_invoke**`

Default value: True

**Keyword-Only Parameters**

**Name**

**Description**

`**filters**`

Required*

**configure**

Configure the function choice behavior.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**update_settings_callback**`

Required*

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`` = ``None``, `

`**kwargs) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`configure(kernel: Kernel, update_settings_callback: Callable[[...], `

`None``], settings: PromptExecutionSettings) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**settings**`

Required*

**from_dict**

Create a FunctionChoiceBehavior from a dictionary.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**from_string**

Create a FunctionChoiceBehavior from a string.

This method converts the provided string to a FunctionChoiceBehavior object
for the

specified type.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

`from_dict(data: dict) -> _T`

ﾉ

**Expand table**

`from_string(data: str) -> _T`

ﾉ

**Expand table**



**get_config**

Get the function call choice configuration based on the type.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

**auto_invoke_kernel_functions**

Return True if auto_invoke_kernel_functions is enabled.

**enable_kernel_functions**

Python

**filters**

Python

**is_experimental**

Python

`get_config(kernel: Kernel) -> FunctionCallChoiceConfiguration`

ﾉ

**Expand table**

**Attributes**

`enable_kernel_functions: bool`

`filters: dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`



**maximum_auto_invoke_attempts**

Python

**stage_status**

Python

**type_**

Python

`is_experimental = ``True`

`maximum_auto_invoke_attempts: int`

`stage_status = ``'experimental'`

`type_: FunctionChoiceType | ``None`



**function_choice_type Module**

Reference

**Enums**

FunctionChoiceType

The type of function choice behavior.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**FunctionChoiceType Enum**

Reference

The type of function choice behavior.

Note: This class is marked as 'experimental' and may change in the future.

AUTO

NONE

REQUIRED

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**hf_prompt_execution_settings Module**

Reference

**Classes**

HuggingFacePromptExecutionSettings

Hugging Face prompt execution settings.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**HuggingFacePromptExecutionSettings**

**Class**

Reference

Hugging Face prompt execution settings.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`HuggingFacePromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, do_sample: bool = ``True``, `

`max_new_tokens: int = 256, num_return_sequences: int = 1, stop_sequences: `

`Any = ``None``, pad_token_id: int = 50256, eos_token_id: int = 50256, `

`temperature: float = 1.0, top_p: float = 1.0)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function_choice_behavior**`

Required*

`**do_sample**`

Default value: True

`**max_new_tokens**`

Default value: 256

`**num_return_sequences**`

Default value: 1

`**stop_sequences**`

Required*

`**pad_token_id**`

Default value: 50256

`**eos_token_id**`

Default value: 50256

`**temperature**`

Default value: 1.0

`**top_p**`

Default value: 1.0

get_generation_config

Get the generation config.

prepare_settings_dict

Prepare the settings dictionary.

**get_generation_config**

Get the generation config.

Python

**prepare_settings_dict**

Prepare the settings dictionary.

Python

**Methods**

ﾉ

**Expand table**

`get_generation_config() -> Any`

`prepare_settings_dict(**kwargs) -> dict[str, Any]`



**do_sample**

Python

**eos_token_id**

Python

**max_new_tokens**

Python

**num_return_sequences**

Python

**pad_token_id**

Python

**stop_sequences**

Python

**Attributes**

`do_sample: bool`

`eos_token_id: int`

`max_new_tokens: int`

`num_return_sequences: int`

`pad_token_id: int`

`stop_sequences: Any`



**temperature**

Python

**top_p**

Python

`temperature: float`

`top_p: float`



**ollama_prompt_execution_settings**

**Module**

Reference

**Classes**

OllamaChatPromptExecutionSettings

Settings for Ollama chat prompt execution.

Initialize the prompt execution settings.

OllamaEmbeddingPromptExecutionSettings

Settings for Ollama embedding prompt execution.

Initialize the prompt execution settings.

OllamaPromptExecutionSettings

Settings for Ollama prompt execution.

Initialize the prompt execution settings.

OllamaTextPromptExecutionSettings

Settings for Ollama text prompt execution.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**OllamaChatPromptExecutionSettings**

**Class**

Reference

Settings for Ollama chat prompt execution.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`OllamaChatPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, format: Literal[``'json'``] | ``None`` = ``None``, `

`options: dict[str, Any] | ``None`` = ``None``, tools: list[dict[str, Any]] | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**format**`

Required*

`**options**`

Required*

`**tools**`

Required*

**tools**

Python

**Attributes**

`tools: Annotated[list[dict[str, Any]] | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, description=``'Do not set `

`this manually. It is set by the service based on the function choice `

`configuration.'``)]`



**OllamaEmbeddingPromptExecution**

**Settings Class**

Reference

Settings for Ollama embedding prompt execution.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`OllamaEmbeddingPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, format: Literal[``'json'``] | ``None`` = ``None``, `

`options: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**format**`

Required*

`**options**`

Required*

**format**

Python

**options**

Python

**Attributes**

`format: Literal[``'json'``] | ``None`

`options: dict[str, Any] | ``None`



**OllamaPromptExecutionSettings Class**

Reference

Settings for Ollama prompt execution.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

`**format**`

`OllamaPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, format: Literal[``'json'``] | ``None`` = ``None``, `

`options: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**options**`

Required*

**format**

Python

**options**

Python

**Attributes**

`format: Literal[``'json'``] | ``None`

`options: dict[str, Any] | ``None`



**OllamaTextPromptExecutionSettings**

**Class**

Reference

Settings for Ollama text prompt execution.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

`OllamaTextPromptExecutionSettings(service_id: str | ``None`` = ``None``, *, `

`extension_data: dict[str, Any] = ``None``, function_choice_behavior: `

`FunctionChoiceBehavior | ``None`` = ``None``, format: Literal[``'json'``] | ``None`` = ``None``, `

`options: dict[str, Any] | ``None`` = ``None``, system: str | ``None`` = ``None``, template: `

`str | ``None`` = ``None``, context: str | ``None`` = ``None``, raw: bool | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**format**`

Required*

`**options**`

Required*

`**system**`

Required*

`**template**`

Required*

`**context**`

Required*

`**raw**`

Required*

**context**

Python

**raw**

Python

**system**

Python

**template**

**Attributes**

`context: str | ``None`

`raw: bool | ``None`

`system: str | ``None`



Python

`template: str | ``None`



**prompt_execution_settings Module**

Reference

**Classes**

PromptExecutionSettings

Base class for prompt execution settings.

Can be used by itself or as a base class for other prompt execution

settings. The methods are used to create specific prompt execution

settings objects based on the keys in the extension_data field, this way

you can create a generic PromptExecutionSettings object in your

application, which gets mapped into the keys of the prompt execution

settings that each services returns by using the

service.get_prompt_execution_settings() method.

Initialize the prompt execution settings.

ﾉ

**Expand table**



**PromptExecutionSettings Class**

Reference

Base class for prompt execution settings.

Can be used by itself or as a base class for other prompt execution settings.
The

methods are used to create specific prompt execution settings objects based on
the

keys in the extension_data field, this way you can create a generic

PromptExecutionSettings object in your application, which gets mapped into the
keys of

the prompt execution settings that each services returns by using the

service.get_prompt_execution_settings() method.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`PromptExecutionSettings(service_id: str | ``None`` = ``None``, *, extension_data: `

`dict[str, Any] = ``None``, function_choice_behavior: FunctionChoiceBehavior | `

`None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

from_prompt_execution_settings

Create a prompt execution settings from another

prompt execution settings.

pack_extension_data

Update the extension data from the prompt execution

settings.

parse_function_choice_behavior

Parse the function choice behavior data.

prepare_settings_dict

Prepares the settings as a dictionary for sending to the

AI service.

unpack_extension_data

Update the prompt execution settings from extension

data.

Does not overwrite existing values with None.

update_from_prompt_execution_settings

Update the keys from another prompt execution

settings object.

**from_prompt_execution_settings**

Create a prompt execution settings from another prompt execution settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_prompt_execution_settings()`

ﾉ

**Expand table**



**Name**

**Description**

`**config**`

Required*

**pack_extension_data**

Update the extension data from the prompt execution settings.

Python

**parse_function_choice_behavior**

Parse the function choice behavior data.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**prepare_settings_dict**

Prepares the settings as a dictionary for sending to the AI service.

Python

**unpack_extension_data**

Update the prompt execution settings from extension data.

`pack_extension_data() -> ``None`

`parse_function_choice_behavior(data: Any) -> dict[str, Any]`

ﾉ

**Expand table**

`prepare_settings_dict()`



Does not overwrite existing values with None.

Python

**update_from_prompt_execution_settings**

Update the keys from another prompt execution settings object.

Python

**Parameters**

**Name**

**Description**

`**config**`

Required*

**keys**

Get the keys of the prompt execution settings.

**service_id**

The service ID to use for the request.

Python

**extension_data**

`unpack_extension_data() -> ``None`

`update_from_prompt_execution_settings()`

ﾉ

**Expand table**

**Attributes**

`service_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[MinLen(min_length=1)])]`



Any additional data to send with the request.

Python

**function_choice_behavior**

The function choice behavior settings.

Python

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, exclude=``True``)]`



**text_completion_client_base Module**

Reference

**Classes**

TextCompletionClientBase

Base class for text completion AI services.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextCompletionClientBase Class**

Reference

Base class for text completion AI services.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

get_streaming_text_content

This is the method that is called from the kernel to get a stream

response from a text-optimized LLM.

get_streaming_text_contents

Create streaming text contents, in the number specified by the

settings.

`TextCompletionClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str = `

`''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_text_content

This is the method that is called from the kernel to get a response

from a text-optimized LLM.

get_text_contents

Create text contents, in the number specified by the settings.

**get_streaming_text_content**

This is the method that is called from the kernel to get a stream response
from a text-

optimized LLM.

Python

**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to send to the LLM.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.text_completion_client_base.PromptExecutionSettings>

Settings for the request.

**Returns**

**Type**

**Description**

StreamingTextContent

A stream representing the response(s) from the LLM.

**get_streaming_text_contents**

Create streaming text contents, in the number specified by the settings.

Python

`async`` get_streaming_text_content(prompt: str, settings: `

`PromptExecutionSettings) -> AsyncGenerator[StreamingTextContent | ``None``, `

`Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_streaming_text_contents(prompt: str, settings: `

`PromptExecutionSettings) -> AsyncGenerator[list[StreamingTextContent], `



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to send to the LLM.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.text_completion_client_base.PromptExecutionSettings>

Settings for the request.

**get_text_content**

This is the method that is called from the kernel to get a response from a
text-

optimized LLM.

Python

**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to send to the LLM.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.text_completion_client_base.PromptExecutionSettings>

Settings for the request.

**Returns**

**Type**

**Description**

TextContent

A string or list of strings representing the response(s) from the LLM.

`Any]`

ﾉ

**Expand table**

`async`` get_text_content(prompt: str, settings: PromptExecutionSettings) -> `

`TextContent | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**get_text_contents**

Create text contents, in the number specified by the settings.

Python

**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to send to the LLM.

`**settings**`

Required*

<xref:semantic_kernel.connectors.ai.text_completion_client_base.PromptExecutionSettings>

Settings for the request.

**Returns**

**Type**

**Description**

list[TextContent]

A string or list of strings representing the response(s) from the LLM.

**ai_model_id**

Python

**service_id**

Python

`async`` get_text_contents(prompt: str, settings: PromptExecutionSettings) ->
`

`list[TextContent]`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`



`service_id: str`



**text_to_audio_client_base Module**

Reference

**Classes**

TextToAudioClientBase

Base class for text to audio client.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextToAudioClientBase Class**

Reference

Base class for text to audio client.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

get_audio_content

Get audio content from text.

get_audio_contents

Get audio contents from text.

`TextToAudioClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**get_audio_content**

Get audio content from text.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

The text to convert to audio.

`**settings**`

Required*

Prompt execution settings.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

AudioContent

The generated audio content.

**get_audio_contents**

Get audio contents from text.

Python

**Parameters**

`async`` get_audio_content(text: str, settings: PromptExecutionSettings | `

`None`` = ``None``, **kwargs: Any) -> AudioContent`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` get_audio_contents(text: str, settings: `

`PromptExecutionSettings | ``None`` = ``None``, **kwargs: Any) -> `

`list[AudioContent]`



**Name**

**Description**

`**text**`

Required*

The text to convert to audio.

`**settings**`

Required*

Prompt execution settings.

Default value: None

`**kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

list[AudioCo

ntent]

The generated audio contents.

Some services may return multiple audio contents in one call. some services

don't. It is ok to return a list of one element.

**ai_model_id**

Python

**service_id**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`

`service_id: str`



**text_to_image_client_base Module**

Reference

**Classes**

TextToImageClientBase

Base class for text to image client.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextToImageClientBase Class**

Reference

Base class for text to image client.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

generate_image

Generate image from text.

**generate_image**

`TextToImageClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Generate image from text.

Python

**Parameters**

**Name**

**Description**

`**description**`

Required*

Description of the image.

`**width**`

Required*

Width of the image.

`**height**`

Required*

Height of the image.

`**kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

bytes | str

Image bytes or image URL.

**ai_model_id**

Python

`abstract ``async`` generate_image(description: str, width: int, height: int,
`

`**kwargs: Any) -> bytes | str`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`



**service_id**

Python

`service_id: str`



**FunctionChoiceBehavior Class**

Reference

Class that controls function choice behavior.

Attributes: enable_kernel_functions: Enable kernel functions.
max_auto_invoke_attempts:

The maximum number of auto invoke attempts. filters: Filters for the function
choice

behavior. Available options are: excluded_plugins,

Properties: auto_invoke_kernel_functions: Check if the kernel functions should
be auto-

invoked. Determined as max_auto_invoke_attempts > 0.

Methods: configure: Configures the settings for the function call behavior,
the default

version in this class, does nothing, use subclasses for different behaviors.

Class methods: Auto: Returns FunctionChoiceBehavior class with auto_invoke
enabled,

and the desired functions based on either the specified filters or the full
qualified names.

The model will decide which function to use, if any.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

` included_plugins, excluded_functions, or included_functions.`

` >>type_<<: The type of function choice behavior.`

` NoneInvoke: Returns FunctionChoiceBehavior class with auto_invoke `

`disabled, and the desired functions`

` based on either the specified filters or the full qualified names. The `

`model does not invoke any functions,`

` but can rather describe how it would invoke a function to complete a `

`given task/query.`

` Required: Returns FunctionChoiceBehavior class with auto_invoke enabled, `

`and the desired functions`

` based on either the specified filters or the full qualified names. The `

`model is required to use one of the`

` provided functions to complete a given task/query.`



Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**enable_kernel_functions**`

Default value: True

`**maximum_auto_invoke_attempts**`

Default value: 5

`**filters**`

Required*

`**type_**`

Required*

Auto

Creates a FunctionChoiceBehavior with type AUTO.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

will decide which function to use, if any.

NoneInvoke

Creates a FunctionChoiceBehavior with type NONE.

Returns FunctionChoiceBehavior class with auto_invoke disabled, and the
desired

functions based on either the specified filters or the full qualified names.
The model

`FunctionChoiceBehavior(*, enable_kernel_functions: bool = ``True``, `

`maximum_auto_invoke_attempts: int = 5, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``,
``'excluded_functions'``, `

`'included_functions'``], list[str]] | ``None`` = ``None``, type_: FunctionChoiceType | `

`None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



does not invoke any functions, but can rather describe how it would invoke a

function to complete a given task/query.

Required

Creates a FunctionChoiceBehavior with type REQUIRED.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

is required to use one of the provided functions to complete a given
task/query.

configure

Configure the function choice behavior.

from_dict

Create a FunctionChoiceBehavior from a dictionary.

from_string

Create a FunctionChoiceBehavior from a string.

This method converts the provided string to a FunctionChoiceBehavior object
for

the specified type.

get_config

Get the function call choice configuration based on the type.

**Auto**

Creates a FunctionChoiceBehavior with type AUTO.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

will decide which function to use, if any.

Python

**Parameters**

**Name**

**Description**

`**auto_invoke**`

Default value: True

**Keyword-Only Parameters**

`Auto(auto_invoke: bool = ``True``, *, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`` = ``None``, `

`**kwargs) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**filters**`

Required*

**NoneInvoke**

Creates a FunctionChoiceBehavior with type NONE.

Returns FunctionChoiceBehavior class with auto_invoke disabled, and the
desired

functions based on either the specified filters or the full qualified names.
The model

does not invoke any functions, but can rather describe how it would invoke a

function to complete a given task/query.

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filters**`

Required*

**Required**

Creates a FunctionChoiceBehavior with type REQUIRED.

Returns FunctionChoiceBehavior class with auto_invoke enabled, and the desired

functions based on either the specified filters or the full qualified names.
The model

is required to use one of the provided functions to complete a given
task/query.

Python

ﾉ

**Expand table**

`NoneInvoke(*, filters: dict[Literal[``'excluded_plugins'``, `

`'included_plugins'``, ``'excluded_functions'``, ``'included_functions'``], `

`list[str]] | ``None`` = ``None``, **kwargs) -> _T`

ﾉ

**Expand table**

`Required(auto_invoke: bool = ``True``, *, filters: `

`dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `



**Parameters**

**Name**

**Description**

`**auto_invoke**`

Default value: True

**Keyword-Only Parameters**

**Name**

**Description**

`**filters**`

Required*

**configure**

Configure the function choice behavior.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**update_settings_callback**`

Required*

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`` = ``None``, `

`**kwargs) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`configure(kernel: Kernel, update_settings_callback: Callable[[...], `

`None``], settings: PromptExecutionSettings) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**settings**`

Required*

**from_dict**

Create a FunctionChoiceBehavior from a dictionary.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**from_string**

Create a FunctionChoiceBehavior from a string.

This method converts the provided string to a FunctionChoiceBehavior object
for the

specified type.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

`from_dict(data: dict) -> _T`

ﾉ

**Expand table**

`from_string(data: str) -> _T`

ﾉ

**Expand table**



**get_config**

Get the function call choice configuration based on the type.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

**auto_invoke_kernel_functions**

Return True if auto_invoke_kernel_functions is enabled.

**enable_kernel_functions**

Python

**filters**

Python

**is_experimental**

Python

`get_config(kernel: Kernel) -> FunctionCallChoiceConfiguration`

ﾉ

**Expand table**

**Attributes**

`enable_kernel_functions: bool`

`filters: dict[Literal[``'excluded_plugins'``, ``'included_plugins'``, `

`'excluded_functions'``, ``'included_functions'``], list[str]] | ``None`



**maximum_auto_invoke_attempts**

Python

**stage_status**

Python

**type_**

Python

`is_experimental = ``True`

`maximum_auto_invoke_attempts: int`

`stage_status = ``'experimental'`

`type_: FunctionChoiceType | ``None`



**PromptExecutionSettings Class**

Reference

Base class for prompt execution settings.

Can be used by itself or as a base class for other prompt execution settings.
The

methods are used to create specific prompt execution settings objects based on
the

keys in the extension_data field, this way you can create a generic

PromptExecutionSettings object in your application, which gets mapped into the
keys of

the prompt execution settings that each services returns by using the

service.get_prompt_execution_settings() method.

Initialize the prompt execution settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

str

The service ID to use for the request.

Default value: None

`**kwargs**`

Required*

Any

Additional keyword arguments, these are attempted to parse into the keys of
the

specific prompt execution settings.

**Keyword-Only Parameters**

`PromptExecutionSettings(service_id: str | ``None`` = ``None``, *, extension_data: `

`dict[str, Any] = ``None``, function_choice_behavior: FunctionChoiceBehavior | `

`None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**extension_data**`

Required*

`**function_choice_behavior**`

Required*

from_prompt_execution_settings

Create a prompt execution settings from another

prompt execution settings.

pack_extension_data

Update the extension data from the prompt execution

settings.

parse_function_choice_behavior

Parse the function choice behavior data.

prepare_settings_dict

Prepares the settings as a dictionary for sending to the

AI service.

unpack_extension_data

Update the prompt execution settings from extension

data.

Does not overwrite existing values with None.

update_from_prompt_execution_settings

Update the keys from another prompt execution

settings object.

**from_prompt_execution_settings**

Create a prompt execution settings from another prompt execution settings.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_prompt_execution_settings()`

ﾉ

**Expand table**



**Name**

**Description**

`**config**`

Required*

**pack_extension_data**

Update the extension data from the prompt execution settings.

Python

**parse_function_choice_behavior**

Parse the function choice behavior data.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**prepare_settings_dict**

Prepares the settings as a dictionary for sending to the AI service.

Python

**unpack_extension_data**

Update the prompt execution settings from extension data.

`pack_extension_data() -> ``None`

`parse_function_choice_behavior(data: Any) -> dict[str, Any]`

ﾉ

**Expand table**

`prepare_settings_dict()`



Does not overwrite existing values with None.

Python

**update_from_prompt_execution_settings**

Update the keys from another prompt execution settings object.

Python

**Parameters**

**Name**

**Description**

`**config**`

Required*

**keys**

Get the keys of the prompt execution settings.

**service_id**

The service ID to use for the request.

Python

**extension_data**

`unpack_extension_data() -> ``None`

`update_from_prompt_execution_settings()`

ﾉ

**Expand table**

**Attributes**

`service_id: Annotated[str | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, metadata=[MinLen(min_length=1)])]`



Any additional data to send with the request.

Python

**function_choice_behavior**

The function choice behavior settings.

Python

`extension_data: dict[str, Any]`

`function_choice_behavior: Annotated[FunctionChoiceBehavior | ``None``, `

`FieldInfo(annotation=NoneType, required=``True``, exclude=``True``)]`



**memory Package**

Reference

**Packages**

astradb

in_memory

ﾉ

**Expand table**



**astradb Package**

Reference

**Modules**

astra_client

astradb_memory_store

astradb_settings

utils

**Classes**

AstraDBMemoryStore

A memory store that uses Astra database as the backend.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraDBMemoryStore class.

AstraDBSettings

AstraDB model settings.

Settings for AstraDB connection:

app_token: SecretStr | None - AstraDB token (Env var

ASTRADB_APP_TOKEN)

db_id: str | None - AstraDB database ID (Env var ASTRADB_DB_ID)

region: str | None - AstraDB region (Env var ASTRADB_REGION)

keyspace: str | None - AstraDB keyspace (Env var

ASTRADB_KEYSPACE)

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**

ﾉ

**Expand table**



**astra_client Module**

Reference

**Classes**

AstraClient

AstraClient.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraClient class.

ﾉ

**Expand table**



**AstraClient Class**

Reference

AstraClient.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraClient class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**astra_id**`

Required*

`**astra_region**`

Required*

`**astra_application_token**`

Required*

`**keyspace_name**`

Required*

`**embedding_dim**`

Required*

`**similarity_function**`

Required*

`**session**`

Default value: None

`AstraClient(astra_id: str, astra_region: str, astra_application_token: str, `

`keyspace_name: str, embedding_dim: int, similarity_function: str, session: `

`ClientSession | ``None`` = ``None``)`

ﾉ

**Expand table**



create_collection

Creates a new collection in the keyspace.

delete_collection

Deletes a collection from the keyspace.

delete_documents

Deletes documents from the collection.

find_collection

Finds a collection in the keyspace.

find_collections

Finds all collections in the keyspace.

find_documents

Finds all documents in the collection.

insert_document

Inserts a document into the collection.

insert_documents

Inserts multiple documents into the collection.

update_document

Updates a document in the collection.

update_documents

Updates multiple documents in the collection.

**create_collection**

Creates a new collection in the keyspace.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**embedding_dim**`

Required*

Default value: None

`**similarity_function**`

Required*

Default value: None

**Methods**

ﾉ

**Expand table**

`async`` create_collection(collection_name: str, embedding_dim: int | ``None`` = `

`None``, similarity_function: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**delete_collection**

Deletes a collection from the keyspace.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

**delete_documents**

Deletes documents from the collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**filter**`

Required*

**find_collection**

Finds a collection in the keyspace.

Python

`async`` delete_collection(collection_name: str)`

ﾉ

**Expand table**

`async`` delete_documents(collection_name: str, filter: dict) -> int`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

**find_collections**

Finds all collections in the keyspace.

Python

**Parameters**

**Name**

**Description**

`**include_detail**`

Default value: True

**find_documents**

Finds all documents in the collection.

Python

**Parameters**

`async`` find_collection(collection_name: str)`

ﾉ

**Expand table**

`async`` find_collections(include_detail: bool = ``True``)`

ﾉ

**Expand table**

`async`` find_documents(collection_name: str, filter: dict | ``None`` = ``None``, `

`vector: list[float] | ``None`` = ``None``, limit: int | ``None`` = ``None``, `

`include_vector: bool | ``None`` = ``None``, include_similarity: bool | ``None`` = `

`None``) -> list[dict]`



**Name**

**Description**

`**collection_name**`

Required*

`**filter**`

Required*

Default value: None

`**vector**`

Required*

Default value: None

`**limit**`

Required*

Default value: None

`**include_vector**`

Required*

Default value: None

`**include_similarity**`

Required*

Default value: None

**insert_document**

Inserts a document into the collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**document**`

Required*

**insert_documents**

Inserts multiple documents into the collection.

ﾉ

**Expand table**

`async`` insert_document(collection_name: str, document: dict) -> str`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**documents**`

Required*

**update_document**

Updates a document in the collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**filter**`

Required*

`**update**`

Required*

`**upsert**`

Required*

Default value: True

`async`` insert_documents(collection_name: str, documents: list[dict]) -> `

`list[str]`

ﾉ

**Expand table**

`async`` update_document(collection_name: str, filter: dict, update: dict, `

`upsert: bool = ``True``) -> dict`

ﾉ

**Expand table**



**update_documents**

Updates multiple documents in the collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**filter**`

Required*

`**update**`

Required*

**is_experimental**

Python

**stage_status**

Python

`async`` update_documents(collection_name: str, filter: dict, update: dict)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**astradb_memory_store Module**

Reference

**Classes**

AstraDBMemoryStore

A memory store that uses Astra database as the backend.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraDBMemoryStore class.

ﾉ

**Expand table**



**AstraDBMemoryStore Class**

Reference

A memory store that uses Astra database as the backend.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraDBMemoryStore class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**astra_application_token**`

Required*

str

The Astra application token.

`**astra_id**`

Required*

str

The Astra id of database.

`**astra_region**`

Required*

str

The Astra region

`**keyspace_name**`

Required*

str

The Astra keyspace

`**embedding_dim**`

Required*

int

The dimensionality to use for new collections.

`**similarity**`

Required*

str

TODO

`**session**`

Optional session parameter

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.memory.astradb.astradb_memory_store.str

| None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

`AstraDBMemoryStore(astra_application_token: str, astra_id: str, astra_region:
str, `

`keyspace_name: str, embedding_dim: int, similarity: str, session: ClientSession | `

`None`` = ``None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

`**env_file_encoding**`

<xref:

<xref:semantic_kernel.connectors.memory.astradb.astradb_memory_store.str

| None>>

The encoding of the environment settings file. (Optional)

Default value: None

create_collection

Creates a new collection in Astra if it does not exist.

delete_collection

Deletes a collection.

does_collection_exist

Checks if a collection exists.

get

Gets a record. Does not guarantee that the collection exists.

get_batch

Gets a batch of records. Does not guarantee that the collection exists.

get_collections

Gets the list of collections.

get_nearest_match

Gets the nearest match to an embedding using cosine similarity.

get_nearest_matches

Gets the nearest matches to an embedding using cosine similarity.

remove

Removes a memory record from the data store. Does not guarantee that the

collection exists.

remove_batch

Removes a batch of records. Does not guarantee that the collection exists.

upsert

Upsert a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

upsert_batch

Upsert a batch of memory records into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

**create_collection**

Creates a new collection in Astra if it does not exist.

Python

**Methods**

ﾉ

**Expand table**

`async`` create_collection(collection_name: str, dimension_num: int | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to create.

`**dimension_num**`

Required*

int

The dimension of the vectors to be stored in this collection.

Default value: None

`**distance_type**`

Required*

str

Specifies the similarity metric to be used when querying or comparing vectors

within

Default value: cosine_similarity

`**dot_product**`

Required*

**dot_product** (<xref:this collection. The available options are>)

`**euclidea**`

Required*

**euclidean**

`**cosine.**`

Required*

**cosine.** (<xref:and>)

**Returns**

**Type**

**Description**

None

**delete_collection**

Deletes a collection.

Python

**Parameters**

`distance_type: str | ``None`` = ``'cosine_similarity'``) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` delete_collection(collection_name: str) -> ``None`



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to delete.

**Returns**

**Type**

**Description**

None

**does_collection_exist**

Checks if a collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to check.

**Returns**

**Type**

**Description**

bool

True if the collection exists; otherwise, False.

**get**

Gets a record. Does not guarantee that the collection exists.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` does_collection_exist(collection_name: str) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the record from.

`**key**`

Required*

str

The unique database key of the record.

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

MemoryRecord

The record.

**get_batch**

Gets a batch of records. Does not guarantee that the collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the records from.

`**keys**`

<xref:List>[str]

`async`` get(collection_name: str, key: str, with_embedding: bool = ``False``)
-> `

`MemoryRecord`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_batch(collection_name: str, keys: list[str], with_embeddings:
bool = `

`False``) -> list[MemoryRecord]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The unique database keys of the records.

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[MemoryRecord]

The records.

**get_collections**

Gets the list of collections.

Python

**Returns**

**Type**

**Description**

List[str]

The list of collections.

**get_nearest_match**

Gets the nearest match to an embedding using cosine similarity.

Python

**Parameters**

ﾉ

**Expand table**

`async`` get_collections() -> list[str]`

ﾉ

**Expand table**

`async`` get_nearest_match(collection_name: str, embedding: ndarray, `

`min_relevance_score: float = 0.0, with_embedding: bool = ``False``) -> `

`tuple[MemoryRecord, float]`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.astradb_memory_store.ndarray>

The embedding to find the nearest matches to.

`**min_relevance_score**`

Required*

float

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

`**with_embedding**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

Tuple[MemoryRecord, float]

The record and the relevance score.

**get_nearest_matches**

Gets the nearest matches to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.astradb_memory_store.ndarray>

The embedding to find the nearest matches to.

`**limit**`

Required*

int

The maximum number of matches to return.

`**min_relevance_score**`

float

ﾉ

**Expand table**

`async`` get_nearest_matches(collection_name: str, embedding: ndarray, limit:
int, `

`min_relevance_score: float = 0.0, with_embeddings: bool = ``False``) -> `

`list[tuple[MemoryRecord, float]]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[Tuple[MemoryRecord, float]]

The records and their relevance scores.

**remove**

Removes a memory record from the data store. Does not guarantee that the
collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the record from.

`**key**`

Required*

str

The unique id associated with the memory record to remove.

**Returns**

**Type**

**Description**

None

**remove_batch**

ﾉ

**Expand table**

`async`` remove(collection_name: str, key: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Removes a batch of records. Does not guarantee that the collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the records from.

`**keys**`

Required*

<xref:List>[str]

The unique ids associated with the memory records to remove.

**Returns**

**Type**

**Description**

None

**upsert**

Upsert a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be updated. If

the record does not exist, it will be created.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`async`` remove_batch(collection_name: str, keys: list[str]) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` upsert(collection_name: str, record: MemoryRecord) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**record**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.astradb_memory_store.MemoryRecord>

The memory record to upsert.

**Returns**

**Type**

**Description**

str

The unique identifier for the memory record.

**upsert_batch**

Upsert a batch of memory records into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be updated. If

the record does not exist, it will be created.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**records**`

Required*

<xref:List>[<xref:MemoryRecord>]

The memory records to upsert.

**Returns**

**Type**

**Description**

List[str]

The unique identifiers for the memory record.

ﾉ

**Expand table**

`async`` upsert_batch(collection_name: str, records: list[MemoryRecord]) -> `

`list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**astradb_settings Module**

Reference

**Classes**

AstraDBSettings

AstraDB model settings.

Settings for AstraDB connection:

app_token: SecretStr | None - AstraDB token (Env var

ASTRADB_APP_TOKEN)

db_id: str | None - AstraDB database ID (Env var ASTRADB_DB_ID)

region: str | None - AstraDB region (Env var ASTRADB_REGION)

keyspace: str | None - AstraDB keyspace (Env var ASTRADB_KEYSPACE)

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**AstraDBSettings Class**

Reference

AstraDB model settings.

Settings for AstraDB connection:

app_token: SecretStr | None - AstraDB token (Env var ASTRADB_APP_TOKEN)

db_id: str | None - AstraDB database ID (Env var ASTRADB_DB_ID)

region: str | None - AstraDB region (Env var ASTRADB_REGION)

keyspace: str | None - AstraDB keyspace (Env var ASTRADB_KEYSPACE)

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`AstraDBSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`app_token: SecretStr, db_id: str, region: str, keyspace: str)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

ﾉ

**Expand table**



**Name**

**Description**

`**app_token**`

Required*

`**db_id**`

Required*

`**region**`

Required*

`**keyspace**`

Required*

**app_token**

Python

**db_id**

Python

**env_file_encoding**

Python

**env_file_path**

Python

**Attributes**

`app_token: SecretStr`

`db_id: str`

`env_file_encoding: str`

`env_file_path: str | ``None`



**env_prefix**

Python

**is_experimental**

Python

**keyspace**

Python

**region**

Python

**stage_status**

Python

`env_prefix: ClassVar[str] = ``'ASTRADB_'`

`is_experimental = ``True`

`keyspace: str`

`region: str`

`stage_status = ``'experimental'`



**utils Module**

Reference

**Classes**

AsyncSession

A wrapper around aiohttp.ClientSession that can be used as an async context

manager.

Initializes a new instance of the AsyncSession class.

**build_payload**

Builds a metadata payload to be sent to AstraDb from a MemoryRecord.

Python

**Parameters**

**Name**

**Description**

`**record**`

Required*

**parse_payload**

Parses a record from AstraDb into a MemoryRecord.

Python

ﾉ

**Expand table**

**Functions**

`build_payload(record: MemoryRecord) -> dict[str, Any]`

ﾉ

**Expand table**

`parse_payload(document: dict[str, Any]) -> MemoryRecord`



**Parameters**

**Name**

**Description**

`**document**`

Required*

ﾉ

**Expand table**



**AsyncSession Class**

Reference

A wrapper around aiohttp.ClientSession that can be used as an async context
manager.

Initializes a new instance of the AsyncSession class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**session**`

Default value: None

`AsyncSession(session: ClientSession = ``None``)`

ﾉ

**Expand table**



**AstraDBMemoryStore Class**

Reference

A memory store that uses Astra database as the backend.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the AstraDBMemoryStore class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**astra_application_token**`

Required*

str

The Astra application token.

`**astra_id**`

Required*

str

The Astra id of database.

`**astra_region**`

Required*

str

The Astra region

`**keyspace_name**`

Required*

str

The Astra keyspace

`**embedding_dim**`

Required*

int

The dimensionality to use for new collections.

`**similarity**`

Required*

str

TODO

`**session**`

Optional session parameter

Default value: None

`AstraDBMemoryStore(astra_application_token: str, astra_id: str, `

`astra_region: str, keyspace_name: str, embedding_dim: int, similarity: str, `

`session: ClientSession | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.memory.astradb.str |

None>>

Use the environment settings file as a fallback to environment

variables. (Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.memory.astradb.str |

None>>

The encoding of the environment settings file. (Optional)

Default value: None

create_collection

Creates a new collection in Astra if it does not exist.

delete_collection

Deletes a collection.

does_collection_exist

Checks if a collection exists.

get

Gets a record. Does not guarantee that the collection exists.

get_batch

Gets a batch of records. Does not guarantee that the collection exists.

get_collections

Gets the list of collections.

get_nearest_match

Gets the nearest match to an embedding using cosine similarity.

get_nearest_matches

Gets the nearest matches to an embedding using cosine similarity.

remove

Removes a memory record from the data store. Does not guarantee that

the collection exists.

remove_batch

Removes a batch of records. Does not guarantee that the collection exists.

upsert

Upsert a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it

will be updated. If the record does not exist, it will be created.

upsert_batch

Upsert a batch of memory records into the data store.

Does not guarantee that the collection exists. If the record already exists,
it

will be updated. If the record does not exist, it will be created.

**create_collection**

**Methods**

ﾉ

**Expand table**



Creates a new collection in Astra if it does not exist.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to create.

`**dimension_num**`

Required*

int

The dimension of the vectors to be stored in this collection.

Default value: None

`**distance_type**`

Required*

str

Specifies the similarity metric to be used when querying or comparing

vectors within

Default value: cosine_similarity

`**dot_product**`

Required*

**dot_product** (<xref:this collection. The available options are>)

`**euclidea**`

Required*

**euclidean**

`**cosine.**`

Required*

**cosine.** (<xref:and>)

**Returns**

**Type**

**Description**

None

**delete_collection**

Deletes a collection.

`async`` create_collection(collection_name: str, dimension_num: int | ``None`` = `

`None``, distance_type: str | ``None`` = ``'cosine_similarity'``) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to delete.

**Returns**

**Type**

**Description**

None

**does_collection_exist**

Checks if a collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to check.

**Returns**

`async`` delete_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` does_collection_exist(collection_name: str) -> bool`

ﾉ

**Expand table**



**Type**

**Description**

bool

True if the collection exists; otherwise, False.

**get**

Gets a record. Does not guarantee that the collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the record from.

`**key**`

Required*

str

The unique database key of the record.

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

MemoryRecord

The record.

**get_batch**

Gets a batch of records. Does not guarantee that the collection exists.

ﾉ

**Expand table**

`async`` get(collection_name: str, key: str, with_embedding: bool = ``False``)
-`

`> MemoryRecord`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the records from.

`**keys**`

Required*

<xref:List>[str]

The unique database keys of the records.

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[MemoryRecord]

The records.

**get_collections**

Gets the list of collections.

Python

**Returns**

`async`` get_batch(collection_name: str, keys: list[str], with_embeddings: `

`bool = ``False``) -> list[MemoryRecord]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_collections() -> list[str]`

ﾉ

**Expand table**



**Type**

**Description**

List[str]

The list of collections.

**get_nearest_match**

Gets the nearest match to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.ndarray>

The embedding to find the nearest matches to.

`**min_relevance_score**`

Required*

float

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

`**with_embedding**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

Tuple[MemoryRecord, float]

The record and the relevance score.

**get_nearest_matches**

`async`` get_nearest_match(collection_name: str, embedding: ndarray, `

`min_relevance_score: float = 0.0, with_embedding: bool = ``False``) -> `

`tuple[MemoryRecord, float]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Gets the nearest matches to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.ndarray>

The embedding to find the nearest matches to.

`**limit**`

Required*

int

The maximum number of matches to return.

`**min_relevance_score**`

Required*

float

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[Tuple[MemoryRecord, float]]

The records and their relevance scores.

**remove**

Removes a memory record from the data store. Does not guarantee that the

collection exists.

Python

`async`` get_nearest_matches(collection_name: str, embedding: ndarray, `

`limit: int, min_relevance_score: float = 0.0, with_embeddings: bool = `

`False``) -> list[tuple[MemoryRecord, float]]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the record from.

`**key**`

Required*

str

The unique id associated with the memory record to remove.

**Returns**

**Type**

**Description**

None

**remove_batch**

Removes a batch of records. Does not guarantee that the collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the records from.

`**keys**`

Required*

<xref:List>[str]

The unique ids associated with the memory records to remove.

`async`` remove(collection_name: str, key: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` remove_batch(collection_name: str, keys: list[str]) -> ``None`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

None

**upsert**

Upsert a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**record**`

Required*

<xref:semantic_kernel.connectors.memory.astradb.MemoryRecord>

The memory record to upsert.

**Returns**

**Type**

**Description**

str

The unique identifier for the memory record.

**upsert_batch**

Upsert a batch of memory records into the data store.

ﾉ

**Expand table**

`async`` upsert(collection_name: str, record: MemoryRecord) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**records**`

Required*

<xref:List>[<xref:MemoryRecord>]

The memory records to upsert.

**Returns**

**Type**

**Description**

List[str]

The unique identifiers for the memory record.

**is_experimental**

Python

**stage_status**

Python

`async`` upsert_batch(collection_name: str, records: list[MemoryRecord]) -> `

`list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



`stage_status = ``'experimental'`



**AstraDBSettings Class**

Reference

AstraDB model settings.

Settings for AstraDB connection:

app_token: SecretStr | None - AstraDB token (Env var ASTRADB_APP_TOKEN)

db_id: str | None - AstraDB database ID (Env var ASTRADB_DB_ID)

region: str | None - AstraDB region (Env var ASTRADB_REGION)

keyspace: str | None - AstraDB keyspace (Env var ASTRADB_KEYSPACE)

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`AstraDBSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`app_token: SecretStr, db_id: str, region: str, keyspace: str)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

ﾉ

**Expand table**



**Name**

**Description**

`**app_token**`

Required*

`**db_id**`

Required*

`**region**`

Required*

`**keyspace**`

Required*

**app_token**

Python

**db_id**

Python

**env_prefix**

Python

**is_experimental**

Python

**Attributes**

`app_token: SecretStr`

`db_id: str`

`env_prefix: ClassVar[str] = ``'ASTRADB_'`

`is_experimental = ``True`



**keyspace**

Python

**region**

Python

**stage_status**

Python

`keyspace: str`

`region: str`

`stage_status = ``'experimental'`



**in_memory Package**

Reference

**Modules**

const

in_memory_collection

in_memory_store

**Classes**

InMemoryVectorCollection

In Memory Collection.

Create a In Memory Collection.

InMemoryVectorStore

Create a In Memory Vector Store.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**



**const Module**

Reference



**in_memory_collection Module**

Reference

**Classes**

InMemoryVectorCollection

In Memory Collection.

Create a In Memory Collection.

ﾉ

**Expand table**



**InMemoryVectorCollection Class**

Reference

In Memory Collection.

Create a In Memory Collection.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Default value: None

create_collection

delete_collection

does_collection_exist

**create_collection**

`InMemoryVectorCollection(collection_name: str, data_model_type: `

`type[TModel], data_model_definition: VectorStoreRecordDefinition | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**delete_collection**

Python

**does_collection_exist**

Python

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**inner_storage**

`async`` create_collection(**kwargs: Any) -> ``None`

`async`` delete_collection(**kwargs: Any) -> ``None`

`async`` does_collection_exist(**kwargs: Any) -> bool`

**Attributes**

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`



Python

**managed_client**

Python

**supported_key_types**

Python

`inner_storage: dict[str | int | float, dict]`

`managed_client: bool`

`supported_key_types: ClassVar[list[str] | ``None``] = [``'str'``, ``'int'``, ``'float'``]`



**in_memory_store Module**

Reference

**Classes**

InMemoryVectorStore

Create a In Memory Vector Store.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**InMemoryVectorStore Class**

Reference

Create a In Memory Vector Store.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**vector_record_collections**`

Required*

`**managed_client**`

Default value: True

get_collection

list_collection_names

**get_collection**

`InMemoryVectorStore(*, vector_record_collections: dict[str, `

`VectorStoreRecordCollection] = ``None``, managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

Default value: None

**list_collection_names**

Python

**is_experimental**

Python

**managed_client**

Python

`get_collection(collection_name: str, data_model_type: type[TModel], `

`data_model_definition: VectorStoreRecordDefinition | ``None`` = ``None``, `

`**kwargs: Any) -> VectorStoreRecordCollection`

ﾉ

**Expand table**

`async`` list_collection_names(**kwargs) -> Sequence[str]`

**Attributes**

`is_experimental = ``True`

`managed_client: bool`



**stage_status**

Python

**vector_record_collections**

Python

`stage_status = ``'experimental'`

`vector_record_collections: dict[str, VectorStoreRecordCollection]`



**InMemoryVectorCollection Class**

Reference

In Memory Collection.

Create a In Memory Collection.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Default value: None

create_collection

delete_collection

does_collection_exist

**create_collection**

`InMemoryVectorCollection(collection_name: str, data_model_type: `

`type[TModel], data_model_definition: VectorStoreRecordDefinition | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**delete_collection**

Python

**does_collection_exist**

Python

**inner_storage**

Python

**supported_key_types**

Python

`async`` create_collection(**kwargs: Any) -> ``None`

`async`` delete_collection(**kwargs: Any) -> ``None`

`async`` does_collection_exist(**kwargs: Any) -> bool`

**Attributes**

`inner_storage: dict[str | int | float, dict]`

`supported_key_types: ClassVar[list[str] | ``None``] = [``'str'``, ``'int'``, ``'float'``]`



**InMemoryVectorStore Class**

Reference

Create a In Memory Vector Store.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**vector_record_collections**`

Required*

`**managed_client**`

Default value: True

get_collection

list_collection_names

**get_collection**

`InMemoryVectorStore(*, vector_record_collections: dict[str, `

`VectorStoreRecordCollection] = ``None``, managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

Default value: None

**list_collection_names**

Python

**is_experimental**

Python

**managed_client**

Python

`get_collection(collection_name: str, data_model_type: type[TModel], `

`data_model_definition: VectorStoreRecordDefinition | ``None`` = ``None``, `

`**kwargs: Any) -> VectorStoreRecordCollection`

ﾉ

**Expand table**

`async`` list_collection_names(**kwargs) -> Sequence[str]`

**Attributes**

`is_experimental = ``True`

`managed_client: bool`



**stage_status**

Python

**vector_record_collections**

Python

`stage_status = ``'experimental'`

`vector_record_collections: dict[str, VectorStoreRecordCollection]`



**azure_ai_search_settings Module**

Reference

**Classes**

AzureAISearchSettings

Azure AI Search model settings currently used by the

AzureCognitiveSearchMemoryStore connector.

Args:

api_key: SecretStr - Azure AI Search API key (Env var

AZURE_AI_SEARCH_API_KEY)

endpoint: HttpsUrl - Azure AI Search endpoint (Env var

AZURE_AI_SEARCH_ENDPOINT)

index_name: str - Azure AI Search index name (Env var

AZURE_AI_SEARCH_INDEX_NAME)

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**AzureAISearchSettings Class**

Reference

Azure AI Search model settings currently used by the

AzureCognitiveSearchMemoryStore connector.

Args:

api_key: SecretStr - Azure AI Search API key (Env var AZURE_AI_SEARCH_API_KEY)

endpoint: HttpsUrl - Azure AI Search endpoint (Env var

AZURE_AI_SEARCH_ENDPOINT)

index_name: str - Azure AI Search index name (Env var

AZURE_AI_SEARCH_INDEX_NAME)

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

`AzureAISearchSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr | ``None`` = ``None``, endpoint: Annotated[Url, `

`UrlConstraints(max_length=2083, allowed_schemes=[``'https'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)], index_name: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**endpoint**`

Required*

`**index_name**`

Required*

model_dump

Dump the model to a dictionary.

**model_dump**

Dump the model to a dictionary.

Python

**api_key**

Python

**endpoint**

Python

**Methods**

ﾉ

**Expand table**

`model_dump() -> dict[str, str]`

**Attributes**

`api_key: SecretStr | ``None`



**env_prefix**

Python

**index_name**

Python

**is_experimental**

Python

**stage_status**

Python

`endpoint: Annotated[Url, UrlConstraints(max_length=2083, allowed_schemes=`

`[``'https'``], host_required=``None``, default_host=``None``,
default_port=``None``, `

`default_path=``None``)]`

`env_prefix: ClassVar[str] = ``'AZURE_AI_SEARCH_'`

`index_name: str | ``None`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**openapi_plugin Package**

Reference

**Packages**

models

**Modules**

const

openapi_function_execution_parameters

openapi_manager

openapi_parser

openapi_runner

operation_selection_predicate_context

**Classes**

OpenAPIFunctionExecutionParameters

OpenAPI function execution parameters.

Note: This class is marked as 'experimental' and may

change in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



OpenApiParser

NOTE: SK Python only supports the OpenAPI Spec >=3.0.

Import an OpenAPI file.

OperationSelectionPredicateContext

The context for the operation selection predicate.

Initialize the operation selection predicate context.



**models Package**

Reference

**Modules**

rest_api_expected_response

rest_api_oauth_flow

rest_api_oauth_flows

rest_api_operation

rest_api_parameter

rest_api_parameter_location

rest_api_parameter_style

rest_api_payload

rest_api_payload_property

rest_api_run_options

rest_api_security_requirement

rest_api_security_scheme

rest_api_uri

ﾉ

**Expand table**



**rest_api_expected_response Module**

Reference

**Classes**

RestApiExpectedResponse

RestApiExpectedResponse.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize the RestApiExpectedResponse.

ﾉ

**Expand table**



**RestApiExpectedResponse Class**

Reference

RestApiExpectedResponse.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiExpectedResponse.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**description**`

Required*

`**media_type**`

Required*

`**schema**`

Default value: None

**is_experimental**

Python

**stage_status**

`RestApiExpectedResponse(description: str, media_type: str, schema: dict[str,
`

`str] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



Python

`stage_status = ``'experimental'`



**rest_api_oauth_flow Module**

Reference

**Classes**

RestApiOAuthFlow

Represents the OAuth flow used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**RestApiOAuthFlow Class**

Reference

Represents the OAuth flow used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**authorization_url**`

Required*

`**token_url**`

Required*

`**scopes**`

Required*

`**refresh_url**`

Default value: None

**authorization_url**

Python

**is_experimental**

`RestApiOAuthFlow(authorization_url: str, token_url: str, scopes: dict[str, `

`str], refresh_url: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`authorization_url: str`



Python

**refresh_url**

Python

**scopes**

Python

**stage_status**

Python

**token_url**

Python

`is_experimental = ``True`

`refresh_url: str | ``None`` = ``None`

`scopes: dict[str, str]`

`stage_status = ``'experimental'`

`token_url: str`



**rest_api_oauth_flows Module**

Reference

**Classes**

RestApiOAuthFlows

Represents the OAuth flows used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**RestApiOAuthFlows Class**

Reference

Represents the OAuth flows used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**implicit**`

Default value: None

`**password**`

Default value: None

`**client_credentials**`

Default value: None

`**authorization_code**`

Default value: None

**authorization_code**

Python

**client_credentials**

Python

`RestApiOAuthFlows(implicit: RestApiOAuthFlow | ``None`` = ``None``, password: `

`RestApiOAuthFlow | ``None`` = ``None``, client_credentials: RestApiOAuthFlow | ``None`` `

`= ``None``, authorization_code: RestApiOAuthFlow | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`authorization_code: RestApiOAuthFlow | ``None`` = ``None`



**implicit**

Python

**is_experimental**

Python

**password**

Python

**stage_status**

Python

`client_credentials: RestApiOAuthFlow | ``None`` = ``None`

`implicit: RestApiOAuthFlow | ``None`` = ``None`

`is_experimental = ``True`

`password: RestApiOAuthFlow | ``None`` = ``None`

`stage_status = ``'experimental'`



**rest_api_operation Module**

Reference

**Classes**

RestApiOperation

RestApiOperation.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiOperation.

ﾉ

**Expand table**



**RestApiOperation Class**

Reference

RestApiOperation.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiOperation.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**id**`

Required*

`**method**`

Required*

`**servers**`

Required*

`**path**`

Required*

`**summary**`

Default value: None

`**description**`

Default value: None

`**params**`

Default value: None

`**request_body**`

Default value: None

`RestApiOperation(id: str, method: str, servers: list[dict[str, Any]], path: `

`str, summary: str | ``None`` = ``None``, description: str | ``None`` = ``None``, params: `

`list[RestApiParameter] | ``None`` = ``None``, request_body: RestApiPayload | ``None`` = `

`None``, responses: dict[str, RestApiExpectedResponse] | ``None`` = ``None``, `

`security_requirements: list[RestApiSecurityRequirement] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**responses**`

Default value: None

`**security_requirements**`

Default value: None

build_headers

Build the headers for the operation.

build_operation_url

Build the URL for the operation.

build_path

Build the path for the operation.

build_query_string

Build the query string for the operation.

create_content_type_artificial_parameter

Create an artificial parameter for the content type of

the REST API request body.

create_payload_artificial_parameter

Create an artificial parameter for the REST API request

body.

freeze

Make the instance and its components immutable.

get_default_response

Get the default response for the operation.

If no appropriate response is found, returns None.

get_default_return_parameter

Get the default return parameter for the operation.

get_parameters

Get the parameters for the operation.

get_payload_parameters

Get the payload parameters for the operation.

get_server_url

Get the server URL for the operation.

replace_invalid_symbols

Replace invalid symbols in the parameter name with

underscores.

url_join

Join a base URL and a path, correcting for any missing

slashes.

**build_headers**

Build the headers for the operation.

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

**build_operation_url**

Build the URL for the operation.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**server_url_override**`

Required*

Default value: None

`**api_host_url**`

Required*

Default value: None

**build_path**

Build the path for the operation.

Python

`build_headers(arguments: dict[str, Any]) -> dict[str, str]`

ﾉ

**Expand table**

`build_operation_url(arguments, server_url_override=``None``, `

`api_host_url=``None``)`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**path_template**`

Required*

`**arguments**`

Required*

**build_query_string**

Build the query string for the operation.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

**create_content_type_artificial_parameter**

Create an artificial parameter for the content type of the REST API request
body.

Python

**create_payload_artificial_parameter**

`build_path(path_template: str, arguments: dict[str, Any]) -> str`

ﾉ

**Expand table**

`build_query_string(arguments: dict[str, Any]) -> str`

ﾉ

**Expand table**

`create_content_type_artificial_parameter() -> RestApiParameter`



Create an artificial parameter for the REST API request body.

Python

**Parameters**

**Name**

**Description**

`**operation**`

Required*

**freeze**

Make the instance and its components immutable.

Python

**get_default_response**

Get the default response for the operation.

If no appropriate response is found, returns None.

Python

**Parameters**

**Name**

**Description**

`**responses**`

`create_payload_artificial_parameter(operation: RestApiOperation) -> `

`RestApiParameter`

ﾉ

**Expand table**

`freeze()`

`get_default_response(responses: dict[str, RestApiExpectedResponse], `

`preferred_responses: list[str]) -> RestApiExpectedResponse | ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**preferred_responses**`

Required*

**get_default_return_parameter**

Get the default return parameter for the operation.

Python

**Parameters**

**Name**

**Description**

`**preferred_responses**`

Default value: None

**get_parameters**

Get the parameters for the operation.

Python

**Parameters**

**Name**

**Description**

`**operation**`

Required*

`get_default_return_parameter(preferred_responses: list[str] | ``None`` = `

`None``) -> KernelParameterMetadata`

ﾉ

**Expand table**

`get_parameters(operation: RestApiOperation, `

`add_payload_params_from_metadata: bool = ``True``, enable_payload_spacing: `

`bool = ``False``) -> list[RestApiParameter]`

ﾉ

**Expand table**



**Name**

**Description**

`**add_payload_params_from_metadata**`

Required*

Default value: True

`**enable_payload_spacing**`

Required*

Default value: False

**get_payload_parameters**

Get the payload parameters for the operation.

Python

**Parameters**

**Name**

**Description**

`**operation**`

Required*

`**use_parameters_from_metadata**`

Required*

`**enable_namespacing**`

Required*

**get_server_url**

Get the server URL for the operation.

Python

**Parameters**

`get_payload_parameters(operation: RestApiOperation, `

`use_parameters_from_metadata: bool, enable_namespacing: bool)`

ﾉ

**Expand table**

`get_server_url(server_url_override=``None``, api_host_url=``None``, `

`arguments=``None``)`



**Name**

**Description**

`**server_url_override**`

Default value: None

`**api_host_url**`

Default value: None

`**arguments**`

Default value: None

**replace_invalid_symbols**

Replace invalid symbols in the parameter name with underscores.

Python

**Parameters**

**Name**

**Description**

`**parameter_name**`

Required*

**url_join**

Join a base URL and a path, correcting for any missing slashes.

Python

**Parameters**

**Name**

**Description**

`**base_url**`

ﾉ

**Expand table**

`replace_invalid_symbols(parameter_name)`

ﾉ

**Expand table**

`url_join(base_url: str, path: str)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**path**`

Required*

**description**

Get the description of the operation.

**id**

Get the ID of the operation.

**method**

Get the method of the operation.

**parameters**

Get the parameters of the operation.

**path**

Get the path of the operation.

**request_body**

Get the request body of the operation.

**responses**

Get the responses of the operation.

**security_requirements**

Get the security requirements of the operation.

**Attributes**



**servers**

Get the servers of the operation.

**summary**

Get the summary of the operation.

**CONTENT_TYPE_ARGUMENT_NAME**

Python

**INVALID_SYMBOLS_REGEX**

Python

**MEDIA_TYPE_TEXT_PLAIN**

Python

**PAYLOAD_ARGUMENT_NAME**

Python

**is_experimental**

Python

`CONTENT_TYPE_ARGUMENT_NAME = ``'content-type'`

`INVALID_SYMBOLS_REGEX = re.compile(``'[^0-9A-Za-z_]+'``)`

`MEDIA_TYPE_TEXT_PLAIN = ``'text/plain'`

`PAYLOAD_ARGUMENT_NAME = ``'payload'`

`is_experimental = ``True`



**stage_status**

Python

`stage_status = ``'experimental'`



**rest_api_parameter Module**

Reference

**Classes**

RestApiParameter

RestApiParameter.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiParameter.

ﾉ

**Expand table**



**RestApiParameter Class**

Reference

RestApiParameter.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiParameter.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

`**type**`

Required*

`**location**`

Required*

`**style**`

Default value: None

`**alternative_name**`

Default value: None

`**description**`

Default value: None

`**is_required**`

Default value: False

`**default_value**`

Default value: None

`**schema**`

Default value: None

`RestApiParameter(name: str, type: str, location: RestApiParameterLocation, `

`style: RestApiParameterStyle | ``None`` = ``None``, alternative_name: str | ``None`` = `

`None``, description: str | ``None`` = ``None``, is_required: bool = ``False``, `

`default_value: Any | ``None`` = ``None``, schema: str | dict | ``None`` = ``None``, `

`response: RestApiExpectedResponse | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**response**`

Default value: None

freeze

Make the instance immutable.

**freeze**

Make the instance immutable.

Python

**alternative_name**

Get the alternative name of the parameter.

**default_value**

Get the default value of the parameter.

**description**

Get the description of the parameter.

**is_required**

Get whether the parameter is required.

**location**

Get the location of the parameter.

**Methods**

ﾉ

**Expand table**

`freeze()`

**Attributes**



**name**

Get the name of the parameter.

**response**

Get the response of the parameter.

**schema**

Get the schema of the parameter.

**style**

Get the style of the parameter.

**type**

Get the type of the parameter.

**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_parameter_location Module**

Reference

**Enums**

RestApiParameterLocation

The location of the REST API parameter.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**RestApiParameterLocation Enum**

Reference

The location of the REST API parameter.

Note: This class is marked as 'experimental' and may change in the future.

BODY

COOKIE

HEADER

PATH

QUERY

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**rest_api_parameter_style Module**

Reference

**Enums**

RestApiParameterStyle

RestApiParameterStyle.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**RestApiParameterStyle Enum**

Reference

RestApiParameterStyle.

Note: This class is marked as 'experimental' and may change in the future.

SIMPLE

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**rest_api_payload Module**

Reference

**Classes**

RestApiPayload

RestApiPayload.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiPayload.

ﾉ

**Expand table**



**RestApiPayload Class**

Reference

RestApiPayload.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiPayload.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**media_type**`

Required*

`**properties**`

Required*

`**description**`

Default value: None

`**schema**`

Default value: None

freeze

Make the instance immutable and freeze properties.

**freeze**

Make the instance immutable and freeze properties.

`RestApiPayload(media_type: str, properties: list[RestApiPayloadProperty], `

`description: str | ``None`` = ``None``, schema: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**description**

Get the description of the payload.

**media_type**

Get the media type of the payload.

**properties**

Get the properties of the payload.

**schema**

Get the schema of the payload.

**is_experimental**

Python

**stage_status**

Python

`freeze()`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_payload_property Module**

Reference

**Classes**

RestApiPayloadProperty

RestApiPayloadProperty.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize the RestApiPayloadProperty.

ﾉ

**Expand table**



**RestApiPayloadProperty Class**

Reference

RestApiPayloadProperty.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the RestApiPayloadProperty.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

`**type**`

Required*

`**properties**`

Default value: None

`**description**`

Default value: None

`**is_required**`

Default value: False

`**default_value**`

Default value: None

`**schema**`

Default value: None

`RestApiPayloadProperty(name: str, type: str, properties: `

`list[RestApiPayloadProperty] | ``None`` = ``None``, description: str | ``None`` = ``None``, `

`is_required: bool = ``False``, default_value: Any | ``None`` = ``None``, schema: str | `

`None`` = ``None``)`

ﾉ

**Expand table**

**Methods**



freeze

Make the instance immutable, and freeze nested properties.

**freeze**

Make the instance immutable, and freeze nested properties.

Python

**default_value**

Get the default value of the property.

**description**

Get the description of the property.

**is_required**

Get whether the property is required.

**name**

Get the name of the property.

**properties**

Get the properties of the property.

**schema**

Get the schema of the property.

**type**

ﾉ

**Expand table**

`freeze()`

**Attributes**



Get the type of the property.

**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_run_options Module**

Reference

**Classes**

RestApiRunOptions

The options for running the REST API operation.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the REST API operation run options.

ﾉ

**Expand table**



**RestApiRunOptions Class**

Reference

The options for running the REST API operation.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the REST API operation run options.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**server_url_override**`

Default value: None

`**api_host_url**`

Default value: None

**is_experimental**

Python

**stage_status**

Python

`RestApiRunOptions(server_url_override=``None``, api_host_url=``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_security_requirement Module**

Reference

**Classes**

RestApiSecurityRequirement

Represents the security requirements used by the REST API.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the RestApiSecurityRequirement class.

ﾉ

**Expand table**



**RestApiSecurityRequirement Class**

Reference

Represents the security requirements used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the RestApiSecurityRequirement class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**dictionary**`

Required*

**is_experimental**

Python

**stage_status**

Python

`RestApiSecurityRequirement(dictionary: dict[RestApiSecurityScheme, `

`list[str]])`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_security_scheme Module**

Reference

**Classes**

RestApiSecurityScheme

Represents the security scheme used by the REST API.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the RestApiSecurityScheme class.

ﾉ

**Expand table**



**RestApiSecurityScheme Class**

Reference

Represents the security scheme used by the REST API.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the RestApiSecurityScheme class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**security_scheme_type**`

Required*

`**name**`

Required*

`**in_**`

Required*

`**scheme**`

Required*

`**open_id_connect_url**`

Required*

`**description**`

Default value: None

`**bearer_format**`

Default value: None

`**flows**`

Default value: None

`RestApiSecurityScheme(security_scheme_type: str, name: str, in_: `

`RestApiParameterLocation, scheme: str, open_id_connect_url: str, `

`description: str | ``None`` = ``None``, bearer_format: str | ``None`` = ``None``, flows: `

`RestApiOAuthFlows | ``None`` = ``None``)`

ﾉ

**Expand table**



**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**rest_api_uri Module**

Reference

**Classes**

Uri

The Uri class that represents the URI.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the Uri.

ﾉ

**Expand table**



**Uri Class**

Reference

The Uri class that represents the URI.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the Uri.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**uri**`

Required*

get_left_part

Get the left part of the URI.

**get_left_part**

Get the left part of the URI.

Python

`Uri(uri)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`get_left_part()`



**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**const Module**

Reference

**Enums**

OperationExtensions

The operation extensions.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**OperationExtensions Enum**

Reference

The operation extensions.

Note: This class is marked as 'experimental' and may change in the future.

INFO_KEY

METADATA_KEY

METHOD_KEY

OPERATION_KEY

SECURITY_KEY

SERVER_URLS_KEY

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**openapi_function_execution_parameters**

**Module**

Reference

**Classes**

OpenAPIFunctionExecutionParameters

OpenAPI function execution parameters.

Note: This class is marked as 'experimental' and may

change in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

ﾉ

**Expand table**



**OpenAPIFunctionExecutionParameters**

**Class**

Reference

OpenAPI function execution parameters.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**http_client**`

Required*

`**auth_callback**`

Required*

`**server_url_override**`

Required*

`**ignore_non_compliant_errors**`

`OpenAPIFunctionExecutionParameters(*, http_client: AsyncClient | ``None`` = `

`None``, auth_callback: Callable[[...], Awaitable[Any]] | ``None`` = ``None``, `

`server_url_override: str | ``None`` = ``None``, ignore_non_compliant_errors: bool = `

`False``, user_agent: str | ``None`` = ``None``, enable_dynamic_payload: bool = ``True``, `

`enable_payload_namespacing: bool = ``False``, operations_to_exclude:
list[str] = `

`None``, operation_selection_predicate: `

`Callable[[OperationSelectionPredicateContext], bool] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**user_agent**`

Required*

`**enable_dynamic_payload**`

Default value: True

`**enable_payload_namespacing**`

Required*

`**operations_to_exclude**`

Required*

`**operation_selection_predicate**`

Required*

model_post_init

Post initialization method for the model.

**model_post_init**

Post initialization method for the model.

Python

**Parameters**

**Name**

**Description**

`**_OpenAPIFunctionExecutionParameters__context**`

Required*

**Methods**

ﾉ

**Expand table**

`model_post_init(_OpenAPIFunctionExecutionParameters__context: Any) -> `

`None`

ﾉ

**Expand table**

**Attributes**



**auth_callback**

Python

**enable_dynamic_payload**

Python

**enable_payload_namespacing**

Python

**http_client**

Python

**ignore_non_compliant_errors**

Python

**is_experimental**

Python

**operation_selection_predicate**

`auth_callback: Callable[[...], Awaitable[Any]] | ``None`

`enable_dynamic_payload: bool`

`enable_payload_namespacing: bool`

`http_client: AsyncClient | ``None`

`ignore_non_compliant_errors: bool`

`is_experimental = ``True`



Python

**operations_to_exclude**

Python

**server_url_override**

Python

**stage_status**

Python

**user_agent**

Python

`operation_selection_predicate: `

`Callable[[OperationSelectionPredicateContext], bool] | ``None`

`operations_to_exclude: list[str]`

`server_url_override: str | ``None`

`stage_status = ``'experimental'`

`user_agent: str | ``None`



**openapi_manager Module**

Reference

**create_functions_from_openapi**

Creates the functions from OpenAPI document.

Args: plugin_name: The name of the plugin openapi_document_path: The OpenAPI

document path, it must be a file path to the spec (optional)
openapi_parsed_spec:

The parsed OpenAPI spec (optional) execution_settings: The execution settings

Returns: list[KernelFunctionFromMethod]: the operations as functions

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

`**openapi_document_path**`

Required*

Default value: None

`**openapi_parsed_spec**`

Required*

Default value: None

`**execution_settings**`

Required*

Default value: None

**Functions**

`create_functions_from_openapi(plugin_name: str, openapi_document_path: `

`str | ``None`` = ``None``, openapi_parsed_spec: dict[str, Any] | ``None`` = ``None``, `

`execution_settings: OpenAPIFunctionExecutionParameters | ``None`` = ``None``) -> `

`list[KernelFunctionFromMethod]`

ﾉ

**Expand table**



**openapi_parser Module**

Reference

**Classes**

OpenApiParser

NOTE: SK Python only supports the OpenAPI Spec >=3.0.

Import an OpenAPI file.

ﾉ

**Expand table**



**OpenApiParser Class**

Reference

NOTE: SK Python only supports the OpenAPI Spec >=3.0.

Import an OpenAPI file.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**openapi_file**`

Required*

The path to the OpenAPI file which can be local or a URL.

`**openapi_file**`

Required*

The path to the OpenAPI file which can be local or a URL.

create_rest_api_operations

Create REST API operations from the parsed OpenAPI document.

parse

Parse the OpenAPI document.

**create_rest_api_operations**

Create REST API operations from the parsed OpenAPI document.

Python

`OpenApiParser()`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`create_rest_api_operations(parsed_document: Any, execution_settings: `

`OpenAPIFunctionExecutionParameters | ``None`` = ``None``) -> dict[str, `



**Parameters**

**Name**

**Description**

`**parsed_document**`

Required*

The parsed OpenAPI document.

`**execution_settings**`

Required*

The execution settings.

Default value: None

**Returns**

**Type**

**Description**

A dictionary of RestApiOperation instances.

**parse**

Parse the OpenAPI document.

Python

**Parameters**

**Name**

**Description**

`**openapi_document**`

Required*

`RestApiOperation]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`parse(openapi_document: str) -> Any | dict[str, Any] | ``None`

ﾉ

**Expand table**

**Attributes**



**PAYLOAD_PROPERTIES_HIERARCHY_MAX_DEPTH**

Python

**SUPPORTED_MEDIA_TYPES**

Python

`PAYLOAD_PROPERTIES_HIERARCHY_MAX_DEPTH: int = 10`

`SUPPORTED_MEDIA_TYPES: Final[list[str]] = [``'application/json'``, `

`'text/plain'``]`



**openapi_runner Module**

Reference

**Classes**

OpenApiRunner

The OpenApiRunner that runs the operations defined in the OpenAPI manifest.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the OpenApiRunner.

ﾉ

**Expand table**



**OpenApiRunner Class**

Reference

The OpenApiRunner that runs the operations defined in the OpenAPI manifest.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the OpenApiRunner.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**parsed_openapi_document**`

Required*

`**auth_callback**`

Default value: None

`**http_client**`

Default value: None

`**enable_dynamic_payload**`

Default value: True

`**enable_payload_namespacing**`

Default value: False

build_full_url

Build the full URL.

build_json_object

Build the JSON payload object.

`OpenApiRunner(parsed_openapi_document: Mapping[str, str], auth_callback: `

`Callable[[...], dict[str, str] | Awaitable[dict[str, str]]] | ``None`` = ``None``, `

`http_client: AsyncClient | ``None`` = ``None``, enable_dynamic_payload: bool = ``True``, `

`enable_payload_namespacing: bool = ``False``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



build_json_payload

Build the JSON payload.

build_operation_payload

Build the operation payload.

build_operation_url

Build the operation URL.

get_argument_name_for_payload

Get argument name for the payload.

run_operation

Runs the operation defined in the OpenAPI manifest.

**build_full_url**

Build the full URL.

Python

**Parameters**

**Name**

**Description**

`**base_url**`

Required*

`**query_string**`

Required*

**build_json_object**

Build the JSON payload object.

Python

**Parameters**

`build_full_url(base_url, query_string)`

ﾉ

**Expand table**

`build_json_object(properties, arguments, property_namespace=``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**properties**`

Required*

`**arguments**`

Required*

`**property_namespace**`

Required*

Default value: None

**build_json_payload**

Build the JSON payload.

Python

**Parameters**

**Name**

**Description**

`**payload_metadata**`

Required*

`**arguments**`

Required*

**build_operation_payload**

Build the operation payload.

Python

**Parameters**

`build_json_payload(payload_metadata: RestApiPayload, arguments: dict[str, `

`Any]) -> tuple[str, str]`

ﾉ

**Expand table**

`build_operation_payload(operation: RestApiOperation, arguments: `

`KernelArguments) -> tuple[str, str] | tuple[``None``, ``None``]`



**Name**

**Description**

`**operation**`

Required*

`**arguments**`

Required*

**build_operation_url**

Build the operation URL.

Python

**Parameters**

**Name**

**Description**

`**operation**`

Required*

`**arguments**`

Required*

`**server_url_override**`

Required*

Default value: None

`**api_host_url**`

Required*

Default value: None

**get_argument_name_for_payload**

Get argument name for the payload.

Python

ﾉ

**Expand table**

`build_operation_url(operation: RestApiOperation, arguments: `

`KernelArguments, server_url_override=``None``, api_host_url=``None``)`

ﾉ

**Expand table**

`get_argument_name_for_payload(property_name, property_namespace=``None``)`



**Parameters**

**Name**

**Description**

`**property_name**`

Required*

`**property_namespace**`

Required*

Default value: None

**run_operation**

Runs the operation defined in the OpenAPI manifest.

Python

**Parameters**

**Name**

**Description**

`**operation**`

Required*

`**arguments**`

Required*

Default value: None

`**options**`

Required*

Default value: None

**is_experimental**

Python

ﾉ

**Expand table**

`async`` run_operation(operation: RestApiOperation, arguments: `

`KernelArguments | ``None`` = ``None``, options: RestApiRunOptions | ``None`` = ``None``) `

`-> str`

ﾉ

**Expand table**

**Attributes**



**media_type_application_json**

Python

**payload_argument_name**

Python

**stage_status**

Python

`is_experimental = ``True`

`media_type_application_json = ``'application/json'`

`payload_argument_name = ``'payload'`

`stage_status = ``'experimental'`



**operation_selection_predicate_context**

**Module**

Reference

**Classes**

OperationSelectionPredicateContext

The context for the operation selection predicate.

Initialize the operation selection predicate context.

ﾉ

**Expand table**



**OperationSelectionPredicateContext**

**Class**

Reference

The context for the operation selection predicate.

Initialize the operation selection predicate context.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**operation_id**`

Required*

`**path**`

Required*

`**method**`

Required*

`**description**`

Default value: None

`OperationSelectionPredicateContext(operation_id: str, path: str, method: `

`str, description: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**OpenAPIFunctionExecutionParameters**

**Class**

Reference

OpenAPI function execution parameters.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**http_client**`

Required*

`**auth_callback**`

Required*

`**server_url_override**`

Required*

`**ignore_non_compliant_errors**`

`OpenAPIFunctionExecutionParameters(*, http_client: AsyncClient | ``None`` = `

`None``, auth_callback: Callable[[...], Awaitable[Any]] | ``None`` = ``None``, `

`server_url_override: str | ``None`` = ``None``, ignore_non_compliant_errors: bool = `

`False``, user_agent: str | ``None`` = ``None``, enable_dynamic_payload: bool = ``True``, `

`enable_payload_namespacing: bool = ``False``, operations_to_exclude:
list[str] = `

`None``, operation_selection_predicate: `

`Callable[[OperationSelectionPredicateContext], bool] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**user_agent**`

Required*

`**enable_dynamic_payload**`

Default value: True

`**enable_payload_namespacing**`

Required*

`**operations_to_exclude**`

Required*

`**operation_selection_predicate**`

Required*

model_post_init

Post initialization method for the model.

**model_post_init**

Post initialization method for the model.

Python

**Parameters**

**Name**

**Description**

`**_OpenAPIFunctionExecutionParameters__context**`

Required*

**Methods**

ﾉ

**Expand table**

`model_post_init(_OpenAPIFunctionExecutionParameters__context: Any) -> `

`None`

ﾉ

**Expand table**

**Attributes**



**auth_callback**

Python

**enable_dynamic_payload**

Python

**enable_payload_namespacing**

Python

**http_client**

Python

**ignore_non_compliant_errors**

Python

**is_experimental**

Python

**operation_selection_predicate**

`auth_callback: Callable[[...], Awaitable[Any]] | ``None`

`enable_dynamic_payload: bool`

`enable_payload_namespacing: bool`

`http_client: AsyncClient | ``None`

`ignore_non_compliant_errors: bool`

`is_experimental = ``True`



Python

**operations_to_exclude**

Python

**server_url_override**

Python

**stage_status**

Python

**user_agent**

Python

`operation_selection_predicate: `

`Callable[[OperationSelectionPredicateContext], bool] | ``None`

`operations_to_exclude: list[str]`

`server_url_override: str | ``None`

`stage_status = ``'experimental'`

`user_agent: str | ``None`



**OpenApiParser Class**

Reference

NOTE: SK Python only supports the OpenAPI Spec >=3.0.

Import an OpenAPI file.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**openapi_file**`

Required*

The path to the OpenAPI file which can be local or a URL.

`**openapi_file**`

Required*

The path to the OpenAPI file which can be local or a URL.

create_rest_api_operations

Create REST API operations from the parsed OpenAPI document.

parse

Parse the OpenAPI document.

**create_rest_api_operations**

Create REST API operations from the parsed OpenAPI document.

Python

`OpenApiParser()`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`create_rest_api_operations(parsed_document: Any, execution_settings: `

`OpenAPIFunctionExecutionParameters | ``None`` = ``None``) -> dict[str, `



**Parameters**

**Name**

**Description**

`**parsed_document**`

Required*

The parsed OpenAPI document.

`**execution_settings**`

Required*

The execution settings.

Default value: None

**Returns**

**Type**

**Description**

A dictionary of RestApiOperation instances.

**parse**

Parse the OpenAPI document.

Python

**Parameters**

**Name**

**Description**

`**openapi_document**`

Required*

`RestApiOperation]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`parse(openapi_document: str) -> Any | dict[str, Any] | ``None`

ﾉ

**Expand table**

**Attributes**



**PAYLOAD_PROPERTIES_HIERARCHY_MAX_DEPTH**

Python

**SUPPORTED_MEDIA_TYPES**

Python

`PAYLOAD_PROPERTIES_HIERARCHY_MAX_DEPTH: int = 10`

`SUPPORTED_MEDIA_TYPES: Final[list[str]] = [``'application/json'``, `

`'text/plain'``]`



**OperationSelectionPredicateContext**

**Class**

Reference

The context for the operation selection predicate.

Initialize the operation selection predicate context.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**operation_id**`

Required*

`**path**`

Required*

`**method**`

Required*

`**description**`

Default value: None

`OperationSelectionPredicateContext(operation_id: str, path: str, method: `

`str, description: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**search Package**

Reference

**Packages**

bing

google

ﾉ

**Expand table**



**bing Package**

Reference

**Modules**

bing_search

bing_search_response

bing_search_settings

bing_web_page

const

**Classes**

BingSearch

A search engine connector that uses the Bing Search API to perform a web

search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Bing Search class.

BingWebPage

A Bing web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**



**bing_search Module**

Reference

**Classes**

BingSearch

A search engine connector that uses the Bing Search API to perform a web
search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Bing Search class.

ﾉ

**Expand table**



**BingSearch Class**

Reference

A search engine connector that uses the Bing Search API to perform a web
search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Bing Search class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

The Bing Search API key. If provided, will override the value in the env vars

or .env file.

Default value: None

`**custom_config**`

The Bing Custom Search instance's unique identifier. If provided, will

override the value in the env vars or .env file.

Default value: None

`**env_file_path**`

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

The optional encoding of the .env file.

Default value: None

`BingSearch(api_key: str | ``None`` = ``None``, custom_config: str | ``None`` = ``None``, `

`env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_search_results

Search for text, returning a KernelSearchResult with the results directly

from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of strings.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[BingWebPage]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**is_experimental**

Python

**settings**

Python

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`settings: BingSettings`

`stage_status = ``'experimental'`



**bing_search_response Module**

Reference

**Classes**

BingSearchResponse

The response from a Bing search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

BingWebPages

The web pages from a Bing search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BingSearchResponse Class**

Reference

The response from a Bing search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**_type**`

Required*

`**queryContext**`

Required*

`**webPages**`

Required*

**is_experimental**

Python

`BingSearchResponse(*, _type: str = ``''``, queryContext: dict[str, Any] =
``None``, `

`webPages: BingWebPages | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**query_context**

Python

**stage_status**

Python

**type_**

Python

**web_pages**

Python

`is_experimental = ``True`

`query_context: dict[str, Any]`

`stage_status = ``'experimental'`

`type_: str`

`web_pages: BingWebPages | ``None`



**BingWebPages Class**

Reference

The web pages from a Bing search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**id**`

Required*

`**someResultsRemoved**`

Required*

`**totalEstimatedMatches**`

Required*

`**webSearchUrl**`

Required*

`**value**`

Required*

`BingWebPages(*, id: str | ``None`` = ``None``, someResultsRemoved: bool | ``None`` = `

`None``, totalEstimatedMatches: int | ``None`` = ``None``, webSearchUrl: str | ``None`` = `

`None``, value: list[BingWebPage] = ``None``)`

ﾉ

**Expand table**



**id**

Python

**is_experimental**

Python

**some_results_removed**

Python

**stage_status**

Python

**total_estimated_matches**

Python

**value**

Python

**Attributes**

`id: str | ``None`

`is_experimental = ``True`

`some_results_removed: bool | ``None`

`stage_status = ``'experimental'`

`total_estimated_matches: int | ``None`

`value: list[BingWebPage]`



**web_search_url**

Python

`web_search_url: str | ``None`



**bing_search_settings Module**

Reference

**Classes**

BingSettings

Bing Search settings.

The settings are first loaded from environment variables with the prefix

'>>BING_<<'. If the environment variables are not found, the settings can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the

.env file, the settings are ignored; however, validation will fail alerting
that the

settings are missing.

Optional settings for prefix '>>BING_<<' are:

api_key: SecretStr - The Bing API key (Env var BING_API_KEY)

custom_config: str - The Bing Custom Search instance's unique identifier

(Env var BING_CUSTOM_CONFIG)

ﾉ

**Expand table**



**BingSettings Class**

Reference

Bing Search settings.

The settings are first loaded from environment variables with the prefix
'>>BING_<<'. If

the environment variables are not found, the settings can be loaded from a
.env file with

the encoding 'utf-8'. If the settings are not found in the .env file, the
settings are

ignored; however, validation will fail alerting that the settings are missing.

Optional settings for prefix '>>BING_<<' are:

api_key: SecretStr - The Bing API key (Env var BING_API_KEY)

custom_config: str - The Bing Custom Search instance's unique identifier (Env
var

BING_CUSTOM_CONFIG)

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`BingSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr, custom_config: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**custom_config**`

Required*

**api_key**

Python

**custom_config**

Python

**env_prefix**

Python

**Attributes**

`api_key: SecretStr`

`custom_config: str | ``None`

`env_prefix: ClassVar[str] = ``'BING_'`



**bing_web_page Module**

Reference

**Classes**

BingWebPage

A Bing web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**BingWebPage Class**

Reference

A Bing web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**id**`

Required*

`**name**`

Required*

`**url**`

Required*

`**display_url**`

Required*

`**snippet**`

`BingWebPage(*, id: str | ``None`` = ``None``, name: str | ``None`` = ``None``, url: str | `

`None`` = ``None``, display_url: str | ``None`` = ``None``, snippet: str | ``None`` = ``None``, `

`date_last_crawled: str | ``None`` = ``None``, deep_links: list[BingWebPage] | ``None`` = `

`None``, open_graph_image: list[dict[str, str | int]] | ``None`` = ``None``, `

`search_tags: list[dict[str, str]] | ``None`` = ``None``, language: str | ``None`` = `

`None``, is_family_friendly: bool | ``None`` = ``None``, is_navigational: bool | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**date_last_crawled**`

Required*

`**deep_links**`

Required*

`**open_graph_image**`

Required*

`**search_tags**`

Required*

`**language**`

Required*

`**is_family_friendly**`

Required*

`**is_navigational**`

Required*

**date_last_crawled**

Python

**deep_links**

Python

**display_url**

Python

**Attributes**

`date_last_crawled: str | ``None`

`deep_links: list[BingWebPage] | ``None`

`display_url: str | ``None`



**id**

Python

**is_experimental**

Python

**is_family_friendly**

Python

**is_navigational**

Python

**language**

Python

**name**

Python

**open_graph_image**

`id: str | ``None`

`is_experimental = ``True`

`is_family_friendly: bool | ``None`

`is_navigational: bool | ``None`

`language: str | ``None`

`name: str | ``None`



Python

**search_tags**

Python

**snippet**

Python

**stage_status**

Python

**url**

Python

`open_graph_image: list[dict[str, str | int]] | ``None`

`search_tags: list[dict[str, str]] | ``None`

`snippet: str | ``None`

`stage_status = ``'experimental'`

`url: str | ``None`



**const Module**

Reference



**BingSearch Class**

Reference

A search engine connector that uses the Bing Search API to perform a web
search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Bing Search class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

The Bing Search API key. If provided, will override the value in the env vars

or .env file.

Default value: None

`**custom_config**`

The Bing Custom Search instance's unique identifier. If provided, will

override the value in the env vars or .env file.

Default value: None

`**env_file_path**`

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

The optional encoding of the .env file.

Default value: None

`BingSearch(api_key: str | ``None`` = ``None``, custom_config: str | ``None`` = ``None``, `

`env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_search_results

Search for text, returning a KernelSearchResult with the results directly

from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of strings.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[BingWebPage]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**is_experimental**

Python

**settings**

Python

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`settings: BingSettings`

`stage_status = ``'experimental'`



**BingWebPage Class**

Reference

A Bing web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**id**`

Required*

`**name**`

Required*

`**url**`

Required*

`**display_url**`

Required*

`**snippet**`

`BingWebPage(*, id: str | ``None`` = ``None``, name: str | ``None`` = ``None``, url: str | `

`None`` = ``None``, display_url: str | ``None`` = ``None``, snippet: str | ``None`` = ``None``, `

`date_last_crawled: str | ``None`` = ``None``, deep_links: list[BingWebPage] | ``None`` = `

`None``, open_graph_image: list[dict[str, str | int]] | ``None`` = ``None``, `

`search_tags: list[dict[str, str]] | ``None`` = ``None``, language: str | ``None`` = `

`None``, is_family_friendly: bool | ``None`` = ``None``, is_navigational: bool | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**date_last_crawled**`

Required*

`**deep_links**`

Required*

`**open_graph_image**`

Required*

`**search_tags**`

Required*

`**language**`

Required*

`**is_family_friendly**`

Required*

`**is_navigational**`

Required*

**date_last_crawled**

Python

**deep_links**

Python

**display_url**

Python

**Attributes**

`date_last_crawled: str | ``None`

`deep_links: list[BingWebPage] | ``None`

`display_url: str | ``None`



**id**

Python

**is_experimental**

Python

**is_family_friendly**

Python

**is_navigational**

Python

**language**

Python

**name**

Python

**open_graph_image**

`id: str | ``None`

`is_experimental = ``True`

`is_family_friendly: bool | ``None`

`is_navigational: bool | ``None`

`language: str | ``None`

`name: str | ``None`



Python

**search_tags**

Python

**snippet**

Python

**stage_status**

Python

**url**

Python

`open_graph_image: list[dict[str, str | int]] | ``None`

`search_tags: list[dict[str, str]] | ``None`

`snippet: str | ``None`

`stage_status = ``'experimental'`

`url: str | ``None`



**google Package**

Reference

**Modules**

const

google_search

google_search_response

google_search_result

google_search_settings

**Classes**

GoogleSearch

A search engine connector that uses the Google Search API to perform

a web search.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of the Google Search class.

GoogleSearchResponse

The response from a Google search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

GoogleSearchResult

A Google web page.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**

ﾉ

**Expand table**



Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.



**const Module**

Reference



**google_search Module**

Reference

**Classes**

GoogleSearch

A search engine connector that uses the Google Search API to perform a web

search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Google Search class.

ﾉ

**Expand table**



**GoogleSearch Class**

Reference

A search engine connector that uses the Google Search API to perform a web
search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Google Search class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

The Google Search API key. If provided, will override the value in the env

vars or .env file.

Default value: None

`**search_engine_id**`

The Google search engine ID. If provided, will override the value in the

env vars or .env file.

Default value: None

`**env_file_path**`

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

The optional encoding of the .env file.

Default value: None

`GoogleSearch(api_key: str | ``None`` = ``None``, search_engine_id: str | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_search_results

Search for text, returning a KernelSearchResult with the results directly

from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of strings.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[GoogleSearchResult]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**is_experimental**

Python

**settings**

Python

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`settings: GoogleSearchSettings`

`stage_status = ``'experimental'`



**google_search_response Module**

Reference

**Classes**

GoogleSearchInformation

Information about the search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

GoogleSearchResponse

The response from a Google search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**GoogleSearchInformation Class**

Reference

Information about the search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**searchTime**`

Required*

`**formattedSearchTime**`

Required*

`**totalResults**`

Required*

`**formattedTotalResults**`

Required*

**formatted_search_time**

`GoogleSearchInformation(*, searchTime: float, formattedSearchTime: str, `

`totalResults: str, formattedTotalResults: str)`

ﾉ

**Expand table**

**Attributes**



Python

**formatted_total_results**

Python

**is_experimental**

Python

**search_time**

Python

**stage_status**

Python

**total_results**

Python

`formatted_search_time: str`

`formatted_total_results: str`

`is_experimental = ``True`

`search_time: float`

`stage_status = ``'experimental'`

`total_results: str`



**GoogleSearchResponse Class**

Reference

The response from a Google search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kind**`

Required*

`**url**`

Required*

`**queries**`

Required*

`**context**`

Required*

`**search_information**`

Required*

`GoogleSearchResponse(*, kind: str = ``''``, url: dict[str, str] = ``None``,
queries: `

`dict[str, list[dict[str, str | int]]] = ``None``, context: dict[str, Any] = `

`None``, search_information: GoogleSearchInformation | ``None`` = ``None``, spelling: `

`dict[str, Any] = ``None``, promotions: list[dict[str, Any]] = ``None``,
items: `

`list[GoogleSearchResult] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**spelling**`

Required*

`**promotions**`

Required*

`**items**`

Required*

**context**

Python

**is_experimental**

Python

**items**

Python

**kind**

Python

**promotions**

**Attributes**

`context: dict[str, Any]`

`is_experimental = ``True`

`items: list[GoogleSearchResult] | ``None`

`kind: str`



Python

**queries**

Python

**search_information**

Python

**spelling**

Python

**stage_status**

Python

**url**

Python

`promotions: list[dict[str, Any]]`

`queries: dict[str, list[dict[str, str | int]]]`

`search_information: GoogleSearchInformation | ``None`

`spelling: dict[str, Any]`

`stage_status = ``'experimental'`

`url: dict[str, str]`



**google_search_result Module**

Reference

**Classes**

GoogleSearchResult

A Google web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**GoogleSearchResult Class**

Reference

A Google web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kind**`

Required*

`**title**`

Required*

`**htmlTitle**`

Required*

`**link**`

Required*

`**displayLink**`

Required*

`GoogleSearchResult(*, kind: str = ``''``, title: str = ``''``, htmlTitle: str
= ``''``, `

`link: str = ``''``, displayLink: str = ``''``, snippet: str = ``''``,
htmlSnippet: str = `

`''``, cacheId: str = ``''``, formattedUrl: str = ``''``, htmlFormattedUrl:
str = ``''``, `

`pagemap: dict[str, Any] = ``None``, mime: str = ``''``, fileFormat: str =
``''``, image: `

`dict[str, Any] = ``None``, labels: list[dict[str, Any]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**snippet**`

Required*

`**htmlSnippet**`

Required*

`**cacheId**`

Required*

`**formattedUrl**`

Required*

`**htmlFormattedUrl**`

Required*

`**pagemap**`

Required*

`**mime**`

Required*

`**fileFormat**`

Required*

`**image**`

Required*

`**labels**`

Required*

**cache_id**

Python

**display_link**

Python

**Attributes**

`cache_id: str`

`display_link: str`



**file_format**

Python

**formatted_url**

Python

**html_formatted_url**

Python

**html_snippet**

Python

**html_title**

Python

**image**

Python

**is_experimental**

`file_format: str`

`formatted_url: str`

`html_formatted_url: str`

`html_snippet: str`

`html_title: str`

`image: dict[str, Any]`



Python

**kind**

Python

**labels**

Python

**link**

Python

**mime**

Python

**pagemap**

Python

**snippet**

Python

`is_experimental = ``True`

`kind: str`

`labels: list[dict[str, Any]]`

`link: str`

`mime: str`

`pagemap: dict[str, Any]`



**stage_status**

Python

**title**

Python

`snippet: str`

`stage_status = ``'experimental'`

`title: str`



**google_search_settings Module**

Reference

**Classes**

GoogleSearchSettings

Google Search Connector settings.

The settings are first loaded from environment variables with the prefix

'>>GOOGLE_<<'. If the environment variables are not found, the settings

can be loaded from a .env file with the encoding 'utf-8'. If the settings are

not found in the .env file, the settings are ignored; however, validation

will fail alerting that the settings are missing.

Required settings for prefix '>>GOOGLE_SEARCH_<<' are:

api_key: SecretStr - The Google Search API key (Env var

GOOGLE_SEARCH_API_KEY)

Optional settings for prefix '>>GOOGLE_SEARCH_<<' are:

engine_id: str - The Google search engine ID (Env var

GOOGLE_SEARCH_ENGINE_ID)

env_file_path: str | None - if provided, the .env settings are read

from this file path location

env_file_encoding: str - if provided, the .env file encoding used.

Defaults to "utf-8".

ﾉ

**Expand table**



**GoogleSearchSettings Class**

Reference

Google Search Connector settings.

The settings are first loaded from environment variables with the prefix

'>>GOOGLE_<<'. If the environment variables are not found, the settings can be
loaded

from a .env file with the encoding 'utf-8'. If the settings are not found in
the .env file, the

settings are ignored; however, validation will fail alerting that the settings
are missing.

Required settings for prefix '>>GOOGLE_SEARCH_<<' are:

api_key: SecretStr - The Google Search API key (Env var GOOGLE_SEARCH_API_KEY)

Optional settings for prefix '>>GOOGLE_SEARCH_<<' are:

engine_id: str - The Google search engine ID (Env var

GOOGLE_SEARCH_ENGINE_ID)

env_file_path: str | None - if provided, the .env settings are read from this file path

location

env_file_encoding: str - if provided, the .env file encoding used. Defaults to
"utf-8".

**Constructor**

Python

**Parameters**

`GoogleSearchSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr, engine_id: str | ``None`` = ``None``)`



**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**engine_id**`

Required*

**api_key**

Python

**engine_id**

Python

**env_prefix**

Python

**Attributes**

`api_key: SecretStr`

`engine_id: str | ``None`

`env_prefix: ClassVar[str] = ``'GOOGLE_SEARCH_'`



**GoogleSearch Class**

Reference

A search engine connector that uses the Google Search API to perform a web
search.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the Google Search class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

The Google Search API key. If provided, will override the value in the env

vars or .env file.

Default value: None

`**search_engine_id**`

The Google search engine ID. If provided, will override the value in the

env vars or .env file.

Default value: None

`**env_file_path**`

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

The optional encoding of the .env file.

Default value: None

`GoogleSearch(api_key: str | ``None`` = ``None``, search_engine_id: str | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_search_results

Search for text, returning a KernelSearchResult with the results directly

from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of strings.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[GoogleSearchResult]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**is_experimental**

Python

**settings**

Python

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`settings: GoogleSearchSettings`

`stage_status = ``'experimental'`



**GoogleSearchResponse Class**

Reference

The response from a Google search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kind**`

Required*

`**url**`

Required*

`**queries**`

Required*

`**context**`

Required*

`**search_information**`

Required*

`GoogleSearchResponse(*, kind: str = ``''``, url: dict[str, str] = ``None``,
queries: `

`dict[str, list[dict[str, str | int]]] = ``None``, context: dict[str, Any] = `

`None``, search_information: GoogleSearchInformation | ``None`` = ``None``, spelling: `

`dict[str, Any] = ``None``, promotions: list[dict[str, Any]] = ``None``,
items: `

`list[GoogleSearchResult] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**spelling**`

Required*

`**promotions**`

Required*

`**items**`

Required*

**context**

Python

**is_experimental**

Python

**items**

Python

**kind**

Python

**promotions**

**Attributes**

`context: dict[str, Any]`

`is_experimental = ``True`

`items: list[GoogleSearchResult] | ``None`

`kind: str`



Python

**queries**

Python

**search_information**

Python

**spelling**

Python

**stage_status**

Python

**url**

Python

`promotions: list[dict[str, Any]]`

`queries: dict[str, list[dict[str, str | int]]]`

`search_information: GoogleSearchInformation | ``None`

`spelling: dict[str, Any]`

`stage_status = ``'experimental'`

`url: dict[str, str]`



**GoogleSearchResult Class**

Reference

A Google web page.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kind**`

Required*

`**title**`

Required*

`**htmlTitle**`

Required*

`**link**`

Required*

`**displayLink**`

Required*

`GoogleSearchResult(*, kind: str = ``''``, title: str = ``''``, htmlTitle: str
= ``''``, `

`link: str = ``''``, displayLink: str = ``''``, snippet: str = ``''``,
htmlSnippet: str = `

`''``, cacheId: str = ``''``, formattedUrl: str = ``''``, htmlFormattedUrl:
str = ``''``, `

`pagemap: dict[str, Any] = ``None``, mime: str = ``''``, fileFormat: str =
``''``, image: `

`dict[str, Any] = ``None``, labels: list[dict[str, Any]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**snippet**`

Required*

`**htmlSnippet**`

Required*

`**cacheId**`

Required*

`**formattedUrl**`

Required*

`**htmlFormattedUrl**`

Required*

`**pagemap**`

Required*

`**mime**`

Required*

`**fileFormat**`

Required*

`**image**`

Required*

`**labels**`

Required*

**cache_id**

Python

**display_link**

Python

**Attributes**

`cache_id: str`

`display_link: str`



**file_format**

Python

**formatted_url**

Python

**html_formatted_url**

Python

**html_snippet**

Python

**html_title**

Python

**image**

Python

**is_experimental**

`file_format: str`

`formatted_url: str`

`html_formatted_url: str`

`html_snippet: str`

`html_title: str`

`image: dict[str, Any]`



Python

**kind**

Python

**labels**

Python

**link**

Python

**mime**

Python

**pagemap**

Python

**snippet**

Python

`is_experimental = ``True`

`kind: str`

`labels: list[dict[str, Any]]`

`link: str`

`mime: str`

`pagemap: dict[str, Any]`



**stage_status**

Python

**title**

Python

`snippet: str`

`stage_status = ``'experimental'`

`title: str`



**search_engine Package**

Reference

**Modules**

bing_connector

bing_connector_settings

connector

google_connector

google_search_settings

**Classes**

BingConnector

A search engine connector that uses the Bing Search API to perform a web

search.

Initializes a new instance of the BingConnector class.

GoogleConnector

A search engine connector that uses the Google Custom Search API to

perform a web search.

Initializes a new instance of the GoogleConnector class.

ﾉ

**Expand table**

ﾉ

**Expand table**



**bing_connector Module**

Reference

**Classes**

BingConnector

A search engine connector that uses the Bing Search API to perform a web

search.

Initializes a new instance of the BingConnector class.

ﾉ

**Expand table**



**BingConnector Class**

Reference

A search engine connector that uses the Bing Search API to perform a web
search.

Initializes a new instance of the BingConnector class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

<xref:<xref:semantic_kernel.connectors.search_engine.bing_connector.str |

None>>

The Bing Search API key. If provided, will override the value in the env vars

or .env file.

Default value: None

`**custom_config**`

<xref:<xref:semantic_kernel.connectors.search_engine.bing_connector.str |

None>>

The Bing Custom Search instance's unique identifier. If provided, will

override the value in the env vars or .env file.

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.search_engine.bing_connector.str |

None>>

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.search_engine.bing_connector.str |

None>>

The optional encoding of the .env file.

Default value: None

`BingConnector(api_key: str | ``None`` = ``None``, custom_config: str | ``None`` = ``None``, `

`env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



search

Returns the search results of the query provided by pinging the Bing web
search API.

**search**

Returns the search results of the query provided by pinging the Bing web
search API.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**num_results**`

Required*

Default value: 1

`**offset**`

Required*

Default value: 0

**Methods**

ﾉ

**Expand table**

`async`` search(query: str, num_results: int = 1, offset: int = 0) -> `

`list[str]`

ﾉ

**Expand table**



**bing_connector_settings Module**

Reference

**Classes**

BingSettings

Bing Connector settings.

The settings are first loaded from environment variables with the prefix

'>>BING_<<'. If the environment variables are not found, the settings can be

loaded from a .env file with the encoding 'utf-8'. If the settings are not
found in the

.env file, the settings are ignored; however, validation will fail alerting
that the

settings are missing.

Optional settings for prefix '>>BING_<<' are:

api_key: SecretStr - The Bing API key (Env var BING_API_KEY)

custom_config: str - The Bing Custom Search instance's unique identifier

(Env var BING_CUSTOM_CONFIG)

ﾉ

**Expand table**



**BingSettings Class**

Reference

Bing Connector settings.

The settings are first loaded from environment variables with the prefix
'>>BING_<<'. If

the environment variables are not found, the settings can be loaded from a
.env file with

the encoding 'utf-8'. If the settings are not found in the .env file, the
settings are

ignored; however, validation will fail alerting that the settings are missing.

Optional settings for prefix '>>BING_<<' are:

api_key: SecretStr - The Bing API key (Env var BING_API_KEY)

custom_config: str - The Bing Custom Search instance's unique identifier (Env
var

BING_CUSTOM_CONFIG)

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`BingSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`api_key: SecretStr, custom_config: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_encoding**`

Default value: utf-8

`**api_key**`

Required*

`**custom_config**`

Required*

**api_key**

Python

**custom_config**

Python

**env_prefix**

Python

**Attributes**

`api_key: SecretStr`

`custom_config: str | ``None`

`env_prefix: ClassVar[str] = ``'BING_'`



**connector Module**

Reference

**Classes**

ConnectorBase

Base class for search engine connectors.

ﾉ

**Expand table**



**ConnectorBase Class**

Reference

Base class for search engine connectors.

**Constructor**

Python

search

Returns the search results of the query provided by pinging the search engine
API.

**search**

Returns the search results of the query provided by pinging the search engine
API.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**num_results**`

Required*

Default value: 1

`**offset**`

Required*

Default value: 0

`ConnectorBase()`

**Methods**

ﾉ

**Expand table**

`abstract ``async`` search(query: str, num_results: int = 1, offset: int = 0)
`

`-> list[str]`

ﾉ

**Expand table**







**google_connector Module**

Reference

**Classes**

GoogleConnector

A search engine connector that uses the Google Custom Search API to

perform a web search.

Initializes a new instance of the GoogleConnector class.

ﾉ

**Expand table**



**GoogleConnector Class**

Reference

A search engine connector that uses the Google Custom Search API to perform a
web

search.

Initializes a new instance of the GoogleConnector class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

<xref:

<xref:semantic_kernel.connectors.search_engine.google_connector.str |

None>>

The Google Custom Search API key. If provided, will override the value in

the env vars or .env file.

Default value: None

`**search_engine_id**`

<xref:

<xref:semantic_kernel.connectors.search_engine.google_connector.str |

None>>

The Google search engine ID. If provided, will override the value in the

env vars or .env file.

Default value: None

`**env_file_path**`

<xref:

<xref:semantic_kernel.connectors.search_engine.google_connector.str |

None>>

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`GoogleConnector(api_key: str | ``None`` = ``None``, search_engine_id: str | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_encoding**`

<xref:

<xref:semantic_kernel.connectors.search_engine.google_connector.str |

None>>

The optional encoding of the .env file.

Default value: None

search

Returns the search results of the query provided by pinging the Google Custom
search

API.

**search**

Returns the search results of the query provided by pinging the Google Custom

search API.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

str

The search query.

`**num_results**`

Required*

int

The number of search results to return. Default is 1.

Default value: 1

`**offset**`

Required*

int

The offset of the search results. Default is 0.

Default value: 0

**Methods**

ﾉ

**Expand table**

`async`` search(query: str, num_results: int = 1, offset: int = 0) -> `

`list[str]`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

list[str]

A list of search results snippets.

ﾉ

**Expand table**



**google_search_settings Module**

Reference

**Classes**

GoogleSearchSettings

Google Search Connector settings.

The settings are first loaded from environment variables with the prefix

'>>GOOGLE_<<'. If the environment variables are not found, the settings

can be loaded from a .env file with the encoding 'utf-8'. If the settings are

not found in the .env file, the settings are ignored; however, validation

will fail alerting that the settings are missing.

Required settings for prefix '>>GOOGLE_<<' are:

search_api_key: SecretStr - The Google Search API key (Env var

GOOGLE_API_KEY)

Optional settings for prefix '>>GOOGLE_<<' are:

search_engine_id: str - The Google search engine ID (Env var

GOOGLE_SEARCH_ENGINE_ID)

env_file_path: str | None - if provided, the .env settings are read

from this file path location

env_file_encoding: str - if provided, the .env file encoding used.

Defaults to "utf-8".

ﾉ

**Expand table**



**GoogleSearchSettings Class**

Reference

Google Search Connector settings.

The settings are first loaded from environment variables with the prefix

'>>GOOGLE_<<'. If the environment variables are not found, the settings can be
loaded

from a .env file with the encoding 'utf-8'. If the settings are not found in
the .env file, the

settings are ignored; however, validation will fail alerting that the settings
are missing.

Required settings for prefix '>>GOOGLE_<<' are:

search_api_key: SecretStr - The Google Search API key (Env var GOOGLE_API_KEY)

Optional settings for prefix '>>GOOGLE_<<' are:

search_engine_id: str - The Google search engine ID (Env var

GOOGLE_SEARCH_ENGINE_ID)

env_file_path: str | None - if provided, the .env settings are read from this file path

location

env_file_encoding: str - if provided, the .env file encoding used. Defaults to
"utf-8".

**Constructor**

Python

**Parameters**

`GoogleSearchSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`search_api_key: SecretStr, search_engine_id: str | ``None`` = ``None``)`



**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**search_api_key**`

Required*

`**search_engine_id**`

Required*

**env_prefix**

Python

**search_api_key**

Python

**search_engine_id**

Python

**Attributes**

`env_prefix: ClassVar[str] = ``'GOOGLE_'`

`search_api_key: SecretStr`

`search_engine_id: str | ``None`



**BingConnector Class**

Reference

A search engine connector that uses the Bing Search API to perform a web
search.

Initializes a new instance of the BingConnector class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The Bing Search API key. If provided, will override the value in the env vars

or .env file.

Default value: None

`**custom_config**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The Bing Custom Search instance's unique identifier. If provided, will

override the value in the env vars or .env file.

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The optional encoding of the .env file.

Default value: None

`BingConnector(api_key: str | ``None`` = ``None``, custom_config: str | ``None`` = ``None``, `

`env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



search

Returns the search results of the query provided by pinging the Bing web
search API.

**search**

Returns the search results of the query provided by pinging the Bing web
search API.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**num_results**`

Required*

Default value: 1

`**offset**`

Required*

Default value: 0

`async`` search(query: str, num_results: int = 1, offset: int = 0) -> `

`list[str]`

ﾉ

**Expand table**



**GoogleConnector Class**

Reference

A search engine connector that uses the Google Custom Search API to perform a
web

search.

Initializes a new instance of the GoogleConnector class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**api_key**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The Google Custom Search API key. If provided, will override the value in

the env vars or .env file.

Default value: None

`**search_engine_id**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The Google search engine ID. If provided, will override the value in the

env vars or .env file.

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The optional path to the .env file. If provided, the settings are read from

this file path location.

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.connectors.search_engine.str | None>>

The optional encoding of the .env file.

Default value: None

`GoogleConnector(api_key: str | ``None`` = ``None``, search_engine_id: str | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, env_file_encoding: str | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Methods**



search

Returns the search results of the query provided by pinging the Google Custom
search

API.

**search**

Returns the search results of the query provided by pinging the Google Custom

search API.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

str

The search query.

`**num_results**`

Required*

int

The number of search results to return. Default is 1.

Default value: 1

`**offset**`

Required*

int

The offset of the search results. Default is 0.

Default value: 0

**Returns**

**Type**

**Description**

list[str]

A list of search results snippets.

ﾉ

**Expand table**

`async`` search(query: str, num_results: int = 1, offset: int = 0) -> `

`list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**utils Package**

Reference

**Modules**

document_loader

structured_output_schema

**Classes**

DocumentLoader

Utility class to load a document from a URL.

ﾉ

**Expand table**

ﾉ

**Expand table**



**document_loader Module**

Reference

**Classes**

DocumentLoader

Utility class to load a document from a URL.

ﾉ

**Expand table**



**DocumentLoader Class**

Reference

Utility class to load a document from a URL.

**Constructor**

Python

from_uri

Load the manifest from the given URL.

**from_uri**

Load the manifest from the given URL.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

`**http_client**`

Required*

`**auth_callback**`

`DocumentLoader()`

**Methods**

ﾉ

**Expand table**

`async`` static from_uri(url: str, http_client: AsyncClient, auth_callback: `

`Callable[[...], Awaitable[dict[str, str]] | ``None``] | ``None``, user_agent: str `

`| ``None`` = ``'semantic-kernel-python'``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**user_agent**`

Required*

Default value: semantic-kernel-python



**structured_output_schema Module**

Reference

**generate_structured_output_response_format_schema**

Generate the structured output response format schema.

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

`**schema**`

Required*

**Functions**

`generate_structured_output_response_format_schema(name: str, schema: `

`dict[str, Any]) -> dict[str, Any]`

ﾉ

**Expand table**



**DocumentLoader Class**

Reference

Utility class to load a document from a URL.

**Constructor**

Python

from_uri

Load the manifest from the given URL.

**from_uri**

Load the manifest from the given URL.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

`**http_client**`

Required*

`**auth_callback**`

`DocumentLoader()`

**Methods**

ﾉ

**Expand table**

`async`` static from_uri(url: str, http_client: AsyncClient, auth_callback: `

`Callable[[...], Awaitable[dict[str, str]] | ``None``] | ``None``, user_agent: str `

`| ``None`` = ``'semantic-kernel-python'``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**user_agent**`

Required*

Default value: semantic-kernel-python



**contents Package**

Reference

**Packages**

history_reducer

utils

**Modules**

annotation_content

audio_content

binary_content

chat_history

chat_message_content

const

file_reference_content

function_call_content

function_result_content

image_content

kernel_content

streaming_annotation_content

streaming_chat_message_content

streaming_content_mixin

streaming_file_reference_content

streaming_text_content

ﾉ

**Expand table**

ﾉ

**Expand table**



text_content

**Classes**

AnnotationContent

Annotation content.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

AudioContent

Audio Content class.

This can be created either the bytes data or a data uri,

additionally it can have a uri. The uri is a reference to the

source, and might or might not point to the same thing as

the data.

Use the .from_audio_file method to create an instance from

an audio file. This reads the file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a

warning is logged.

Args: uri (Url | None): The reference uri of the content.

data_uri (DataUrl | None): The data uri of the content. data

(str | bytes | None): The data of the content. data_format (str

| None): The format of the data (e.g. base64). mime_type (str

| None): The mime type of the audio, only used with data.

kwargs (Any): Any additional arguments:

ﾉ

**Expand table**

` inner_content (Any): The inner content `

`of the response,`

` this should hold all the information `

`from the response so even`

` when not creating a subclass a `

`developer can leverage the full thing.`

` ai_model_id (str | None): The id of the `

`AI model that generated this response.`



Note: This class is marked as 'experimental' and may change

in the future.

Create an Audio Content object, either from a data_uri or

data.

ChatHistory

This class holds the history of chat messages from a chat

conversation.

Note: the system_message is added to the messages as a

ChatMessageContent instance with

role=AuthorRole.SYSTEM, but updating it will not update

the messages list.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ChatHistoryReducer

Defines a contract for reducing chat history.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ChatHistorySummarizationReducer

A ChatHistory with logic to summarize older messages past

a target count.

This class inherits from ChatHistoryReducer, which in turn

inherits from ChatHistory. It can be used anywhere a

ChatHistory is expected, while adding summarization

capability.

Args: target_count: The target message count.

threshold_count: The threshold count to avoid orphaning

messages. auto_reduce: Whether to automatically reduce

the chat history, default is False. service: The

ChatCompletion service to use for summarization.

summarization_instructions: The summarization instructions,

` metadata (dict[str, Any]): Any metadata `

`that should be attached to the response.`



optional. use_single_summary: Whether to use a single

summary message, default is True. fail_on_error: Raise error

if summarization fails, default is True.

include_function_content_in_summary: Whether to include

function calls/results in the summary, default is False.

execution_settings: The execution settings for the

summarization prompt, optional.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ChatHistoryTruncationReducer

A ChatHistory that supports truncation logic.

Because this class inherits from ChatHistoryReducer (which

in turn inherits from ChatHistory), it can also be used

anywhere a ChatHistory is expected, while adding truncation

capability.

Args: target_count: The target message count.

threshold_count: The threshold count to avoid orphaning

messages. auto_reduce: Whether to automatically reduce

the chat history, default is False.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ChatMessageContent

This is the class for chat message response content.

All Chat Completion Services should return an instance of

this class as response. Or they can implement their own

subclass of this class and return an instance.

Create a ChatMessageContent instance.

FileReferenceContent

File reference content.



Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

FunctionCallContent

Class to hold a function call response.

Create function call content.

FunctionResultContent

This class represents function result content.

Create function result content.

ImageContent

Image Content class.

This can be created either the bytes data or a data uri,

additionally it can have a uri. The uri is a reference to the

source, and might or might not point to the same thing as

the data.

Use the .from_image_file method to create an instance from

a image file. This reads the file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a

warning is logged.

Args: uri (Url | None): The reference uri of the content.

data_uri (DataUrl | None): The data uri of the content. data

(str | bytes | None): The data of the content. data_format (str

| None): The format of the data (e.g. base64). mime_type (str

| None): The mime type of the image, only used with data.

kwargs (Any): Any additional arguments:

` inner_content (Any): The inner content `

`of the response,`

` this should hold all the information `

`from the response so even`

` when not creating a subclass a `

`developer can leverage the full thing.`

` ai_model_id (str | None): The id of the `

`AI model that generated this response.`

` metadata (dict[str, Any]): Any metadata `

`that should be attached to the response.`



Methods: from_image_path: Create an instance from an

image file. **str** : Returns the string representation of the

image.

Raises: ValidationError: If neither uri or data is provided.

Note: This class is marked as 'experimental' and may change

in the future.

Create an Image Content object, either from a data_uri or

data.

StreamingAnnotationContent

Streaming Annotation content.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

StreamingChatMessageContent

This is the class for streaming chat message response

content.

All Chat Completion Services should return an instance of

this class as streaming response, where each part of the

response as it is streamed is converted to an instance of this

class, the end-user will have to either do something directly

or gather them and combine them into a new instance. A

service can implement their own subclass of this class and

return instances of that.

Create a new instance of StreamingChatMessageContent.

StreamingFileReferenceContent

Streaming File reference content.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

StreamingTextContent

This represents streaming text response content.



Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TextContent

This represents text response content.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

AuthorRole

Author role enum.

FinishReason

Finish Reason enum.

ﾉ

**Expand table**



**history_reducer Package**

Reference

**Modules**

chat_history_reducer

chat_history_reducer_utils

chat_history_summarization_reducer

chat_history_truncation_reducer

ﾉ

**Expand table**



**chat_history_reducer Module**

Reference

**Classes**

ChatHistoryReducer

Defines a contract for reducing chat history.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatHistoryReducer Class**

Reference

Defines a contract for reducing chat history.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

`ChatHistoryReducer(*, messages: list[ChatMessageContent] = ``None``, `

`system_message: str | ``None`` = ``None``, target_count: Annotated[int, Gt(gt=0)], `

`threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool =
``False``)`

ﾉ

**Expand table**



add_message_async

Add a message to the chat history.

If auto_reduce is enabled, the history will be reduced after adding the

message.

reduce

Reduce the chat history in some way (e.g., truncate, summarize).

**add_message_async**

Add a message to the chat history.

If auto_reduce is enabled, the history will be reduced after adding the
message.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

`**encoding**`

Required*

Default value: None

`**metadata**`

Required*

Default value: None

**reduce**

Reduce the chat history in some way (e.g., truncate, summarize).

Python

**Methods**

ﾉ

**Expand table**

`async`` add_message_async(message: ChatMessageContent | dict[str, Any], `

`encoding: str | ``None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``) -> `

`None`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

A possibly shorter list of messages, or None if no change is needed.

**auto_reduce**

Python

**is_experimental**

Python

**messages**

Python

**stage_status**

Python

**system_message**

`abstract ``async`` reduce() -> Self | ``None`

ﾉ

**Expand table**

**Attributes**

`auto_reduce: bool`

`is_experimental = ``True`

`messages: list[ChatMessageContent]`

`stage_status = ``'experimental'`



Python

**target_count**

Python

**threshold_count**

Python

`system_message: str | ``None`

`target_count: int`

`threshold_count: int`



**chat_history_reducer_utils Module**

Reference

**contains_function_call_or_result**

Return True if the message has any function call or function result.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**msg**`

Required*

**extract_range**

Extract a range of messages from the source history, skipping any message for
which

we do not want to keep.

For example, function calls/results, if desired.

Args: history: The source history. start: The index of the first message to
extract

(inclusive). end: The index of the last message to extract (exclusive). If
None, extracts

through end. filter_func: A function that takes a ChatMessageContent and
returns

True if the message should

**Functions**

`contains_function_call_or_result(msg: ChatMessageContent) -> bool`

ﾉ

**Expand table**

` be skipped, False otherwise.`



Returns: A list of extracted messages.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

`**start**`

Required*

`**end**`

Required*

Default value: None

`**filter_func**`

Required*

Default value: None

`**preserve_pairs**`

Required*

Default value: False

**get_call_result_pairs**

Identify all (FunctionCallContent, FunctionResultContent) pairs in the
history.

Return a list of (call_index, result_index) pairs for safe referencing.

Note: This function is marked as 'experimental' and may change in the future.

Python

` preserve_pairs: If True, ensures that function call and result pairs `

`are either both kept or both skipped.`

`extract_range(history: list[ChatMessageContent], start: int, end: int | `

`None`` = ``None``, filter_func: Callable[[ChatMessageContent], bool] | ``None`` = `

`None``, preserve_pairs: bool = ``False``) -> list[ChatMessageContent]`

ﾉ

**Expand table**

`get_call_result_pairs(history: list[ChatMessageContent]) -> `

`list[tuple[int, int]]`



**Parameters**

**Name**

**Description**

`**history**`

Required*

**locate_safe_reduction_index**

Identify the index of the first message at or beyond the specified
target_count.

This index does not orphan sensitive content (function calls/results).

This method ensures that the presence of a function-call always follows with
its

result, so the function-call and its function-result are never separated.

In addition, it attempts to locate a user message within the threshold window
so that

context with the subsequent assistant response is preserved.

Args: history: The entire chat history. target_count: The desired message
count after

reduction. threshold_count: The threshold beyond target_count required to
trigger

reduction.

Returns: The index that identifies the starting point for a reduced history
that does

not orphan sensitive content. Returns None if reduction is not needed.

Note: This function is marked as 'experimental' and may change in the future.

Python

ﾉ

**Expand table**

` If total messages <= (target_count + threshold_count), no reduction `

`occurs.`

` offset_count: Optional number of messages to skip at the start (e.g. `

`existing summary messages).`

`locate_safe_reduction_index(history: list[ChatMessageContent], `

`target_count: int, threshold_count: int = 0, offset_count: int = 0) -> `

`int | ``None`



**Parameters**

**Name**

**Description**

`**history**`

Required*

`**target_count**`

Required*

`**threshold_count**`

Required*

Default value: 0

`**offset_count**`

Required*

Default value: 0

**locate_summarization_boundary**

Identify the index of the first message that is not a summary message.

This is indicated by the presence of the SUMMARY_METADATA_KEY in the message

metadata.

Returns: The insertion point index for normal history messages (i.e., after
all summary

messages).

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**history**`

Required*

ﾉ

**Expand table**

`locate_summarization_boundary(history: list[ChatMessageContent]) -> int`

ﾉ

**Expand table**



**chat_history_summarization_reducer**

**Module**

Reference

**Classes**

ChatHistorySummarizationReducer

A ChatHistory with logic to summarize older messages past

a target count.

This class inherits from ChatHistoryReducer, which in turn

inherits from ChatHistory. It can be used anywhere a

ChatHistory is expected, while adding summarization

capability.

Args: target_count: The target message count.

threshold_count: The threshold count to avoid orphaning

messages. auto_reduce: Whether to automatically reduce

the chat history, default is False. service: The

ChatCompletion service to use for summarization.

summarization_instructions: The summarization instructions,

optional. use_single_summary: Whether to use a single

summary message, default is True. fail_on_error: Raise error

if summarization fails, default is True.

include_function_content_in_summary: Whether to include

function calls/results in the summary, default is False.

execution_settings: The execution settings for the

summarization prompt, optional.

Note: This class is marked as 'experimental' and may change

in the future.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatHistorySummarizationReducer**

**Class**

Reference

A ChatHistory with logic to summarize older messages past a target count.

This class inherits from ChatHistoryReducer, which in turn inherits from
ChatHistory. It

can be used anywhere a ChatHistory is expected, while adding summarization
capability.

Args: target_count: The target message count. threshold_count: The threshold
count to

avoid orphaning messages. auto_reduce: Whether to automatically reduce the
chat

history, default is False. service: The ChatCompletion service to use for
summarization.

summarization_instructions: The summarization instructions, optional.

use_single_summary: Whether to use a single summary message, default is True.

fail_on_error: Raise error if summarization fails, default is True.

include_function_content_in_summary: Whether to include function calls/results
in the

summary, default is False. execution_settings: The execution settings for the

summarization prompt, optional.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

`ChatHistorySummarizationReducer(*, messages: list[ChatMessageContent] = `

`None``, system_message: str | ``None`` = ``None``, target_count: Annotated[int, `

`Gt(gt=0)], threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool `

`= ``False``, service: ChatCompletionClientBase, summarization_instructions:
str `

`= ``'\nProvide a concise and complete summarization of the entire dialog that
`

`does not exceed 5 sentences.\n\nThis summary must always:\n- Consider both `

`user and assistant interactions\n- Maintain continuity for the purpose of `

`further dialog\n- Include details from any existing summary\n- Focus on the `

`most significant aspects of the dialog\n\nThis summary must never:\n- `

`Critique, correct, interpret, presume, or assume\n- Identify faults, `

`mistakes, misunderstanding, or correctness\n- Analyze what has not `

`occurred\n- Exclude details from any existing summary\n'``, `



**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

`**service**`

Required*

`**summarization_instructions**`

Default value: Provide a concise and complete

summarization of the entire dialog that does not

exceed 5 sentences. This summary must always: -

Consider both user and assistant interactions -

Maintain continuity for the purpose of further

dialog - Include details from any existing summary

\- Focus on the most significant aspects of the

dialog This summary must never: - Critique, correct,

interpret, presume, or assume - Identify faults,

mistakes, misunderstanding, or correctness -

Analyze what has not occurred - Exclude details

from any existing summary

`**use_single_summary**`

Default value: True

`**fail_on_error**`

Default value: True

`**include_function_content_in_summary**`

Required*

`**execution_settings**`

`use_single_summary: bool = ``True``, fail_on_error: bool = ``True``, `

`include_function_content_in_summary: bool = ``False``, execution_settings: `

`PromptExecutionSettings | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

reduce

**reduce**

Python

**auto_reduce**

Python

**execution_settings**

Python

**fail_on_error**

Python

**include_function_content_in_summary**

**Methods**

ﾉ

**Expand table**

`async`` reduce() -> Self | ``None`

**Attributes**

`auto_reduce: bool`

`execution_settings: PromptExecutionSettings | ``None`

`fail_on_error: bool`



Python

**is_experimental**

Python

**messages**

Python

**service**

Python

**stage_status**

Python

**summarization_instructions**

Python

**system_message**

Python

`include_function_content_in_summary: bool`

`is_experimental = ``True`

`messages: list[ChatMessageContent]`

`service: ChatCompletionClientBase`

`stage_status = ``'experimental'`

`summarization_instructions: str`



**target_count**

Python

**threshold_count**

Python

**use_single_summary**

Python

`system_message: str | ``None`

`target_count: int`

`threshold_count: int`

`use_single_summary: bool`



**chat_history_truncation_reducer Module**

Reference

**Classes**

ChatHistoryTruncationReducer

A ChatHistory that supports truncation logic.

Because this class inherits from ChatHistoryReducer (which in

turn inherits from ChatHistory), it can also be used anywhere a

ChatHistory is expected, while adding truncation capability.

Args: target_count: The target message count. threshold_count:

The threshold count to avoid orphaning messages. auto_reduce:

Whether to automatically reduce the chat history, default is False.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatHistoryTruncationReducer Class**

Reference

A ChatHistory that supports truncation logic.

Because this class inherits from ChatHistoryReducer (which in turn inherits
from

ChatHistory), it can also be used anywhere a ChatHistory is expected, while
adding

truncation capability.

Args: target_count: The target message count. threshold_count: The threshold
count to

avoid orphaning messages. auto_reduce: Whether to automatically reduce the
chat

history, default is False.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

`ChatHistoryTruncationReducer(*, messages: list[ChatMessageContent] =
``None``, `

`system_message: str | ``None`` = ``None``, target_count: Annotated[int, Gt(gt=0)], `

`threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool =
``False``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

reduce

**reduce**

Python

**auto_reduce**

Python

**is_experimental**

Python

**messages**

Python

**Methods**

ﾉ

**Expand table**

`async`` reduce() -> Self | ``None`

**Attributes**

`auto_reduce: bool`

`is_experimental = ``True`



**stage_status**

Python

**system_message**

Python

**target_count**

Python

**threshold_count**

Python

`messages: list[ChatMessageContent]`

`stage_status = ``'experimental'`

`system_message: str | ``None`

`target_count: int`

`threshold_count: int`



**utils Package**

Reference

**Modules**

author_role

data_uri

finish_reason

hashing

ﾉ

**Expand table**



**author_role Module**

Reference

**Enums**

AuthorRole

Author role enum.

ﾉ

**Expand table**



**AuthorRole Enum**

Reference

Author role enum.

ASSISTANT

DEVELOPER

SYSTEM

TOOL

USER

**Fields**

ﾉ

**Expand table**



**data_uri Module**

Reference

**Classes**

DataUri

A class to represent a data uri.

If a array is provided, that will be used as the data since it is the most
efficient,

otherwise the bytes will be used, or the string will be converted to bytes.

When updating either array or bytes, the other will not be updated.

Initialize the data uri.

Make sure to set the data_format to base64 so that it can be decoded properly.

ﾉ

**Expand table**



**DataUri Class**

Reference

A class to represent a data uri.

If a array is provided, that will be used as the data since it is the most
efficient, otherwise

the bytes will be used, or the string will be converted to bytes.

When updating either array or bytes, the other will not be updated.

Initialize the data uri.

Make sure to set the data_format to base64 so that it can be decoded properly.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**data_bytes**`

The data as bytes.

Default value: None

`**data_str**`

The data as a string.

Default value: None

`**data_array**`

The data as a numpy array.

Default value: None

`**mime_type**`

The mime type of the data.

Default value: None

`**parameters**`

Any parameters for the data.

Default value: None

`DataUri(data_bytes: bytes | ``None`` = ``None``, data_str: str | ``None`` = ``None``, `

`data_array: ndarray | ``None`` = ``None``, mime_type: str | ``None`` = ``None``, parameters: `

`Sequence[str] | Mapping[str, str] | ``None`` = ``None``, data_format: str | ``None`` = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**data_bytes**`

Required*

The data as bytes.

`**data_str**`

Required*

The data as a string.

`**data_array**`

Required*

The data as a numpy array.

`**mime_type**`

Required*

The mime type of the data.

`**parameters**`

Required*

Any parameters for the data.

`**data_format**`

Required*

The format of the data (e.g. base64).

`**kwargs**`

Required*

Any additional arguments.

from_data_uri

Create a DataUri object from a data URI string or pydantic URL.

to_string

Return the data uri as a string.

update_data

Update the data, using either a string or bytes.

**from_data_uri**

Create a DataUri object from a data URI string or pydantic URL.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`from_data_uri(data_uri: str | Url, default_mime_type: str = ``'text/plain'``) `

`-> _T`



**Name**

**Description**

`**data_uri**`

Required*

`**default_mime_type**`

Required*

Default value: text/plain

**to_string**

Return the data uri as a string.

Python

**Parameters**

**Name**

**Description**

`**metadata**`

Default value: {}

**update_data**

Update the data, using either a string or bytes.

Python

**Parameters**

**Name**

**Description**

`**value**`

Required*

ﾉ

**Expand table**

`to_string(metadata: dict[str, str] = {}) -> str`

ﾉ

**Expand table**

`update_data(value: str | bytes | ndarray) -> ``None`

ﾉ

**Expand table**



**data_array**

Python

**data_bytes**

Python

**data_format**

Python

**mime_type**

Python

**parameters**

Python

**Attributes**

`data_array: ndarray | ``None`

`data_bytes: bytes | ``None`

`data_format: str | ``None`

`mime_type: str | ``None`

`parameters: MutableMapping[str, str]`



**finish_reason Module**

Reference

**Enums**

FinishReason

Finish Reason enum.

ﾉ

**Expand table**



**FinishReason Enum**

Reference

Finish Reason enum.

CONTENT_FILTER

FUNCTION_CALL

LENGTH

STOP

TOOL_CALLS

**Fields**

ﾉ

**Expand table**



**hashing Module**

Reference

**make_hashable**

Recursively convert unhashable types to hashable equivalents.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

The input to convert to a hashable type.

`**visited**`

Required*

A dictionary of visited objects to prevent infinite recursion.

Default value: None

**Returns**

**Type**

**Description**

Any

The input converted to a hashable type.

**Functions**

`make_hashable(input: Any, visited=``None``) -> Any`

ﾉ

**Expand table**

ﾉ

**Expand table**



**annotation_content Module**

Reference

**Classes**

AnnotationContent

Annotation content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AnnotationContent Class**

Reference

Annotation content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: annotation

`**file_id**`

Required*

`**quote**`

`AnnotationContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.ANNOTATION_CONTENT] = ``'annotation'``, file_id: str | ``None`` `

`= ``None``, quote: str | ``None`` = ``None``, start_index: int | ``None`` = ``None``, end_index: `

`int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**start_index**`

Required*

`**end_index**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the annotation content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**to_element**

Convert the annotation content to an Element.

Python

**ai_model_id**

Python

**content_type**

Python

**end_index**

Python

**file_id**

Python

`to_dict() -> dict[str, Any]`

`to_element() -> Element`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.ANNOTATION_CONTENT]`

`end_index: int | ``None`

`file_id: str | ``None`



**inner_content**

Python

**is_experimental**

Python

**metadata**

Python

**quote**

Python

**stage_status**

Python

**start_index**

Python

**tag**

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`

`metadata: dict[str, Any]`

`quote: str | ``None`

`stage_status = ``'experimental'`

`start_index: int | ``None`



Python

`tag: ClassVar[str] = ``'annotation'`



**audio_content Module**

Reference

**Classes**

AudioContent

Audio Content class.

This can be created either the bytes data or a data uri, additionally it can
have a

uri. The uri is a reference to the source, and might or might not point to the
same

thing as the data.

Use the .from_audio_file method to create an instance from an audio file. This

reads the file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None):

The data uri of the content. data (str | bytes | None): The data of the content.

data_format (str | None): The format of the data (e.g. base64). mime_type (str |

None): The mime type of the audio, only used with data. kwargs (Any): Any

additional arguments:

Note: This class is marked as 'experimental' and may change in the future.

Create an Audio Content object, either from a data_uri or data.

ﾉ

**Expand table**

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the `

`response so even`

` when not creating a subclass a developer can `

`leverage the full thing.`

` ai_model_id (str | None): The id of the AI model that `

`generated this response.`

` metadata (dict[str, Any]): Any metadata that should be `

`attached to the response.`



**AudioContent Class**

Reference

Audio Content class.

This can be created either the bytes data or a data uri, additionally it can
have a uri. The

uri is a reference to the source, and might or might not point to the same
thing as the

data.

Use the .from_audio_file method to create an instance from an audio file. This
reads the

file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None): The

data uri of the content. data (str | bytes | None): The data of the content. data_format

(str | None): The format of the data (e.g. base64). mime_type (str | None): The mime type

of the audio, only used with data. kwargs (Any): Any additional arguments:

Note: This class is marked as 'experimental' and may change in the future.

Create an Audio Content object, either from a data_uri or data.

**Constructor**

Python

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the response so even`

` when not creating a subclass a developer can leverage the full `

`thing.`

` ai_model_id (str | None): The id of the AI model that generated this `

`response.`

` metadata (dict[str, Any]): Any metadata that should be attached to the `

`response.`

`AudioContent(uri: str | ``None`` = ``None``, data_uri: str | ``None`` = ``None``, data: str `

`| bytes | ndarray | ``None`` = ``None``, data_format: str | ``None`` = ``None``, mime_type: `

`str | ``None`` = ``None``, *, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.AUDIO_CONTENT] = ``'audio'``)`



**Parameters**

**Name**

**Description**

`**uri**`

The reference uri of the content.

Default value: None

`**data_uri**`

The data uri of the content.

Default value: None

`**data**`

The data of the content.

Default value: None

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**mime_type**`

The mime type of the audio, only used with data.

Default value: None

`**kwargs**`

Required*

Any additional arguments: inner_content: The inner content of the response,

this should hold all the information from the response so even when not
creating

a subclass a developer can leverage the full thing.

ai_model_id: The id of the AI model that generated this response. metadata:
Any

metadata that should be attached to the response.

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: audio

ﾉ

**Expand table**

ﾉ

**Expand table**

**Methods**



from_audio_file

Create an instance from an audio file.

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

to_dict

Convert the instance to a dictionary.

**from_audio_file**

Create an instance from an audio file.

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

ﾉ

**Expand table**

`from_audio_file(path: str) -> AudioContent`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**ai_model_id**

Python

**content_type**

Python

**inner_content**

Python

**is_experimental**

Python

`to_dict() -> dict[str, Any]`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.AUDIO_CONTENT]`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`



**metadata**

Python

**stage_status**

Python

**tag**

Python

**uri**

Python

`metadata: dict[str, Any]`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'audio'`

`uri: Url | str | ``None`



**binary_content Module**

Reference

**Classes**

BinaryContent

This is a base class for different types of binary content.

This can be created either the bytes data or a data uri, additionally it can
have a

uri. The uri is a reference to the source, and might or might not point to the
same

thing as the data.

Ideally only subclasses of this class are used, like ImageContent.

Methods: **str** : Returns the string representation of the content.

Raises: ValidationError: If any arguments are malformed.

Note: This class is marked as 'experimental' and may change in the future.

Create a Binary Content object, either from a data_uri or data.

ﾉ

**Expand table**



**BinaryContent Class**

Reference

This is a base class for different types of binary content.

This can be created either the bytes data or a data uri, additionally it can
have a uri. The

uri is a reference to the source, and might or might not point to the same
thing as the

data.

Ideally only subclasses of this class are used, like ImageContent.

Methods: **str** : Returns the string representation of the content.

Raises: ValidationError: If any arguments are malformed.

Note: This class is marked as 'experimental' and may change in the future.

Create a Binary Content object, either from a data_uri or data.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**uri**`

The reference uri of the content.

Default value: None

`**data_uri**`

The data uri of the content.

Default value: None

`BinaryContent(uri: Url | str | ``None`` = ``None``, data_uri: Annotated[Url, `

`UrlConstraints(max_length=``None``, allowed_schemes=[``'data'``], `

`host_required=``None``, default_host=``None``, default_port=``None``, `

`default_path=``None``)] | str | ``None`` = ``None``, data: str | bytes | ndarray | ``None`` `

`= ``None``, data_format: str | ``None`` = ``None``, mime_type: str | ``None`` = ``None``, *, `

`inner_content: Any | ``None`` = ``None``, ai_model_id: str | ``None`` = ``None``, metadata: `

`dict[str, Any] = ``None``, content_type: Literal[ContentTypes.BINARY_CONTENT]
= `

`'binary'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**data**`

The data of the content.

Default value: None

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**mime_type**`

The mime type of the content, not always relevant.

Default value: None

`**kwargs**`

Required*

Any additional arguments: inner_content: The inner content of the response,

this should hold all the information from the response so even when not
creating

a subclass a developer can leverage the full thing.

ai_model_id: The id of the AI model that generated this response. metadata:
Any

metadata that should be attached to the response.

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: binary

from_element

Create an instance from an Element.

model_post_init

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when

calling it.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



to_dict

Convert the instance to a dictionary.

to_element

Convert the instance to an Element.

write_to_file

Write the data to a file.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when
calling

it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Parameters**

**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**write_to_file**

Write the data to a file.

Python

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`

`to_element() -> Element`

`write_to_file(path: str | Annotated[Path, PathType(path_type=file)]) -> `

`None`



**Parameters**

**Name**

**Description**

`**path**`

Required*

**data**

Get the data.

**data_uri**

Get the data uri.

**mime_type**

Get the mime type.

**content_type**

Python

**default_mime_type**

Python

**is_experimental**

Python

ﾉ

**Expand table**

**Attributes**

`content_type: Literal[ContentTypes.BINARY_CONTENT]`

`default_mime_type: ClassVar[str] = ``'text/plain'`



**stage_status**

Python

**tag**

Python

**uri**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'binary'`

`uri: Url | str | ``None`



**chat_history Module**

Reference

**Classes**

ChatHistory

This class holds the history of chat messages from a chat conversation.

Note: the system_message is added to the messages as a ChatMessageContent

instance with role=AuthorRole.SYSTEM, but updating it will not update the

messages list.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ChatHistory Class**

Reference

This class holds the history of chat messages from a chat conversation.

Note: the system_message is added to the messages as a ChatMessageContent
instance

with role=AuthorRole.SYSTEM, but updating it will not update the messages
list.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

The messages to add to the chat history.

`**system_message**`

Required*

A system message to add to the chat history, optional. if passed, it is added

to the messages as a ChatMessageContent instance with

role=AuthorRole.SYSTEM before any other messages.

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

`ChatHistory(*, messages: list[ChatMessageContent] = ``None``, system_message:
`

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**system_message**`

Required*

add_assistant_message

Add an assistant message to the chat history.

add_developer_message

Add a system message to the chat history.

add_message

Add a message to the history.

This method accepts either a ChatMessageContent instance or a

dictionary with the necessary information to construct a

ChatMessageContent instance.

add_system_message

Add a system message to the chat history.

add_tool_message

Add a tool message to the chat history.

add_user_message

Add a user message to the chat history.

clear

Clear the chat history.

extend

Extend the chat history with a list of messages.

from_rendered_prompt

Create a ChatHistory instance from a rendered prompt.

load_chat_history_from_file

Loads the ChatHistory from a file.

remove_message

Remove a message from the history.

replace

Replace the chat history with a list of messages.

This calls clear() and then extend(messages=messages).

restore_chat_history

Restores a ChatHistory instance from a JSON string.

serialize

Serializes the ChatHistory instance to a JSON string.

store_chat_history_to_file

Stores the serialized ChatHistory to a file.

to_prompt

Return a string representation of the history.

**add_assistant_message**

**Methods**

ﾉ

**Expand table**



Add an assistant message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the assistant message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_developer_message**

Add a system message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the developer message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`add_assistant_message(content: str | list[KernelContent], **kwargs: Any) `

`-> ``None`

ﾉ

**Expand table**

`add_developer_message(content: str | list[KernelContent], **kwargs) -> `

`None`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

Additional keyword arguments.

**add_message**

Add a message to the history.

This method accepts either a ChatMessageContent instance or a dictionary with
the

necessary information to construct a ChatMessageContent instance.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

<xref:Union>[<xref:ChatMessageContent>,dict]

The message to add, either as a pre-constructed ChatMessageContent instance

or a dictionary specifying 'role' and 'content'.

`**encoding**`

Required*

<xref:Optional>[str]

The encoding of the message. Required if 'message' is a dict.

Default value: None

`**metadata**`

Required*

<xref:Optional>[dict[str,<xref: Any>]]

Any metadata to attach to the message. Required if 'message' is a dict.

Default value: None

**add_system_message**

Add a system message to the chat history.

Python

`add_message(message: ChatMessageContent | dict[str, Any], encoding: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``) -> ``None`

ﾉ

**Expand table**

`add_system_message(content: str | list[KernelContent], **kwargs) -> ``None`



**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the system message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_tool_message**

Add a tool message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the tool message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_user_message**

Add a user message to the chat history.

ﾉ

**Expand table**

`add_tool_message(content: str | list[KernelContent], **kwargs: Any) -> `

`None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the user message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**clear**

Clear the chat history.

Python

**extend**

Extend the chat history with a list of messages.

Python

**Parameters**

`add_user_message(content: str | list[KernelContent], **kwargs: Any) -> `

`None`

ﾉ

**Expand table**

`clear() -> ``None`

`extend(messages: Iterable[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**messages**`

Required*

The messages to add to the history. Can be a list of ChatMessageContent

instances or a ChatHistory itself.

**from_rendered_prompt**

Create a ChatHistory instance from a rendered prompt.

Python

**Parameters**

**Name**

**Description**

`**rendered_prompt**`

Required*

str

The rendered prompt to convert to a ChatHistory instance.

**Returns**

**Type**

**Description**

ChatHistory

The ChatHistory instance created from the rendered prompt.

**load_chat_history_from_file**

Loads the ChatHistory from a file.

Python

**Parameters**

`from_rendered_prompt(rendered_prompt: str) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`load_chat_history_from_file(file_path: str) -> ChatHistory`



**Name**

**Description**

`**file_path**`

Required*

str

The path to the file from which to load the ChatHistory.

**Returns**

**Type**

**Description**

ChatHistory

The deserialized ChatHistory instance.

**remove_message**

Remove a message from the history.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

<xref:semantic_kernel.contents.chat_history.ChatMessageContent>

The message to remove.

**Returns**

**Type**

**Description**

bool

True if the message was removed, False if the message was not found.

**replace**

ﾉ

**Expand table**

ﾉ

**Expand table**

`remove_message(message: ChatMessageContent) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**



Replace the chat history with a list of messages.

This calls clear() and then extend(messages=messages).

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

The messages to add to the history. Can be a list of ChatMessageContent

instances or a ChatHistory itself.

**restore_chat_history**

Restores a ChatHistory instance from a JSON string.

Python

**Parameters**

**Name**

**Description**

`**chat_history_json**`

Required*

str

The JSON string to deserialize into a ChatHistory instance.

**Returns**

**Type**

**Description**

ChatHistory

The deserialized ChatHistory instance.

`replace(messages: Iterable[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`restore_chat_history(chat_history_json: str) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

ValueError

If the JSON string is invalid or the deserialized data fails validation.

**serialize**

Serializes the ChatHistory instance to a JSON string.

Python

**Returns**

**Type**

**Description**

str

A JSON string representation of the ChatHistory instance.

**Exceptions**

**Type**

**Description**

ValueError

If the ChatHistory instance cannot be serialized to JSON.

**store_chat_history_to_file**

Stores the serialized ChatHistory to a file.

Python

**Parameters**

ﾉ

**Expand table**

`serialize() -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`store_chat_history_to_file(file_path: str) -> ``None`



**Name**

**Description**

`**file_path**`

Required*

str

The path to the file where the serialized data will be stored.

**to_prompt**

Return a string representation of the history.

Python

**messages**

Python

**system_message**

Python

ﾉ

**Expand table**

`to_prompt() -> str`

**Attributes**

`messages: list[ChatMessageContent]`

`system_message: str | ``None`



**chat_message_content Module**

Reference

**Classes**

ChatMessageContent

This is the class for chat message response content.

All Chat Completion Services should return an instance of this class as

response. Or they can implement their own subclass of this class and

return an instance.

Create a ChatMessageContent instance.

ﾉ

**Expand table**



**ChatMessageContent Class**

Reference

This is the class for chat message response content.

All Chat Completion Services should return an instance of this class as
response. Or they

can implement their own subclass of this class and return an instance.

Create a ChatMessageContent instance.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**inner_content**`

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

Default value: None

`**ai_model_id**`

Optional[str] - The id of the AI model that generated this response.

Default value: None

`**metadata**`

Dict[str, Any] - Any metadata that should be attached to the response.

Default value: None

`**role**`

Required*

ChatRole - The role of the chat message.

`**content**`

Optional[str] - The text of the response.

Default value: None

`ChatMessageContent(role: AuthorRole, items: list[AnnotationContent | `

`BinaryContent | ImageContent | TextContent | FunctionResultContent | `

`FunctionCallContent | FileReferenceContent | StreamingAnnotationContent | `

`StreamingFileReferenceContent] | ``None`` = ``None``, content: str | ``None`` = ``None``, `

`inner_content: Any | ``None`` = ``None``, name: str | ``None`` = ``None``, encoding: str | `

`None`` = ``None``, finish_reason: FinishReason | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, *, content_type: `

`Literal[ContentTypes.CHAT_MESSAGE_CONTENT] = ``'message'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**encoding**`

Optional[str] - The encoding of the text.

Default value: None

`**role**`

Required*

AuthorRole - The role of the chat message.

`**items**`

list[TextContent, StreamingTextContent, FunctionCallContent,

FunctionResultContent, ImageContent]

The content.

Default value: None

`**content**`

Required*

str - The text of the response.

`**inner_content**`

Required*

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

`**name**`

Optional[str] - The name of the response.

Default value: None

`**encoding**`

Required*

Optional[str] - The encoding of the text.

`**finish_reason**`

Optional[FinishReason] - The reason the response was finished.

Default value: None

`**ai_model_id**`

Required*

Optional[str] - The id of the AI model that generated this response.

`**metadata**`

Required*

Dict[str, Any] - Any metadata that should be attached to the response.

`****kwargs**`

Required*

Any - Any additional fields to set on the instance.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: message

ﾉ

**Expand table**



from_element

Create a new instance of ChatMessageContent from an XML element.

to_dict

Serialize the ChatMessageContent to a dictionary.

to_element

Convert the ChatMessageContent to an XML Element.

to_prompt

Convert the ChatMessageContent to a prompt.

**from_element**

Create a new instance of ChatMessageContent from an XML element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

Element - The XML Element to create the ChatMessageContent from.

**Returns**

**Type**

**Description**

ChatMessageContent - The new instance of ChatMessageContent or a subclass.

**to_dict**

Serialize the ChatMessageContent to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> ChatMessageContent`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**role_key**`

Default value: role

`**content_key**`

Default value: content

**Returns**

**Type**

**Description**

dict - The dictionary representing the ChatMessageContent.

**to_element**

Convert the ChatMessageContent to an XML Element.

Python

**Parameters**

**Name**

**Description**

`**root_key**`

Required*

str - The key to use for the root of the XML Element.

**Returns**

`to_dict(role_key: str = ``'role'``, content_key: str = ``'content'``) -> `

`dict[str, Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`to_element() -> Element`

ﾉ

**Expand table**



**Type**

**Description**

Element - The XML Element representing the ChatMessageContent.

**to_prompt**

Convert the ChatMessageContent to a prompt.

Python

**Returns**

**Type**

**Description**

str - The prompt from the ChatMessageContent.

**content**

Get the content of the response, will find the first TextContent's text.

**ai_model_id**

Python

**content_type**

Python

ﾉ

**Expand table**

`to_prompt() -> str`

ﾉ

**Expand table**

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.CHAT_MESSAGE_CONTENT]`



**encoding**

Python

**finish_reason**

Python

**inner_content**

Python

**items**

Python

**metadata**

Python

**name**

Python

`encoding: str | ``None`

`finish_reason: FinishReason | ``None`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`items: list[Annotated[AnnotationContent | BinaryContent | ImageContent | `

`TextContent | FunctionResultContent | FunctionCallContent | `

`FileReferenceContent | StreamingAnnotationContent | `

`StreamingFileReferenceContent, FieldInfo(annotation=NoneType, `

`required=``True``, discriminator=``'content_type'``)]]`

`metadata: dict[str, Any]`

`name: str | ``None`



**role**

Python

**tag**

Python

`role: AuthorRole`

`tag: ClassVar[str] = ``'message'`



**const Module**

Reference

**Enums**

ContentTypes

Content types enumeration.

ﾉ

**Expand table**



**ContentTypes Enum**

Reference

Content types enumeration.

ANNOTATION_CONTENT

AUDIO_CONTENT

BINARY_CONTENT

CHAT_MESSAGE_CONTENT

FILE_REFERENCE_CONTENT

FUNCTION_CALL_CONTENT

FUNCTION_RESULT_CONTENT

IMAGE_CONTENT

STREAMING_ANNOTATION_CONTENT

STREAMING_FILE_REFERENCE_CONTENT

TEXT_CONTENT

**Fields**

ﾉ

**Expand table**



**file_reference_content Module**

Reference

**Classes**

FileReferenceContent

File reference content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FileReferenceContent Class**

Reference

File reference content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: file_reference

`**file_id**`

Required*

`**tools**`

`FileReferenceContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.FILE_REFERENCE_CONTENT] = ``'file_reference'``, file_id:
`

`str | ``None`` = ``None``, tools: list[Any] = ``None``, data_source: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**data_source**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the file reference content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**to_element**

Convert the file reference content to an Element.

Python

**ai_model_id**

Python

**content_type**

Python

**data_source**

Python

**file_id**

Python

**inner_content**

Python

`to_element() -> Element`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.FILE_REFERENCE_CONTENT]`

`data_source: Any | ``None`

`file_id: str | ``None`



**is_experimental**

Python

**metadata**

Python

**stage_status**

Python

**tag**

Python

**tools**

Python

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`

`metadata: dict[str, Any]`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'file_reference'`

`tools: list[Any]`



**function_call_content Module**

Reference

**Classes**

FunctionCallContent

Class to hold a function call response.

Create function call content.

ﾉ

**Expand table**



**FunctionCallContent Class**

Reference

Class to hold a function call response.

Create function call content.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content_type**`

Required*

The content type.

`**inner_content**`

<xref:<xref:semantic_kernel.contents.function_call_content.Any | None>>

The inner content.

Default value: None

`**ai_model_id**`

<xref:<xref:semantic_kernel.contents.function_call_content.str | None>>

The id of the AI model.

Default value: None

`**id**`

<xref:<xref:semantic_kernel.contents.function_call_content.str | None>>

The id of the function call.

Default value: None

`**index**`

<xref:<xref:semantic_kernel.contents.function_call_content.int | None>>

The index of the function call.

Default value: None

`**name**`

<xref:<xref:semantic_kernel.contents.function_call_content.str | None>>

`FunctionCallContent(inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, id: str | ``None`` = ``None``, index: int | ``None`` = ``None``, name: str | `

`None`` = ``None``, function_name: str | ``None`` = ``None``, plugin_name: str | ``None`` = `

`None``, arguments: str | Mapping[str, Any] | ``None`` = ``None``, metadata: dict[str, `

`Any] | ``None`` = ``None``, *, content_type: `

`Literal[ContentTypes.FUNCTION_CALL_CONTENT] = ``'function_call'``)`

ﾉ

**Expand table**



**Name**

**Description**

The name of the function call. When not supplied function_name and

plugin_name should be supplied.

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.contents.function_call_content.str | None>>

The function name. Not used when 'name' is supplied.

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.contents.function_call_content.str | None>>

The plugin name. Not used when 'name' is supplied.

Default value: None

`**arguments**`

<xref:str | dict>[str,<xref: Any>]<xref: | None>

The arguments of the function call.

Default value: None

`**metadata**`

dict[str,<xref: Any>]<xref: | None>

The metadata of the function call.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: function_call

combine_arguments

Combine two arguments.

custom_fully_qualified_name

Get the fully qualified name of the function with a custom

separator.

from_element

Create an instance from an Element.

parse_arguments

Parse the arguments into a dictionary.

split_name

Split the name into a plugin and function name.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



split_name_dict

Split the name into a plugin and function name.

to_dict

Convert the instance to a dictionary.

to_element

Convert the function call to an Element.

to_kernel_arguments

Return the arguments as a KernelArguments instance.

**combine_arguments**

Combine two arguments.

Python

**Parameters**

**Name**

**Description**

`**arg1**`

Required*

`**arg2**`

Required*

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

`combine_arguments(arg1: str | Mapping[str, Any] | ``None``, arg2: str | `

`Mapping[str, Any] | ``None``) -> str | Mapping[str, Any]`

ﾉ

**Expand table**

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**parse_arguments**

Parse the arguments into a dictionary.

Python

**split_name**

Split the name into a plugin and function name.

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`parse_arguments() -> Mapping[str, Any] | ``None`



Python

**split_name_dict**

Split the name into a plugin and function name.

Python

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the function call to an Element.

Python

**to_kernel_arguments**

Return the arguments as a KernelArguments instance.

Python

**ai_model_id**

`split_name() -> list[str | ``None``]`

`split_name_dict() -> dict`

`to_dict() -> dict[str, str | Any]`

`to_element() -> Element`

`to_kernel_arguments() -> KernelArguments`

**Attributes**



Python

**arguments**

Python

**content_type**

Python

**function_name**

Python

**id**

Python

**index**

Python

**inner_content**

Python

`ai_model_id: str | ``None`

`arguments: str | Mapping[str, Any] | ``None`

`content_type: Literal[ContentTypes.FUNCTION_CALL_CONTENT]`

`function_name: str`

`id: str | ``None`

`index: int | ``None`



**metadata**

Python

**name**

Python

**plugin_name**

Python

**tag**

Python

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`metadata: dict[str, Any]`

`name: str | ``None`

`plugin_name: str | ``None`

`tag: ClassVar[str] = ``'function_call'`



**function_result_content Module**

Reference

**Classes**

FunctionResultContent

This class represents function result content.

Create function result content.

ﾉ

**Expand table**



**FunctionResultContent Class**

Reference

This class represents function result content.

Create function result content.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content_type**`

Required*

The content type.

`**inner_content**`

<xref:<xref:semantic_kernel.contents.function_result_content.Any | None>>

The inner content.

Default value: None

`**ai_model_id**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

The id of the AI model.

Default value: None

`**id**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

The id of the function call that the result relates to.

Default value: None

`**name**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

The name of the function. When not supplied function_name and plugin_name

should be supplied.

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

`FunctionResultContent(inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, id: str | ``None`` = ``None``, name: str | ``None`` = ``None``, function_name: `

`str | ``None`` = ``None``, plugin_name: str | ``None`` = ``None``, result: Any | ``None`` = `

`None``, encoding: str | ``None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, `

`*, content_type: Literal[ContentTypes.FUNCTION_RESULT_CONTENT] = `

`'function_result'``)`

ﾉ

**Expand table**



**Name**

**Description**

The function name. Not used when 'name' is supplied.

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

The plugin name. Not used when 'name' is supplied.

Default value: None

`**result**`

<xref:<xref:semantic_kernel.contents.function_result_content.Any | None>>

The result of the function.

Default value: None

`**encoding**`

<xref:<xref:semantic_kernel.contents.function_result_content.str | None>>

The encoding of the result.

Default value: None

`**metadata**`

dict[str,<xref: Any>]<xref: | None>

The metadata of the function call.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: function_result

custom_fully_qualified_name

Get the fully qualified name of the function with a

custom separator.

from_element

Create an instance from an Element.

from_function_call_content_and_result

Create an instance from a FunctionCallContent and a

result.

serialize_result

Serialize the result.

split_name

Split the name into a plugin and function name.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



to_chat_message_content

Convert the instance to a ChatMessageContent.

to_dict

Convert the instance to a dictionary.

to_element

Convert the instance to an Element.

to_streaming_chat_message_content

Convert the instance to a

StreamingChatMessageContent.

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**from_element**

Create an instance from an Element.

Python

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`



**Parameters**

**Name**

**Description**

`**element**`

Required*

**from_function_call_content_and_result**

Create an instance from a FunctionCallContent and a result.

Python

**Parameters**

**Name**

**Description**

`**function_call_content**`

Required*

`**result**`

Required*

`**metadata**`

Required*

Default value: {}

**serialize_result**

Serialize the result.

Python

**Parameters**

ﾉ

**Expand table**

`from_function_call_content_and_result(function_call_content: `

`FunctionCallContent, result: FunctionResult | TextContent | `

`ChatMessageContent | Any, metadata: dict[str, Any] = {}) -> _T`

ﾉ

**Expand table**

`serialize_result(value: Any) -> str`



**Name**

**Description**

`**value**`

Required*

**split_name**

Split the name into a plugin and function name.

Python

**to_chat_message_content**

Convert the instance to a ChatMessageContent.

Python

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**to_streaming_chat_message_content**

ﾉ

**Expand table**

`split_name() -> list[str]`

`to_chat_message_content() -> ChatMessageContent`

`to_dict() -> dict[str, str]`

`to_element() -> Element`



Convert the instance to a StreamingChatMessageContent.

Python

**ai_model_id**

Python

**content_type**

Python

**encoding**

Python

**function_name**

Python

**id**

Python

`to_streaming_chat_message_content() -> StreamingChatMessageContent`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.FUNCTION_RESULT_CONTENT]`

`encoding: str | ``None`

`function_name: str`

`id: str`



**inner_content**

Python

**metadata**

Python

**name**

Python

**plugin_name**

Python

**result**

Python

**tag**

Python

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`metadata: dict[str, Any]`

`name: str | ``None`

`plugin_name: str | ``None`

`result: Any`

`tag: ClassVar[str] = ``'function_result'`



**image_content Module**

Reference

**Classes**

ImageContent

Image Content class.

This can be created either the bytes data or a data uri, additionally it can
have a

uri. The uri is a reference to the source, and might or might not point to the
same

thing as the data.

Use the .from_image_file method to create an instance from a image file. This

reads the file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None):

The data uri of the content. data (str | bytes | None): The data of the content.

data_format (str | None): The format of the data (e.g. base64). mime_type (str |

None): The mime type of the image, only used with data. kwargs (Any): Any

additional arguments:

Methods: from_image_path: Create an instance from an image file. **str** :
Returns

the string representation of the image.

Raises: ValidationError: If neither uri or data is provided.

Note: This class is marked as 'experimental' and may change in the future.

Create an Image Content object, either from a data_uri or data.

ﾉ

**Expand table**

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the `

`response so even`

` when not creating a subclass a developer can `

`leverage the full thing.`

` ai_model_id (str | None): The id of the AI model that `

`generated this response.`

` metadata (dict[str, Any]): Any metadata that should be `

`attached to the response.`



**ImageContent Class**

Reference

Image Content class.

This can be created either the bytes data or a data uri, additionally it can
have a uri. The

uri is a reference to the source, and might or might not point to the same
thing as the

data.

Use the .from_image_file method to create an instance from a image file. This
reads the

file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None): The

data uri of the content. data (str | bytes | None): The data of the content. data_format

(str | None): The format of the data (e.g. base64). mime_type (str | None): The mime type

of the image, only used with data. kwargs (Any): Any additional arguments:

Methods: from_image_path: Create an instance from an image file. **str** :
Returns the string

representation of the image.

Raises: ValidationError: If neither uri or data is provided.

Note: This class is marked as 'experimental' and may change in the future.

Create an Image Content object, either from a data_uri or data.

**Constructor**

Python

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the response so even`

` when not creating a subclass a developer can leverage the full `

`thing.`

` ai_model_id (str | None): The id of the AI model that generated this `

`response.`

` metadata (dict[str, Any]): Any metadata that should be attached to the `

`response.`



**Parameters**

**Name**

**Description**

`**uri**`

The reference uri of the content.

Default value: None

`**data_uri**`

The data uri of the content.

Default value: None

`**data**`

The data of the content.

Default value: None

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**mime_type**`

The mime type of the image, only used with data.

Default value: None

`**kwargs**`

Required*

Any additional arguments:

`**inner_content**`

Required*

The inner content of the response, this should hold all the information from

the response so even when not creating a subclass a developer can leverage

the full thing.

`**ai_model_id**`

Required*

The id of the AI model that generated this response.

`**metadata**`

Required*

Any metadata that should be attached to the response.

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

`ImageContent(uri: str | ``None`` = ``None``, data_uri: str | ``None`` = ``None``, data: str `

`| bytes | ndarray | ``None`` = ``None``, data_format: str | ``None`` = ``None``, mime_type: `

`str | ``None`` = ``None``, *, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.IMAGE_CONTENT] = ``'image'``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: image

from_image_file

Create an instance from an image file.

from_image_path

Create an instance from an image file.

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

to_dict

Convert the instance to a dictionary.

**from_image_file**

Create an instance from an image file.

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

**from_image_path**

**Methods**

ﾉ

**Expand table**

`from_image_file(path: str) -> _T`

ﾉ

**Expand table**



Create an instance from an image file.

Python

**Parameters**

**Name**

**Description**

`**image_path**`

Required*

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

`from_image_path(image_path: str) -> _T`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**ai_model_id**

Python

**content_type**

Python

**inner_content**

Python

**is_experimental**

Python

**metadata**

Python

**stage_status**

Python

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.IMAGE_CONTENT]`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`

`metadata: dict[str, Any]`

`stage_status = ``'experimental'`



**tag**

Python

**uri**

Python

`tag: ClassVar[str] = ``'image'`

`uri: Url | str | ``None`



**kernel_content Module**

Reference

**Classes**

KernelContent

Base class for all kernel contents.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelContent Class**

Reference

Base class for all kernel contents.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

`KernelContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | ``None`` = `

`None``, metadata: dict[str, Any] = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



to_element

Convert the instance to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**ai_model_id**

`abstract classmethod from_element(element: Any) -> _T`

ﾉ

**Expand table**

`abstract to_dict() -> dict[str, Any]`

`abstract to_element() -> Any`

**Attributes**



Python

**inner_content**

Python

**metadata**

Python

`ai_model_id: str | ``None`

`inner_content: Annotated[Any | ``None``, FieldInfo(annotation=NoneType, `

`required=``True``, exclude=``True``)]`

`metadata: dict[str, Any]`



**streaming_annotation_content Module**

Reference

**Classes**

StreamingAnnotationContent

Streaming Annotation content.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**StreamingAnnotationContent Class**

Reference

Streaming Annotation content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: streaming_annotation

`**file_id**`

Required*

`**quote**`

`StreamingAnnotationContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: `

`str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.STREAMING_ANNOTATION_CONTENT] =
``'streaming_annotation'``, `

`file_id: str | ``None`` = ``None``, quote: str | ``None`` = ``None``, start_index: int | `

`None`` = ``None``, end_index: int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**start_index**`

Required*

`**end_index**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the annotation content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**to_element**

Convert the annotation content to an Element.

Python

**ai_model_id**

Python

**content_type**

Python

**end_index**

Python

**file_id**

Python

`to_dict() -> dict[str, Any]`

`to_element() -> Element`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.STREAMING_ANNOTATION_CONTENT]`

`end_index: int | ``None`

`file_id: str | ``None`



**inner_content**

Python

**is_experimental**

Python

**metadata**

Python

**quote**

Python

**stage_status**

Python

**start_index**

Python

**tag**

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`

`metadata: dict[str, Any]`

`quote: str | ``None`

`stage_status = ``'experimental'`

`start_index: int | ``None`



Python

`tag: ClassVar[str] = ``'streaming_annotation'`



**streaming_chat_message_content**

**Module**

Reference

**Classes**

StreamingChatMessageContent

This is the class for streaming chat message response content.

All Chat Completion Services should return an instance of this

class as streaming response, where each part of the response as

it is streamed is converted to an instance of this class, the end-

user will have to either do something directly or gather them

and combine them into a new instance. A service can

implement their own subclass of this class and return instances

of that.

Create a new instance of StreamingChatMessageContent.

ﾉ

**Expand table**



**StreamingChatMessageContent Class**

Reference

This is the class for streaming chat message response content.

All Chat Completion Services should return an instance of this class as
streaming

response, where each part of the response as it is streamed is converted to an
instance

of this class, the end-user will have to either do something directly or
gather them and

combine them into a new instance. A service can implement their own subclass
of this

class and return instances of that.

Create a new instance of StreamingChatMessageContent.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**choice_index**`

Required*

int - The index of the choice that generated this response.

`**inner_content**`

Optional[Any] - The inner content of the response, this should

hold all the information from the response so even when not

creating a subclass a developer can leverage the full thing.

Default value: None

`**ai_model_id**`

Optional[str] - The id of the AI model that generated this

response.

Default value: None

`StreamingChatMessageContent(role: AuthorRole, choice_index: int, items: `

`list[BinaryContent | ImageContent | StreamingTextContent | `

`FunctionCallContent | FunctionResultContent | StreamingFileReferenceContent `

`| StreamingAnnotationContent] | ``None`` = ``None``, content: str | ``None`` = ``None``, `

`inner_content: Any | ``None`` = ``None``, name: str | ``None`` = ``None``, encoding: str | `

`None`` = ``None``, finish_reason: FinishReason | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, `

`function_invoke_attempt: int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**metadata**`

Dict[str, Any] - Any metadata that should be attached to the

response.

Default value: None

`**role**`

Required*

Optional[ChatRole] - The role of the chat message, defaults to

ASSISTANT.

`**content**`

Optional[str] - The text of the response.

Default value: None

`**encoding**`

Optional[str] - The encoding of the text.

Default value: None

`**role**`

Required*

The role of the chat message.

`**choice_index**`

Required*

The index of the choice that generated this response.

`**items**`

The content.

Default value: None

`**content**`

Required*

The text of the response.

`**inner_content**`

Required*

The inner content of the response, this should hold all the

information from the response so even when not creating a

subclass a developer can leverage the full thing.

`**name**`

The name of the response.

Default value: None

`**encoding**`

Required*

The encoding of the text.

`**finish_reason**`

The reason the response was finished.

Default value: None

`**metadata**`

Required*

Any metadata that should be attached to the response.

`**ai_model_id**`

Required*

The id of the AI model that generated this response.

`**function_invoke_attempt**`

Tracks the current attempt count for automatically invoking

functions. This value increments with each subsequent automatic

invocation attempt.

Default value: None



to_element

Convert the StreamingChatMessageContent to an XML Element.

**to_element**

Convert the StreamingChatMessageContent to an XML Element.

Python

**Parameters**

**Name**

**Description**

`**root_key**`

Required*

str - The key to use for the root of the XML Element.

**Returns**

**Type**

**Description**

Element - The XML Element representing the StreamingChatMessageContent.

**ai_model_id**

Python

**Methods**

ﾉ

**Expand table**

`to_element() -> Element`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`ai_model_id: str | ``None`



**choice_index**

Python

**content_type**

Python

**encoding**

Python

**finish_reason**

Python

**function_invoke_attempt**

Python

**inner_content**

Python

**items**

`choice_index: int`

`content_type: Literal[ContentTypes.CHAT_MESSAGE_CONTENT]`

`encoding: str | ``None`

`finish_reason: FinishReason | ``None`

`function_invoke_attempt: int | ``None`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`



Python

**metadata**

Python

**name**

Python

**role**

Python

`items: list[Annotated[ITEM_TYPES, `

`Field(discriminator=DISCRIMINATOR_FIELD)]]`

`metadata: dict[str, Any]`

`name: str | ``None`

`role: AuthorRole`



**streaming_content_mixin Module**

Reference

**Classes**

StreamingContentMixin

Mixin class for all streaming kernel contents.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**StreamingContentMixin Class**

Reference

Mixin class for all streaming kernel contents.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**choice_index**`

Required*

**choice_index**

Python

`StreamingContentMixin(*, choice_index: int)`

ﾉ

**Expand table**

**Attributes**

`choice_index: int`



**streaming_file_reference_content**

**Module**

Reference

**Classes**

StreamingFileReferenceContent

Streaming File reference content.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**StreamingFileReferenceContent Class**

Reference

Streaming File reference content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: streaming_file_reference

`**file_id**`

Required*

`**tools**`

`StreamingFileReferenceContent(*, inner_content: Any | ``None`` = ``None``, `

`ai_model_id: str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``, `

`content_type: Literal[ContentTypes.STREAMING_FILE_REFERENCE_CONTENT] = `

`'streaming_file_reference'``, file_id: str | ``None`` = ``None``, tools: list[Any] = `

`None``, data_source: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**data_source**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the file reference content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**to_element**

Convert the file reference content to an Element.

Python

**ai_model_id**

Python

**content_type**

Python

**data_source**

Python

**file_id**

Python

**inner_content**

Python

`to_element() -> Element`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.STREAMING_FILE_REFERENCE_CONTENT]`

`data_source: Any | ``None`

`file_id: str | ``None`



**is_experimental**

Python

**metadata**

Python

**stage_status**

Python

**tag**

Python

**tools**

Python

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`is_experimental = ``True`

`metadata: dict[str, Any]`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'streaming_file_reference'`

`tools: list[Any]`



**streaming_text_content Module**

Reference

**Classes**

StreamingTextContent

This represents streaming text response content.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**StreamingTextContent Class**

Reference

This represents streaming text response content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**choice_index**`

Required*

int - The index of the choice that generated this response.

`**inner_content**`

Required*

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

`**ai_model_id**`

Required*

Optional[str] - The id of the AI model that generated this response.

`**metadata**`

Required*

Dict[str, Any] - Any metadata that should be attached to the response.

`**text**`

Required*

Optional[str] - The text of the response.

`**encoding**`

Optional[str] - The encoding of the text.

`StreamingTextContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.TEXT_CONTENT] = ``'text'``, text: str, encoding: str | ``None`` `

`= ``None``, choice_index: int)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: text

`**text**`

Required*

`**encoding**`

Required*

`**choice_index**`

Required*

**ai_model_id**

Python

**choice_index**

Python

ﾉ

**Expand table**

**Attributes**

`ai_model_id: str | ``None`

`choice_index: int`



**content_type**

Python

**encoding**

Python

**inner_content**

Python

**metadata**

Python

**text**

Python

`content_type: Literal[ContentTypes.TEXT_CONTENT]`

`encoding: str | ``None`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`metadata: dict[str, Any]`

`text: str`



**text_content Module**

Reference

**Classes**

TextContent

This represents text response content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextContent Class**

Reference

This represents text response content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**inner_content**`

Required*

Any - The inner content of the response, this should hold all the information

from the response so even when not creating a subclass a developer can

leverage the full thing.

`**ai_model_id**`

Required*

str | None - The id of the AI model that generated this response.

`**metadata**`

Required*

dict[str, Any] - Any metadata that should be attached to the response.

`**text**`

Required*

str | None - The text of the response.

`**encoding**`

Required*

str | None - The encoding of the text.

`TextContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | ``None`` = `

`None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.TEXT_CONTENT] = ``'text'``, text: str, encoding: str | ``None`` `

`= ``None``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: text

`**text**`

Required*

`**encoding**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the instance to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**ai_model_id**

Python

**content_type**

Python

**encoding**

Python

`to_dict() -> dict[str, str]`

`to_element() -> Element`

**Attributes**

`ai_model_id: str | ``None`

`content_type: Literal[ContentTypes.TEXT_CONTENT]`



**inner_content**

Python

**metadata**

Python

**tag**

Python

**text**

Python

`encoding: str | ``None`

`inner_content: Annotated[Any | ``None``, Field(exclude=``True``)]`

`metadata: dict[str, Any]`

`tag: ClassVar[str] = ``'text'`

`text: str`



**AnnotationContent Class**

Reference

Annotation content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: annotation

`**file_id**`

Required*

`**quote**`

`AnnotationContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.ANNOTATION_CONTENT] = ``'annotation'``, file_id: str | ``None`` `

`= ``None``, quote: str | ``None`` = ``None``, start_index: int | ``None`` = ``None``, end_index: `

`int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**start_index**`

Required*

`**end_index**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the annotation content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**to_element**

Convert the annotation content to an Element.

Python

**content_type**

Python

**end_index**

Python

**file_id**

Python

**is_experimental**

Python

`to_dict() -> dict[str, Any]`

`to_element() -> Element`

**Attributes**

`content_type: Literal[ContentTypes.ANNOTATION_CONTENT]`

`end_index: int | ``None`

`file_id: str | ``None`

`is_experimental = ``True`



**quote**

Python

**stage_status**

Python

**start_index**

Python

**tag**

Python

`quote: str | ``None`

`stage_status = ``'experimental'`

`start_index: int | ``None`

`tag: ClassVar[str] = ``'annotation'`



**AudioContent Class**

Reference

Audio Content class.

This can be created either the bytes data or a data uri, additionally it can
have a uri. The

uri is a reference to the source, and might or might not point to the same
thing as the

data.

Use the .from_audio_file method to create an instance from an audio file. This
reads the

file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None): The

data uri of the content. data (str | bytes | None): The data of the content. data_format

(str | None): The format of the data (e.g. base64). mime_type (str | None): The mime type

of the audio, only used with data. kwargs (Any): Any additional arguments:

Note: This class is marked as 'experimental' and may change in the future.

Create an Audio Content object, either from a data_uri or data.

**Constructor**

Python

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the response so even`

` when not creating a subclass a developer can leverage the full `

`thing.`

` ai_model_id (str | None): The id of the AI model that generated this `

`response.`

` metadata (dict[str, Any]): Any metadata that should be attached to the `

`response.`

`AudioContent(uri: str | ``None`` = ``None``, data_uri: str | ``None`` = ``None``, data: str `

`| bytes | ndarray | ``None`` = ``None``, data_format: str | ``None`` = ``None``, mime_type: `

`str | ``None`` = ``None``, *, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.AUDIO_CONTENT] = ``'audio'``)`



**Parameters**

**Name**

**Description**

`**uri**`

The reference uri of the content.

Default value: None

`**data_uri**`

The data uri of the content.

Default value: None

`**data**`

The data of the content.

Default value: None

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**mime_type**`

The mime type of the audio, only used with data.

Default value: None

`**kwargs**`

Required*

Any additional arguments: inner_content: The inner content of the response,

this should hold all the information from the response so even when not
creating

a subclass a developer can leverage the full thing.

ai_model_id: The id of the AI model that generated this response. metadata:
Any

metadata that should be attached to the response.

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: audio

ﾉ

**Expand table**

ﾉ

**Expand table**

**Methods**



from_audio_file

Create an instance from an audio file.

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

to_dict

Convert the instance to a dictionary.

**from_audio_file**

Create an instance from an audio file.

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

ﾉ

**Expand table**

`from_audio_file(path: str) -> AudioContent`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**content_type**

Python

**is_experimental**

Python

**stage_status**

Python

**tag**

Python

`to_dict() -> dict[str, Any]`

**Attributes**

`content_type: Literal[ContentTypes.AUDIO_CONTENT]`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'audio'`



**AuthorRole Enum**

Reference

Author role enum.

ASSISTANT

DEVELOPER

SYSTEM

TOOL

USER

**Fields**

ﾉ

**Expand table**



**ChatHistory Class**

Reference

This class holds the history of chat messages from a chat conversation.

Note: the system_message is added to the messages as a ChatMessageContent
instance

with role=AuthorRole.SYSTEM, but updating it will not update the messages
list.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

The messages to add to the chat history.

`**system_message**`

Required*

A system message to add to the chat history, optional. if passed, it is added

to the messages as a ChatMessageContent instance with

role=AuthorRole.SYSTEM before any other messages.

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

`ChatHistory(*, messages: list[ChatMessageContent] = ``None``, system_message:
`

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**system_message**`

Required*

add_assistant_message

Add an assistant message to the chat history.

add_developer_message

Add a system message to the chat history.

add_message

Add a message to the history.

This method accepts either a ChatMessageContent instance or a

dictionary with the necessary information to construct a

ChatMessageContent instance.

add_system_message

Add a system message to the chat history.

add_tool_message

Add a tool message to the chat history.

add_user_message

Add a user message to the chat history.

clear

Clear the chat history.

extend

Extend the chat history with a list of messages.

from_rendered_prompt

Create a ChatHistory instance from a rendered prompt.

load_chat_history_from_file

Loads the ChatHistory from a file.

remove_message

Remove a message from the history.

replace

Replace the chat history with a list of messages.

This calls clear() and then extend(messages=messages).

restore_chat_history

Restores a ChatHistory instance from a JSON string.

serialize

Serializes the ChatHistory instance to a JSON string.

store_chat_history_to_file

Stores the serialized ChatHistory to a file.

to_prompt

Return a string representation of the history.

**add_assistant_message**

**Methods**

ﾉ

**Expand table**



Add an assistant message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the assistant message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_developer_message**

Add a system message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the developer message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`add_assistant_message(content: str | list[KernelContent], **kwargs: Any) `

`-> ``None`

ﾉ

**Expand table**

`add_developer_message(content: str | list[KernelContent], **kwargs) -> `

`None`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

Additional keyword arguments.

**add_message**

Add a message to the history.

This method accepts either a ChatMessageContent instance or a dictionary with
the

necessary information to construct a ChatMessageContent instance.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

<xref:Union>[<xref:ChatMessageContent>,dict]

The message to add, either as a pre-constructed ChatMessageContent instance

or a dictionary specifying 'role' and 'content'.

`**encoding**`

Required*

<xref:Optional>[str]

The encoding of the message. Required if 'message' is a dict.

Default value: None

`**metadata**`

Required*

<xref:Optional>[dict[str,<xref: Any>]]

Any metadata to attach to the message. Required if 'message' is a dict.

Default value: None

**add_system_message**

Add a system message to the chat history.

Python

`add_message(message: ChatMessageContent | dict[str, Any], encoding: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``) -> ``None`

ﾉ

**Expand table**

`add_system_message(content: str | list[KernelContent], **kwargs) -> ``None`



**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the system message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_tool_message**

Add a tool message to the chat history.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the tool message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**add_user_message**

Add a user message to the chat history.

ﾉ

**Expand table**

`add_tool_message(content: str | list[KernelContent], **kwargs: Any) -> `

`None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the user message, can be a string or a

`**ChatMessageContent.**`

Required*

list of

**ChatMessageContent.** (list* of *<xref:KernelContent instances that

are turned into a single>)

`****kwargs**`

Required*

Additional keyword arguments.

**clear**

Clear the chat history.

Python

**extend**

Extend the chat history with a list of messages.

Python

**Parameters**

`add_user_message(content: str | list[KernelContent], **kwargs: Any) -> `

`None`

ﾉ

**Expand table**

`clear() -> ``None`

`extend(messages: Iterable[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**messages**`

Required*

The messages to add to the history. Can be a list of ChatMessageContent

instances or a ChatHistory itself.

**from_rendered_prompt**

Create a ChatHistory instance from a rendered prompt.

Python

**Parameters**

**Name**

**Description**

`**rendered_prompt**`

Required*

str

The rendered prompt to convert to a ChatHistory instance.

**Returns**

**Type**

**Description**

ChatHistory

The ChatHistory instance created from the rendered prompt.

**load_chat_history_from_file**

Loads the ChatHistory from a file.

Python

**Parameters**

`from_rendered_prompt(rendered_prompt: str) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`load_chat_history_from_file(file_path: str) -> ChatHistory`



**Name**

**Description**

`**file_path**`

Required*

str

The path to the file from which to load the ChatHistory.

**Returns**

**Type**

**Description**

ChatHistory

The deserialized ChatHistory instance.

**remove_message**

Remove a message from the history.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

ChatMessageContent

The message to remove.

**Returns**

**Type**

**Description**

bool

True if the message was removed, False if the message was not found.

**replace**

ﾉ

**Expand table**

ﾉ

**Expand table**

`remove_message(message: ChatMessageContent) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**



Replace the chat history with a list of messages.

This calls clear() and then extend(messages=messages).

Python

**Parameters**

**Name**

**Description**

`**messages**`

Required*

The messages to add to the history. Can be a list of ChatMessageContent

instances or a ChatHistory itself.

**restore_chat_history**

Restores a ChatHistory instance from a JSON string.

Python

**Parameters**

**Name**

**Description**

`**chat_history_json**`

Required*

str

The JSON string to deserialize into a ChatHistory instance.

**Returns**

**Type**

**Description**

ChatHistory

The deserialized ChatHistory instance.

`replace(messages: Iterable[ChatMessageContent]) -> ``None`

ﾉ

**Expand table**

`restore_chat_history(chat_history_json: str) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

ValueError

If the JSON string is invalid or the deserialized data fails validation.

**serialize**

Serializes the ChatHistory instance to a JSON string.

Python

**Returns**

**Type**

**Description**

str

A JSON string representation of the ChatHistory instance.

**Exceptions**

**Type**

**Description**

ValueError

If the ChatHistory instance cannot be serialized to JSON.

**store_chat_history_to_file**

Stores the serialized ChatHistory to a file.

Python

**Parameters**

ﾉ

**Expand table**

`serialize() -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`store_chat_history_to_file(file_path: str) -> ``None`



**Name**

**Description**

`**file_path**`

Required*

str

The path to the file where the serialized data will be stored.

**to_prompt**

Return a string representation of the history.

Python

**messages**

Python

**system_message**

Python

ﾉ

**Expand table**

`to_prompt() -> str`

**Attributes**

`messages: list[ChatMessageContent]`

`system_message: str | ``None`



**ChatHistoryReducer Class**

Reference

Defines a contract for reducing chat history.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

`ChatHistoryReducer(*, messages: list[ChatMessageContent] = ``None``, `

`system_message: str | ``None`` = ``None``, target_count: Annotated[int, Gt(gt=0)], `

`threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool =
``False``)`

ﾉ

**Expand table**



add_message_async

Add a message to the chat history.

If auto_reduce is enabled, the history will be reduced after adding the

message.

reduce

Reduce the chat history in some way (e.g., truncate, summarize).

**add_message_async**

Add a message to the chat history.

If auto_reduce is enabled, the history will be reduced after adding the
message.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

`**encoding**`

Required*

Default value: None

`**metadata**`

Required*

Default value: None

**reduce**

Reduce the chat history in some way (e.g., truncate, summarize).

Python

**Methods**

ﾉ

**Expand table**

`async`` add_message_async(message: ChatMessageContent | dict[str, Any], `

`encoding: str | ``None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``) -> `

`None`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

A possibly shorter list of messages, or None if no change is needed.

**auto_reduce**

Python

**is_experimental**

Python

**stage_status**

Python

**target_count**

Python

**threshold_count**

`abstract ``async`` reduce() -> Self | ``None`

ﾉ

**Expand table**

**Attributes**

`auto_reduce: bool`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`target_count: int`



Python

`threshold_count: int`



**ChatHistorySummarizationReducer**

**Class**

Reference

A ChatHistory with logic to summarize older messages past a target count.

This class inherits from ChatHistoryReducer, which in turn inherits from
ChatHistory. It

can be used anywhere a ChatHistory is expected, while adding summarization
capability.

Args: target_count: The target message count. threshold_count: The threshold
count to

avoid orphaning messages. auto_reduce: Whether to automatically reduce the
chat

history, default is False. service: The ChatCompletion service to use for
summarization.

summarization_instructions: The summarization instructions, optional.

use_single_summary: Whether to use a single summary message, default is True.

fail_on_error: Raise error if summarization fails, default is True.

include_function_content_in_summary: Whether to include function calls/results
in the

summary, default is False. execution_settings: The execution settings for the

summarization prompt, optional.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

`ChatHistorySummarizationReducer(*, messages: list[ChatMessageContent] = `

`None``, system_message: str | ``None`` = ``None``, target_count: Annotated[int, `

`Gt(gt=0)], threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool `

`= ``False``, service: ChatCompletionClientBase, summarization_instructions:
str `

`= ``'\nProvide a concise and complete summarization of the entire dialog that
`

`does not exceed 5 sentences.\n\nThis summary must always:\n- Consider both `

`user and assistant interactions\n- Maintain continuity for the purpose of `

`further dialog\n- Include details from any existing summary\n- Focus on the `

`most significant aspects of the dialog\n\nThis summary must never:\n- `

`Critique, correct, interpret, presume, or assume\n- Identify faults, `

`mistakes, misunderstanding, or correctness\n- Analyze what has not `

`occurred\n- Exclude details from any existing summary\n'``, `



**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

`**service**`

Required*

`**summarization_instructions**`

Default value: Provide a concise and complete

summarization of the entire dialog that does not

exceed 5 sentences. This summary must always: -

Consider both user and assistant interactions -

Maintain continuity for the purpose of further

dialog - Include details from any existing summary

\- Focus on the most significant aspects of the

dialog This summary must never: - Critique, correct,

interpret, presume, or assume - Identify faults,

mistakes, misunderstanding, or correctness -

Analyze what has not occurred - Exclude details

from any existing summary

`**use_single_summary**`

Default value: True

`**fail_on_error**`

Default value: True

`**include_function_content_in_summary**`

Required*

`**execution_settings**`

`use_single_summary: bool = ``True``, fail_on_error: bool = ``True``, `

`include_function_content_in_summary: bool = ``False``, execution_settings: `

`PromptExecutionSettings | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

reduce

**reduce**

Python

**execution_settings**

Python

**fail_on_error**

Python

**include_function_content_in_summary**

Python

**is_experimental**

**Methods**

ﾉ

**Expand table**

`async`` reduce() -> Self | ``None`

**Attributes**

`execution_settings: PromptExecutionSettings | ``None`

`fail_on_error: bool`

`include_function_content_in_summary: bool`



Python

**service**

Python

**stage_status**

Python

**summarization_instructions**

Python

**use_single_summary**

Python

`is_experimental = ``True`

`service: ChatCompletionClientBase`

`stage_status = ``'experimental'`

`summarization_instructions: str`

`use_single_summary: bool`



**ChatHistoryTruncationReducer Class**

Reference

A ChatHistory that supports truncation logic.

Because this class inherits from ChatHistoryReducer (which in turn inherits
from

ChatHistory), it can also be used anywhere a ChatHistory is expected, while
adding

truncation capability.

Args: target_count: The target message count. threshold_count: The threshold
count to

avoid orphaning messages. auto_reduce: Whether to automatically reduce the
chat

history, default is False.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**messages**`

Required*

`**system_message**`

Required*

`**target_count**`

`ChatHistoryTruncationReducer(*, messages: list[ChatMessageContent] =
``None``, `

`system_message: str | ``None`` = ``None``, target_count: Annotated[int, Gt(gt=0)], `

`threshold_count: Annotated[int, Ge(ge=0)] = 0, auto_reduce: bool =
``False``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**threshold_count**`

Required*

`**auto_reduce**`

Required*

reduce

**reduce**

Python

**auto_reduce**

Python

**is_experimental**

Python

**messages**

Python

**Methods**

ﾉ

**Expand table**

`async`` reduce() -> Self | ``None`

**Attributes**

`auto_reduce: bool`

`is_experimental = ``True`



**stage_status**

Python

**system_message**

Python

**target_count**

Python

**threshold_count**

Python

`messages: list[ChatMessageContent]`

`stage_status = ``'experimental'`

`system_message: str | ``None`

`target_count: int`

`threshold_count: int`



**ChatMessageContent Class**

Reference

This is the class for chat message response content.

All Chat Completion Services should return an instance of this class as
response. Or they

can implement their own subclass of this class and return an instance.

Create a ChatMessageContent instance.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**inner_content**`

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

Default value: None

`**ai_model_id**`

Optional[str] - The id of the AI model that generated this response.

Default value: None

`**metadata**`

Dict[str, Any] - Any metadata that should be attached to the response.

Default value: None

`**role**`

Required*

ChatRole - The role of the chat message.

`**content**`

Optional[str] - The text of the response.

Default value: None

`ChatMessageContent(role: AuthorRole, items: list[AnnotationContent | `

`BinaryContent | ImageContent | TextContent | FunctionResultContent | `

`FunctionCallContent | FileReferenceContent | StreamingAnnotationContent | `

`StreamingFileReferenceContent] | ``None`` = ``None``, content: str | ``None`` = ``None``, `

`inner_content: Any | ``None`` = ``None``, name: str | ``None`` = ``None``, encoding: str | `

`None`` = ``None``, finish_reason: FinishReason | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, *, content_type: `

`Literal[ContentTypes.CHAT_MESSAGE_CONTENT] = ``'message'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**encoding**`

Optional[str] - The encoding of the text.

Default value: None

`**role**`

Required*

AuthorRole - The role of the chat message.

`**items**`

list[TextContent, StreamingTextContent, FunctionCallContent,

FunctionResultContent, ImageContent]

The content.

Default value: None

`**content**`

Required*

str - The text of the response.

`**inner_content**`

Required*

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

`**name**`

Optional[str] - The name of the response.

Default value: None

`**encoding**`

Required*

Optional[str] - The encoding of the text.

`**finish_reason**`

Optional[FinishReason] - The reason the response was finished.

Default value: None

`**ai_model_id**`

Required*

Optional[str] - The id of the AI model that generated this response.

`**metadata**`

Required*

Dict[str, Any] - Any metadata that should be attached to the response.

`****kwargs**`

Required*

Any - Any additional fields to set on the instance.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: message

ﾉ

**Expand table**



from_element

Create a new instance of ChatMessageContent from an XML element.

to_dict

Serialize the ChatMessageContent to a dictionary.

to_element

Convert the ChatMessageContent to an XML Element.

to_prompt

Convert the ChatMessageContent to a prompt.

**from_element**

Create a new instance of ChatMessageContent from an XML element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

Element - The XML Element to create the ChatMessageContent from.

**Returns**

**Type**

**Description**

ChatMessageContent - The new instance of ChatMessageContent or a subclass.

**to_dict**

Serialize the ChatMessageContent to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> ChatMessageContent`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**role_key**`

Default value: role

`**content_key**`

Default value: content

**Returns**

**Type**

**Description**

dict - The dictionary representing the ChatMessageContent.

**to_element**

Convert the ChatMessageContent to an XML Element.

Python

**Parameters**

**Name**

**Description**

`**root_key**`

Required*

str - The key to use for the root of the XML Element.

**Returns**

`to_dict(role_key: str = ``'role'``, content_key: str = ``'content'``) -> `

`dict[str, Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`to_element() -> Element`

ﾉ

**Expand table**



**Type**

**Description**

Element - The XML Element representing the ChatMessageContent.

**to_prompt**

Convert the ChatMessageContent to a prompt.

Python

**Returns**

**Type**

**Description**

str - The prompt from the ChatMessageContent.

**content**

Get the content of the response, will find the first TextContent's text.

**content_type**

Python

**encoding**

Python

ﾉ

**Expand table**

`to_prompt() -> str`

ﾉ

**Expand table**

**Attributes**

`content_type: Literal[ContentTypes.CHAT_MESSAGE_CONTENT]`

`encoding: str | ``None`



**finish_reason**

Python

**items**

Python

**name**

Python

**role**

Python

**tag**

Python

`finish_reason: FinishReason | ``None`

`items: list[Annotated[AnnotationContent | BinaryContent | ImageContent | `

`TextContent | FunctionResultContent | FunctionCallContent | `

`FileReferenceContent | StreamingAnnotationContent | `

`StreamingFileReferenceContent, FieldInfo(annotation=NoneType, `

`required=``True``, discriminator=``'content_type'``)]]`

`name: str | ``None`

`role: AuthorRole`

`tag: ClassVar[str] = ``'message'`



**FileReferenceContent Class**

Reference

File reference content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: file_reference

`**file_id**`

Required*

`**tools**`

`FileReferenceContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.FILE_REFERENCE_CONTENT] = ``'file_reference'``, file_id:
`

`str | ``None`` = ``None``, tools: list[Any] = ``None``, data_source: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**data_source**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the file reference content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**to_element**

Convert the file reference content to an Element.

Python

**content_type**

Python

**data_source**

Python

**file_id**

Python

**is_experimental**

Python

**stage_status**

Python

`to_element() -> Element`

**Attributes**

`content_type: Literal[ContentTypes.FILE_REFERENCE_CONTENT]`

`data_source: Any | ``None`

`file_id: str | ``None`

`is_experimental = ``True`



**tag**

Python

**tools**

Python

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'file_reference'`

`tools: list[Any]`



**FinishReason Enum**

Reference

Finish Reason enum.

CONTENT_FILTER

FUNCTION_CALL

LENGTH

STOP

TOOL_CALLS

**Fields**

ﾉ

**Expand table**



**FunctionCallContent Class**

Reference

Class to hold a function call response.

Create function call content.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content_type**`

Required*

The content type.

`**inner_content**`

<xref:<xref:semantic_kernel.contents.Any | None>>

The inner content.

Default value: None

`**ai_model_id**`

<xref:<xref:semantic_kernel.contents.str | None>>

The id of the AI model.

Default value: None

`**id**`

<xref:<xref:semantic_kernel.contents.str | None>>

The id of the function call.

Default value: None

`**index**`

<xref:<xref:semantic_kernel.contents.int | None>>

The index of the function call.

Default value: None

`**name**`

<xref:<xref:semantic_kernel.contents.str | None>>

`FunctionCallContent(inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, id: str | ``None`` = ``None``, index: int | ``None`` = ``None``, name: str | `

`None`` = ``None``, function_name: str | ``None`` = ``None``, plugin_name: str | ``None`` = `

`None``, arguments: str | Mapping[str, Any] | ``None`` = ``None``, metadata: dict[str, `

`Any] | ``None`` = ``None``, *, content_type: `

`Literal[ContentTypes.FUNCTION_CALL_CONTENT] = ``'function_call'``)`

ﾉ

**Expand table**



**Name**

**Description**

The name of the function call. When not supplied function_name and

plugin_name should be supplied.

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.contents.str | None>>

The function name. Not used when 'name' is supplied.

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.contents.str | None>>

The plugin name. Not used when 'name' is supplied.

Default value: None

`**arguments**`

<xref:str | dict>[str,<xref: Any>]<xref: | None>

The arguments of the function call.

Default value: None

`**metadata**`

dict[str,<xref: Any>]<xref: | None>

The metadata of the function call.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: function_call

combine_arguments

Combine two arguments.

custom_fully_qualified_name

Get the fully qualified name of the function with a custom

separator.

from_element

Create an instance from an Element.

parse_arguments

Parse the arguments into a dictionary.

split_name

Split the name into a plugin and function name.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



split_name_dict

Split the name into a plugin and function name.

to_dict

Convert the instance to a dictionary.

to_element

Convert the function call to an Element.

to_kernel_arguments

Return the arguments as a KernelArguments instance.

**combine_arguments**

Combine two arguments.

Python

**Parameters**

**Name**

**Description**

`**arg1**`

Required*

`**arg2**`

Required*

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

`combine_arguments(arg1: str | Mapping[str, Any] | ``None``, arg2: str | `

`Mapping[str, Any] | ``None``) -> str | Mapping[str, Any]`

ﾉ

**Expand table**

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**parse_arguments**

Parse the arguments into a dictionary.

Python

**split_name**

Split the name into a plugin and function name.

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`parse_arguments() -> Mapping[str, Any] | ``None`



Python

**split_name_dict**

Split the name into a plugin and function name.

Python

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the function call to an Element.

Python

**to_kernel_arguments**

Return the arguments as a KernelArguments instance.

Python

**arguments**

`split_name() -> list[str | ``None``]`

`split_name_dict() -> dict`

`to_dict() -> dict[str, str | Any]`

`to_element() -> Element`

`to_kernel_arguments() -> KernelArguments`

**Attributes**



Python

**content_type**

Python

**function_name**

Python

**id**

Python

**index**

Python

**name**

Python

**plugin_name**

Python

`arguments: str | Mapping[str, Any] | ``None`

`content_type: Literal[ContentTypes.FUNCTION_CALL_CONTENT]`

`function_name: str`

`id: str | ``None`

`index: int | ``None`

`name: str | ``None`



**tag**

Python

`plugin_name: str | ``None`

`tag: ClassVar[str] = ``'function_call'`



**FunctionResultContent Class**

Reference

This class represents function result content.

Create function result content.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content_type**`

Required*

The content type.

`**inner_content**`

<xref:<xref:semantic_kernel.contents.Any | None>>

The inner content.

Default value: None

`**ai_model_id**`

<xref:<xref:semantic_kernel.contents.str | None>>

The id of the AI model.

Default value: None

`**id**`

<xref:<xref:semantic_kernel.contents.str | None>>

The id of the function call that the result relates to.

Default value: None

`**name**`

<xref:<xref:semantic_kernel.contents.str | None>>

The name of the function. When not supplied function_name and plugin_name

should be supplied.

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.contents.str | None>>

`FunctionResultContent(inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, id: str | ``None`` = ``None``, name: str | ``None`` = ``None``, function_name: `

`str | ``None`` = ``None``, plugin_name: str | ``None`` = ``None``, result: Any | ``None`` = `

`None``, encoding: str | ``None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, `

`*, content_type: Literal[ContentTypes.FUNCTION_RESULT_CONTENT] = `

`'function_result'``)`

ﾉ

**Expand table**



**Name**

**Description**

The function name. Not used when 'name' is supplied.

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.contents.str | None>>

The plugin name. Not used when 'name' is supplied.

Default value: None

`**result**`

<xref:<xref:semantic_kernel.contents.Any | None>>

The result of the function.

Default value: None

`**encoding**`

<xref:<xref:semantic_kernel.contents.str | None>>

The encoding of the result.

Default value: None

`**metadata**`

dict[str,<xref: Any>]<xref: | None>

The metadata of the function call.

Default value: None

`**kwargs**`

Required*

Any

Additional arguments.

**Keyword-Only Parameters**

**Name**

**Description**

`**content_type**`

Default value: function_result

custom_fully_qualified_name

Get the fully qualified name of the function with a

custom separator.

from_element

Create an instance from an Element.

from_function_call_content_and_result

Create an instance from a FunctionCallContent and a

result.

serialize_result

Serialize the result.

split_name

Split the name into a plugin and function name.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



to_chat_message_content

Convert the instance to a ChatMessageContent.

to_dict

Convert the instance to a dictionary.

to_element

Convert the instance to an Element.

to_streaming_chat_message_content

Convert the instance to a

StreamingChatMessageContent.

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**from_element**

Create an instance from an Element.

Python

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`



**Parameters**

**Name**

**Description**

`**element**`

Required*

**from_function_call_content_and_result**

Create an instance from a FunctionCallContent and a result.

Python

**Parameters**

**Name**

**Description**

`**function_call_content**`

Required*

`**result**`

Required*

`**metadata**`

Required*

Default value: {}

**serialize_result**

Serialize the result.

Python

**Parameters**

ﾉ

**Expand table**

`from_function_call_content_and_result(function_call_content: `

`FunctionCallContent, result: FunctionResult | TextContent | `

`ChatMessageContent | Any, metadata: dict[str, Any] = {}) -> _T`

ﾉ

**Expand table**

`serialize_result(value: Any) -> str`



**Name**

**Description**

`**value**`

Required*

**split_name**

Split the name into a plugin and function name.

Python

**to_chat_message_content**

Convert the instance to a ChatMessageContent.

Python

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**to_streaming_chat_message_content**

ﾉ

**Expand table**

`split_name() -> list[str]`

`to_chat_message_content() -> ChatMessageContent`

`to_dict() -> dict[str, str]`

`to_element() -> Element`



Convert the instance to a StreamingChatMessageContent.

Python

**content_type**

Python

**encoding**

Python

**function_name**

Python

**id**

Python

**name**

Python

`to_streaming_chat_message_content() -> StreamingChatMessageContent`

**Attributes**

`content_type: Literal[ContentTypes.FUNCTION_RESULT_CONTENT]`

`encoding: str | ``None`

`function_name: str`

`id: str`

`name: str | ``None`



**plugin_name**

Python

**result**

Python

**tag**

Python

`plugin_name: str | ``None`

`result: Any`

`tag: ClassVar[str] = ``'function_result'`



**ImageContent Class**

Reference

Image Content class.

This can be created either the bytes data or a data uri, additionally it can
have a uri. The

uri is a reference to the source, and might or might not point to the same
thing as the

data.

Use the .from_image_file method to create an instance from a image file. This
reads the

file and guesses the mime_type.

If both data_uri and data is provided, data will be used and a warning is
logged.

Args: uri (Url | None): The reference uri of the content. data_uri (DataUrl | None): The

data uri of the content. data (str | bytes | None): The data of the content. data_format

(str | None): The format of the data (e.g. base64). mime_type (str | None): The mime type

of the image, only used with data. kwargs (Any): Any additional arguments:

Methods: from_image_path: Create an instance from an image file. **str** :
Returns the string

representation of the image.

Raises: ValidationError: If neither uri or data is provided.

Note: This class is marked as 'experimental' and may change in the future.

Create an Image Content object, either from a data_uri or data.

**Constructor**

Python

` inner_content (Any): The inner content of the response,`

` this should hold all the information from the response so even`

` when not creating a subclass a developer can leverage the full `

`thing.`

` ai_model_id (str | None): The id of the AI model that generated this `

`response.`

` metadata (dict[str, Any]): Any metadata that should be attached to the `

`response.`



**Parameters**

**Name**

**Description**

`**uri**`

The reference uri of the content.

Default value: None

`**data_uri**`

The data uri of the content.

Default value: None

`**data**`

The data of the content.

Default value: None

`**data_format**`

The format of the data (e.g. base64).

Default value: None

`**mime_type**`

The mime type of the image, only used with data.

Default value: None

`**kwargs**`

Required*

Any additional arguments:

`**inner_content**`

Required*

The inner content of the response, this should hold all the information from

the response so even when not creating a subclass a developer can leverage

the full thing.

`**ai_model_id**`

Required*

The id of the AI model that generated this response.

`**metadata**`

Required*

Any metadata that should be attached to the response.

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

`ImageContent(uri: str | ``None`` = ``None``, data_uri: str | ``None`` = ``None``, data: str `

`| bytes | ndarray | ``None`` = ``None``, data_format: str | ``None`` = ``None``, mime_type: `

`str | ``None`` = ``None``, *, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.IMAGE_CONTENT] = ``'image'``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: image

from_image_file

Create an instance from an image file.

from_image_path

Create an instance from an image file.

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

to_dict

Convert the instance to a dictionary.

**from_image_file**

Create an instance from an image file.

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

**from_image_path**

**Methods**

ﾉ

**Expand table**

`from_image_file(path: str) -> _T`

ﾉ

**Expand table**



Create an instance from an image file.

Python

**Parameters**

**Name**

**Description**

`**image_path**`

Required*

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

`from_image_path(image_path: str) -> _T`

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**content_type**

Python

**is_experimental**

Python

**stage_status**

Python

**tag**

Python

**Attributes**

`content_type: Literal[ContentTypes.IMAGE_CONTENT]`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'image'`



**StreamingAnnotationContent Class**

Reference

Streaming Annotation content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: streaming_annotation

`**file_id**`

Required*

`**quote**`

`StreamingAnnotationContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: `

`str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.STREAMING_ANNOTATION_CONTENT] =
``'streaming_annotation'``, `

`file_id: str | ``None`` = ``None``, quote: str | ``None`` = ``None``, start_index: int | `

`None`` = ``None``, end_index: int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**start_index**`

Required*

`**end_index**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the annotation content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**to_element**

Convert the annotation content to an Element.

Python

**content_type**

Python

**end_index**

Python

**file_id**

Python

**is_experimental**

Python

`to_dict() -> dict[str, Any]`

`to_element() -> Element`

**Attributes**

`content_type: Literal[ContentTypes.STREAMING_ANNOTATION_CONTENT]`

`end_index: int | ``None`

`file_id: str | ``None`

`is_experimental = ``True`



**quote**

Python

**stage_status**

Python

**start_index**

Python

**tag**

Python

`quote: str | ``None`

`stage_status = ``'experimental'`

`start_index: int | ``None`

`tag: ClassVar[str] = ``'streaming_annotation'`



**StreamingChatMessageContent Class**

Reference

This is the class for streaming chat message response content.

All Chat Completion Services should return an instance of this class as
streaming

response, where each part of the response as it is streamed is converted to an
instance

of this class, the end-user will have to either do something directly or
gather them and

combine them into a new instance. A service can implement their own subclass
of this

class and return instances of that.

Create a new instance of StreamingChatMessageContent.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**choice_index**`

Required*

int - The index of the choice that generated this response.

`**inner_content**`

Optional[Any] - The inner content of the response, this should

hold all the information from the response so even when not

creating a subclass a developer can leverage the full thing.

Default value: None

`**ai_model_id**`

Optional[str] - The id of the AI model that generated this

response.

Default value: None

`StreamingChatMessageContent(role: AuthorRole, choice_index: int, items: `

`list[BinaryContent | ImageContent | StreamingTextContent | `

`FunctionCallContent | FunctionResultContent | StreamingFileReferenceContent `

`| StreamingAnnotationContent] | ``None`` = ``None``, content: str | ``None`` = ``None``, `

`inner_content: Any | ``None`` = ``None``, name: str | ``None`` = ``None``, encoding: str | `

`None`` = ``None``, finish_reason: FinishReason | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] | ``None`` = ``None``, `

`function_invoke_attempt: int | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**metadata**`

Dict[str, Any] - Any metadata that should be attached to the

response.

Default value: None

`**role**`

Required*

Optional[ChatRole] - The role of the chat message, defaults to

ASSISTANT.

`**content**`

Optional[str] - The text of the response.

Default value: None

`**encoding**`

Optional[str] - The encoding of the text.

Default value: None

`**role**`

Required*

The role of the chat message.

`**choice_index**`

Required*

The index of the choice that generated this response.

`**items**`

The content.

Default value: None

`**content**`

Required*

The text of the response.

`**inner_content**`

Required*

The inner content of the response, this should hold all the

information from the response so even when not creating a

subclass a developer can leverage the full thing.

`**name**`

The name of the response.

Default value: None

`**encoding**`

Required*

The encoding of the text.

`**finish_reason**`

The reason the response was finished.

Default value: None

`**metadata**`

Required*

Any metadata that should be attached to the response.

`**ai_model_id**`

Required*

The id of the AI model that generated this response.

`**function_invoke_attempt**`

Tracks the current attempt count for automatically invoking

functions. This value increments with each subsequent automatic

invocation attempt.

Default value: None



to_element

Convert the StreamingChatMessageContent to an XML Element.

**to_element**

Convert the StreamingChatMessageContent to an XML Element.

Python

**Parameters**

**Name**

**Description**

`**root_key**`

Required*

str - The key to use for the root of the XML Element.

**Returns**

**Type**

**Description**

Element - The XML Element representing the StreamingChatMessageContent.

**function_invoke_attempt**

Python

**Methods**

ﾉ

**Expand table**

`to_element() -> Element`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`function_invoke_attempt: int | ``None`



**StreamingFileReferenceContent Class**

Reference

Streaming File reference content.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: streaming_file_reference

`**file_id**`

Required*

`**tools**`

`StreamingFileReferenceContent(*, inner_content: Any | ``None`` = ``None``, `

`ai_model_id: str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``, `

`content_type: Literal[ContentTypes.STREAMING_FILE_REFERENCE_CONTENT] = `

`'streaming_file_reference'``, file_id: str | ``None`` = ``None``, tools: list[Any] = `

`None``, data_source: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**data_source**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the file reference content to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**

`to_dict() -> dict[str, Any]`



**to_element**

Convert the file reference content to an Element.

Python

**content_type**

Python

**data_source**

Python

**file_id**

Python

**is_experimental**

Python

**stage_status**

Python

`to_element() -> Element`

**Attributes**

`content_type: Literal[ContentTypes.STREAMING_FILE_REFERENCE_CONTENT]`

`data_source: Any | ``None`

`file_id: str | ``None`

`is_experimental = ``True`



**tag**

Python

**tools**

Python

`stage_status = ``'experimental'`

`tag: ClassVar[str] = ``'streaming_file_reference'`

`tools: list[Any]`



**StreamingTextContent Class**

Reference

This represents streaming text response content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**choice_index**`

Required*

int - The index of the choice that generated this response.

`**inner_content**`

Required*

Optional[Any] - The inner content of the response, this should hold all the

information from the response so even when not creating a subclass a

developer can leverage the full thing.

`**ai_model_id**`

Required*

Optional[str] - The id of the AI model that generated this response.

`**metadata**`

Required*

Dict[str, Any] - Any metadata that should be attached to the response.

`**text**`

Required*

Optional[str] - The text of the response.

`**encoding**`

Optional[str] - The encoding of the text.

`StreamingTextContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | `

`None`` = ``None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.TEXT_CONTENT] = ``'text'``, text: str, encoding: str | ``None`` `

`= ``None``, choice_index: int)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: text

`**text**`

Required*

`**encoding**`

Required*

`**choice_index**`

Required*

**choice_index**

Python

ﾉ

**Expand table**

**Attributes**

`choice_index: int`



**TextContent Class**

Reference

This represents text response content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**inner_content**`

Required*

Any - The inner content of the response, this should hold all the information

from the response so even when not creating a subclass a developer can

leverage the full thing.

`**ai_model_id**`

Required*

str | None - The id of the AI model that generated this response.

`**metadata**`

Required*

dict[str, Any] - Any metadata that should be attached to the response.

`**text**`

Required*

str | None - The text of the response.

`**encoding**`

Required*

str | None - The encoding of the text.

`TextContent(*, inner_content: Any | ``None`` = ``None``, ai_model_id: str | ``None`` = `

`None``, metadata: dict[str, Any] = ``None``, content_type: `

`Literal[ContentTypes.TEXT_CONTENT] = ``'text'``, text: str, encoding: str | ``None`` `

`= ``None``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**inner_content**`

Required*

`**ai_model_id**`

Required*

`**metadata**`

Required*

`**content_type**`

Default value: text

`**text**`

Required*

`**encoding**`

Required*

from_element

Create an instance from an Element.

to_dict

Convert the instance to a dictionary.

to_element

Convert the instance to an Element.

**from_element**

Create an instance from an Element.

Python

**Parameters**

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`from_element(element: Element) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**element**`

Required*

**to_dict**

Convert the instance to a dictionary.

Python

**to_element**

Convert the instance to an Element.

Python

**content_type**

Python

**encoding**

Python

**tag**

Python

`to_dict() -> dict[str, str]`

`to_element() -> Element`

**Attributes**

`content_type: Literal[ContentTypes.TEXT_CONTENT]`

`encoding: str | ``None`



**text**

Python

`tag: ClassVar[str] = ``'text'`

`text: str`



**core_plugins Package**

Reference

**Packages**

crew_ai

sessions_python_tool

**Modules**

conversation_summary_plugin

http_plugin

math_plugin

text_memory_plugin

text_plugin

time_plugin

wait_plugin

web_search_engine_plugin

**Classes**

ConversationSummaryPlugin

Semantic plugin that enables conversations summarization.

Initializes a new instance of the ConversationSummaryPlugin.

The template for this plugin is built-in, and will overwrite any

template passed in the prompt_template_config.

HttpPlugin

A plugin that provides HTTP functionality.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Usage: kernel.add_plugin(HttpPlugin(), "http")

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

MathPlugin

Description: MathPlugin provides a set of functions to make Math

calculations.

Usage: kernel.add_plugin(MathPlugin(), plugin_name="math")

SessionsPythonTool

A plugin for running Python code in an Azure Container Apps

dynamic sessions code interpreter.

Initializes a new instance of the SessionsPythonTool class.

TextMemoryPlugin

A plugin to interact with a Semantic Text Memory.

Initialize a new instance of the TextMemoryPlugin.

TextPlugin

TextPlugin provides a set of functions to manipulate strings.

Usage: kernel.add_plugin(TextPlugin(), plugin_name="text")

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TimePlugin

TimePlugin provides a set of functions to get the current time and

date.

Usage: kernel.add_plugin(TimePlugin(), plugin_name="time")

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

WebSearchEnginePlugin

A plugin that provides web search engine functionality.

Usage: connector = BingConnector(bing_search_api_key)

kernel.add_plugin(WebSearchEnginePlugin(connector),

plugin_name="WebSearch")



Initializes a new instance of the WebSearchEnginePlugin class.



**crew_ai Package**

Reference

**Modules**

crew_ai_enterprise

crew_ai_enterprise_client

crew_ai_models

crew_ai_settings

**Classes**

CrewAIEnterprise

Class to interface with Crew.AI Crews from Semantic Kernel.

This object can be used directly or as a plugin in the Kernel.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize a new instance of the class. This object can be used directly or

as a plugin in the Kernel.

CrewAISettings

The Crew.AI settings.

Required:

endpoint: str - The API endpoint.

CrewAIStatusResponse

Represents the status response from Crew AI.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**



**crew_ai_enterprise Module**

Reference

**Classes**

CrewAIEnterprise

Class to interface with Crew.AI Crews from Semantic Kernel.

This object can be used directly or as a plugin in the Kernel.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of the class. This object can be used directly or as
a

plugin in the Kernel.

ﾉ

**Expand table**



**CrewAIEnterprise Class**

Reference

Class to interface with Crew.AI Crews from Semantic Kernel.

This object can be used directly or as a plugin in the Kernel.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of the class. This object can be used directly or as
a plugin in the

Kernel.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**endpoint**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None, optional>>

The API endpoint.

Default value: None

`**auth_token**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None, optional>>

The authentication token.

Default value: None

`**polling_interval**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.float,

optional>>

The polling interval in seconds. Defaults to 1.0.

Default value: 1.0

`**polling_timeout**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.float,

optional>>

`CrewAIEnterprise(endpoint: str | ``None`` = ``None``, auth_token: str | ``None`` = ``None``, `

`polling_interval: float | ``None`` = 1.0, polling_timeout: float | ``None`` = 30.0, `

`session: ClientSession | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The polling timeout in seconds. Defaults to 30.0.

Default value: 30.0

`**session**`

<xref:<xref:aiohttp.ClientSession | None, optional>>

The HTTP client session. Defaults to None.

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None>>

The encoding of the environment settings file. (Optional)

Default value: None

create_kernel_plugin

Creates a kernel plugin that can be used to invoke the CrewAI Crew.

get_crew_kickoff_status

Get the status of a Crew AI task.

kickoff

Kickoff a new Crew AI task.

wait_for_crew_completion

Wait for the completion of a Crew AI task.

**create_kernel_plugin**

Creates a kernel plugin that can be used to invoke the CrewAI Crew.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`create_kernel_plugin(name: str, description: str, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, task_webhook_url: str | ``None`` `

`= ``None``, step_webhook_url: str | ``None`` = ``None``, crew_webhook_url: str | ``None`` `

`= ``None``) -> KernelPlugin`



**Name**

**Description**

`**name**`

Required*

str

The name of the kernel plugin.

`**description**`

Required*

str

The description of the kernel plugin.

`**parameters**`

Required*

<xref:List>[<xref:KernelParameterMetadata>]<xref: | None>,<xref:

optional>

The definitions of the Crew's

Default value: None

`**None.**`

Required*

**None.** (<xref:required inputs. Defaults to>)

`**task_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The task level webhook URL. Defaults to None.

Default value: None

`**step_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The step level webhook URL. Defaults to None.

Default value: None

`**crew_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The crew level webhook URL. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

dict[str, Any]

A dictionary representing the kernel plugin.

**get_crew_kickoff_status**

Get the status of a Crew AI task.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_crew_kickoff_status(kickoff_id: str) -> CrewAIStatusResponse`



**Parameters**

**Name**

**Description**

`**kickoff_id**`

Required*

str

The ID of the kickoff response.

**Returns**

**Type**

**Description**

CrewAIStatusResponse

The status response of the task.

**kickoff**

Kickoff a new Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**inputs**`

dict[str,<xref: Any>],<xref: optional>

The inputs for the task. Defaults to None.

Default value: None

`**task_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None, optional>>

The webhook URL for task updates. Defaults to None.

Default value: None

`**step_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None, optional>>

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` kickoff(inputs: dict[str, Any] | ``None`` = ``None``, task_webhook_url: str `

`| ``None`` = ``None``, step_webhook_url: str | ``None`` = ``None``, crew_webhook_url: str `

`| ``None`` = ``None``) -> str`

ﾉ

**Expand table**



**Name**

**Description**

The webhook URL for step updates. Defaults to None.

Default value: None

`**crew_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.crew_ai_enterprise.str |

None, optional>>

The webhook URL for crew updates. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

str

The ID of the kickoff response.

**wait_for_crew_completion**

Wait for the completion of a Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**kickoff_id**`

Required*

str

The ID of the kickoff response.

**Returns**

**Type**

**Description**

str

The result of the task.

ﾉ

**Expand table**

`async`` wait_for_crew_completion(kickoff_id: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

FunctionExecutionException

If the task fails or an error occurs while waiting for completion.

**client**

Python

**is_experimental**

Python

**polling_interval**

Python

**polling_timeout**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`client: CrewAIEnterpriseClient`

`is_experimental = ``True`

`polling_interval: float`

`polling_timeout: float`



`stage_status = ``'experimental'`



**crew_ai_enterprise_client Module**

Reference

**Classes**

CrewAIEnterpriseClient

Client to interact with the Crew AI Enterprise API.

Initializes a new instance of the CrewAIEnterpriseClient class.

ﾉ

**Expand table**



**CrewAIEnterpriseClient Class**

Reference

Client to interact with the Crew AI Enterprise API.

Initializes a new instance of the CrewAIEnterpriseClient class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**endpoint**`

Required*

str

The API endpoint.

`**auth_token**`

Required*

str

The authentication token.

`**session**`

<xref:<xref:aiohttp.ClientSession | None, optional>>

The HTTP client session. Defaults to None.

Default value: None

get_inputs

Get the required inputs for Crew AI.

get_status

Get the status of a Crew AI task.

kickoff

Kickoff a new Crew AI task.

**get_inputs**

`CrewAIEnterpriseClient(endpoint: str, auth_token: str, session: `

`ClientSession | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Get the required inputs for Crew AI.

Python

**Returns**

**Type**

**Description**

CrewAIRequiredInputs

The required inputs for Crew AI.

**get_status**

Get the status of a Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**task_id**`

Required*

str

The ID of the task.

**Returns**

**Type**

**Description**

CrewAIStatusResponse

The status response of the task.

**kickoff**

`async`` get_inputs() -> CrewAIRequiredInputs`

ﾉ

**Expand table**

`async`` get_status(task_id: str) -> CrewAIStatusResponse`

ﾉ

**Expand table**

ﾉ

**Expand table**



Kickoff a new Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**inputs**`

<xref:Optional>[dict[str,<xref: Any>]],<xref: optional>

The inputs for the task. Defaults to None.

Default value: None

`**task_webhook_url**`

<xref:Optional>[str],<xref: optional>

The webhook URL for task updates. Defaults to None.

Default value: None

`**step_webhook_url**`

<xref:Optional>[str],<xref: optional>

The webhook URL for step updates. Defaults to None.

Default value: None

`**crew_webhook_url**`

<xref:Optional>[str],<xref: optional>

The webhook URL for crew updates. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

CrewAIKickoffResponse

The response from the kickoff request.

`async`` kickoff(inputs: dict[str, Any] | ``None`` = ``None``, task_webhook_url: str `

`| ``None`` = ``None``, step_webhook_url: str | ``None`` = ``None``, crew_webhook_url: str `

`| ``None`` = ``None``) -> CrewAIKickoffResponse`

ﾉ

**Expand table**

ﾉ

**Expand table**



**crew_ai_models Module**

Reference

**Classes**

CrewAIKickoffResponse

Represents the kickoff response from Crew AI.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

CrewAIRequiredInputs

Represents the required inputs for Crew AI.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

CrewAIStatusResponse

Represents the status response from Crew AI.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

CrewAIEnterpriseKickoffState

The Crew.AI Enterprise kickoff state.

ﾉ

**Expand table**

ﾉ

**Expand table**



**CrewAIEnterpriseKickoffState Enum**

Reference

The Crew.AI Enterprise kickoff state.

Failed

Failure

Not_Found

Pending

Running

Started

Success

**Fields**

ﾉ

**Expand table**



**CrewAIKickoffResponse Class**

Reference

Represents the kickoff response from Crew AI.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kickoff_id**`

Required*

**kickoff_id**

Python

`CrewAIKickoffResponse(*, kickoff_id: str)`

ﾉ

**Expand table**

**Attributes**

`kickoff_id: str`



**CrewAIRequiredInputs Class**

Reference

Represents the required inputs for Crew AI.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inputs**`

Required*

**inputs**

Python

`CrewAIRequiredInputs(*, inputs: dict[str, str])`

ﾉ

**Expand table**

**Attributes**

`inputs: dict[str, str]`



**CrewAIStatusResponse Class**

Reference

Represents the status response from Crew AI.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**state**`

Required*

`**result**`

Required*

`**last_step**`

Required*

**last_step**

Python

`CrewAIStatusResponse(*, state: CrewAIEnterpriseKickoffState, result: str | `

`None`` = ``None``, last_step: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**result**

Python

**state**

Python

`last_step: dict[str, Any] | ``None`

`result: str | ``None`

`state: CrewAIEnterpriseKickoffState`



**crew_ai_settings Module**

Reference

**Classes**

CrewAISettings

The Crew.AI settings.

Required:

endpoint: str - The API endpoint.

ﾉ

**Expand table**



**CrewAISettings Class**

Reference

The Crew.AI settings.

Required:

endpoint: str - The API endpoint.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`CrewAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`endpoint: str, auth_token: SecretStr, polling_interval: float = 1.0, `

`polling_timeout: float = 30.0)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**endpoint**`

Required*

`**auth_token**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**polling_interval**`

Default value: 1.0

`**polling_timeout**`

Default value: 30.0

**auth_token**

Python

**endpoint**

Python

**env_file_encoding**

Python

**env_file_path**

Python

**env_prefix**

Python

**Attributes**

`auth_token: SecretStr`

`endpoint: str`

`env_file_encoding: str`

`env_file_path: str | ``None`

`env_prefix: ClassVar[str] = ``'CREW_AI_'`



**polling_interval**

Python

**polling_timeout**

Python

`polling_interval: float`

`polling_timeout: float`



**CrewAIEnterprise Class**

Reference

Class to interface with Crew.AI Crews from Semantic Kernel.

This object can be used directly or as a plugin in the Kernel.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of the class. This object can be used directly or as
a plugin in the

Kernel.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**endpoint**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None, optional>>

The API endpoint.

Default value: None

`**auth_token**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None, optional>>

The authentication token.

Default value: None

`**polling_interval**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.float, optional>>

The polling interval in seconds. Defaults to 1.0.

Default value: 1.0

`**polling_timeout**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.float, optional>>

The polling timeout in seconds. Defaults to 30.0.

Default value: 30.0

`**session**`

<xref:<xref:aiohttp.ClientSession | None, optional>>

The HTTP client session. Defaults to None.

`CrewAIEnterprise(endpoint: str | ``None`` = ``None``, auth_token: str | ``None`` = ``None``, `

`polling_interval: float | ``None`` = 1.0, polling_timeout: float | ``None`` = 30.0, `

`session: ClientSession | ``None`` = ``None``, env_file_path: str | ``None`` = ``None``, `

`env_file_encoding: str | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

`**env_file_path**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None>>

Use the environment settings file as a fallback to environment variables.

(Optional)

Default value: None

`**env_file_encoding**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None>>

The encoding of the environment settings file. (Optional)

Default value: None

create_kernel_plugin

Creates a kernel plugin that can be used to invoke the CrewAI Crew.

get_crew_kickoff_status

Get the status of a Crew AI task.

kickoff

Kickoff a new Crew AI task.

wait_for_crew_completion

Wait for the completion of a Crew AI task.

**create_kernel_plugin**

Creates a kernel plugin that can be used to invoke the CrewAI Crew.

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

str

The name of the kernel plugin.

**Methods**

ﾉ

**Expand table**

`create_kernel_plugin(name: str, description: str, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, task_webhook_url: str | ``None`` `

`= ``None``, step_webhook_url: str | ``None`` = ``None``, crew_webhook_url: str | ``None`` `

`= ``None``) -> KernelPlugin`

ﾉ

**Expand table**



**Name**

**Description**

`**description**`

Required*

str

The description of the kernel plugin.

`**parameters**`

Required*

<xref:List>[<xref:KernelParameterMetadata>]<xref: | None>,<xref:

optional>

The definitions of the Crew's

Default value: None

`**None.**`

Required*

**None.** (<xref:required inputs. Defaults to>)

`**task_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The task level webhook URL. Defaults to None.

Default value: None

`**step_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The step level webhook URL. Defaults to None.

Default value: None

`**crew_webhook_url**`

Required*

<xref:Optional>[str],<xref: optional>

The crew level webhook URL. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

dict[str, Any]

A dictionary representing the kernel plugin.

**get_crew_kickoff_status**

Get the status of a Crew AI task.

Python

**Parameters**

ﾉ

**Expand table**

`async`` get_crew_kickoff_status(kickoff_id: str) -> CrewAIStatusResponse`

ﾉ

**Expand table**



**Name**

**Description**

`**kickoff_id**`

Required*

str

The ID of the kickoff response.

**Returns**

**Type**

**Description**

CrewAIStatusResponse

The status response of the task.

**kickoff**

Kickoff a new Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**inputs**`

dict[str,<xref: Any>],<xref: optional>

The inputs for the task. Defaults to None.

Default value: None

`**task_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None, optional>>

The webhook URL for task updates. Defaults to None.

Default value: None

`**step_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None, optional>>

The webhook URL for step updates. Defaults to None.

Default value: None

`**crew_webhook_url**`

<xref:<xref:semantic_kernel.core_plugins.crew_ai.str | None, optional>>

The webhook URL for crew updates. Defaults to None.

Default value: None

ﾉ

**Expand table**

`async`` kickoff(inputs: dict[str, Any] | ``None`` = ``None``, task_webhook_url: str `

`| ``None`` = ``None``, step_webhook_url: str | ``None`` = ``None``, crew_webhook_url: str `

`| ``None`` = ``None``) -> str`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

str

The ID of the kickoff response.

**wait_for_crew_completion**

Wait for the completion of a Crew AI task.

Python

**Parameters**

**Name**

**Description**

`**kickoff_id**`

Required*

str

The ID of the kickoff response.

**Returns**

**Type**

**Description**

str

The result of the task.

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If the task fails or an error occurs while waiting for completion.

ﾉ

**Expand table**

`async`` wait_for_crew_completion(kickoff_id: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**client**

Python

**is_experimental**

Python

**polling_interval**

Python

**polling_timeout**

Python

**stage_status**

Python

**Attributes**

`client: CrewAIEnterpriseClient`

`is_experimental = ``True`

`polling_interval: float`

`polling_timeout: float`

`stage_status = ``'experimental'`



**CrewAISettings Class**

Reference

The Crew.AI settings.

Required:

endpoint: str - The API endpoint.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`CrewAISettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`endpoint: str, auth_token: SecretStr, polling_interval: float = 1.0, `

`polling_timeout: float = 30.0)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**endpoint**`

Required*

`**auth_token**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**polling_interval**`

Default value: 1.0

`**polling_timeout**`

Default value: 30.0

**auth_token**

Python

**endpoint**

Python

**env_prefix**

Python

**polling_interval**

Python

**polling_timeout**

Python

**Attributes**

`auth_token: SecretStr`

`endpoint: str`

`env_prefix: ClassVar[str] = ``'CREW_AI_'`

`polling_interval: float`

`polling_timeout: float`



**CrewAIStatusResponse Class**

Reference

Represents the status response from Crew AI.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**state**`

Required*

`**result**`

Required*

`**last_step**`

Required*

**last_step**

Python

`CrewAIStatusResponse(*, state: CrewAIEnterpriseKickoffState, result: str | `

`None`` = ``None``, last_step: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**result**

Python

**state**

Python

`last_step: dict[str, Any] | ``None`

`result: str | ``None`

`state: CrewAIEnterpriseKickoffState`



**sessions_python_tool Package**

Reference

**Modules**

sessions_python_plugin

sessions_python_settings

sessions_remote_file_metadata

**Classes**

SessionsPythonSettings

The Sessions Python code interpreter settings.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

SessionsPythonTool

A plugin for running Python code in an Azure Container Apps dynamic

sessions code interpreter.

Initializes a new instance of the SessionsPythonTool class.

ﾉ

**Expand table**

ﾉ

**Expand table**



**sessions_python_plugin Module**

Reference

**Classes**

SessionsPythonTool

A plugin for running Python code in an Azure Container Apps dynamic

sessions code interpreter.

Initializes a new instance of the SessionsPythonTool class.

ﾉ

**Expand table**



**SessionsPythonTool Class**

Reference

A plugin for running Python code in an Azure Container Apps dynamic sessions
code

interpreter.

Initializes a new instance of the SessionsPythonTool class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**auth_callback**`

Default value: None

`**pool_management_endpoint**`

Default value: None

`**settings**`

Default value: None

`**http_client**`

Default value: None

`**env_file_path**`

Default value: None

`**token_endpoint**`

Default value: None

download_file

Download a file from the session pool.

execute_code

Executes the provided Python code.

`SessionsPythonTool(auth_callback: Callable[[...], Any | Awaitable[Any]] | `

`None`` = ``None``, pool_management_endpoint: str | ``None`` = ``None``, settings: `

`SessionsPythonSettings | ``None`` = ``None``, http_client: AsyncClient | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, token_endpoint: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



list_files

List the files in the session pool.

upload_file

Upload a file to the session pool.

**download_file**

Download a file from the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

The name of the file to download, relative to _/mnt/data_.

`**local_file_path**`

Required*

The path to save the downloaded file to. Should include the extension.

If not provided, the file is returned as a BufferedReader.

**Keyword-Only Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

`**local_file_path**`

Required*

**Returns**

`async`` download_file(*, remote_file_name: Annotated[str, ``'The name of the
`

`file to download, relative to /mnt/data'``], local_file_path: Annotated[str `

`| ``None``, ``'The local file path to save the file to, optional'``] =
``None``) -> `

`Annotated[BytesIO | ``None``, ``'The data of the downloaded file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

<xref:BufferedReader>

The data of the downloaded file.

**execute_code**

Executes the provided Python code.

Python

**Parameters**

**Name**

**Description**

`**code**`

Required*

str

The valid Python code to execute

**Returns**

**Type**

**Description**

str

The result of the Python code execution in the form of Result, Stdout, and
Stderr

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If the provided code is empty.

**list_files**

List the files in the session pool.

`async`` execute_code(code: Annotated[str, ``'The valid Python code to `

`execute'``]) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Returns**

**Type**

**Description**

list[SessionsRemoteFileMetadata]

The metadata for the files in the session pool

**upload_file**

Upload a file to the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_path**`

Required*

str

The path to the file in the session.

`**local_file_path**`

Required*

str

The path to the file on the local machine.

**Keyword-Only Parameters**

**Name**

**Description**

`**local_file_path**`

`async`` list_files() -> list[SessionsRemoteFileMetadata]`

ﾉ

**Expand table**

`async`` upload_file(*, local_file_path: Annotated[str, ``'The path to the `

`local file on the machine'``], remote_file_path: Annotated[str | ``None``, ``'The `

`remote path to the file in the session. Defaults to /mnt/data'``] = ``None``)
-`

`> Annotated[SessionsRemoteFileMetadata, ``'The metadata of the uploaded `

`file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**remote_file_path**`

Required*

**Returns**

**Type**

**Description**

<xref:RemoteFileMetadata>

The metadata of the uploaded file.

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If local_file_path is not provided.

**auth_callback**

Python

**http_client**

Python

**pool_management_endpoint**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`auth_callback: Callable[[...], Any | Awaitable[Any]]`

`http_client: AsyncClient`



**settings**

Python

`pool_management_endpoint: Annotated[Url, UrlConstraints(max_length=2083, `

`allowed_schemes=[``'https'``], host_required=``None``, default_host=``None``,
`

`default_port=``None``, default_path=``None``)]`

`settings: SessionsPythonSettings`



**sessions_python_settings Module**

Reference

**Classes**

ACASessionsSettings

Azure Container Apps sessions settings.

Required:

pool_management_endpoint: HttpsUrl - The URL of the Azure

Container Apps pool management endpoint.

(Env var ACA_POOL_MANAGEMENT_ENDPOINT)

SessionsPythonSettings

The Sessions Python code interpreter settings.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

CodeExecutionType

Code execution type.

CodeInputType

Code input type.

ﾉ

**Expand table**

ﾉ

**Expand table**



**ACASessionsSettings Class**

Reference

Azure Container Apps sessions settings.

Required:

pool_management_endpoint: HttpsUrl - The URL of the Azure Container Apps pool

management endpoint.

(Env var ACA_POOL_MANAGEMENT_ENDPOINT)

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`ACASessionsSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `

`pool_management_endpoint: Annotated[Url, UrlConstraints(max_length=2083, `

`allowed_schemes=[``'https'``], host_required=``None``, default_host=``None``,
`

`default_port=``None``, default_path=``None``)], token_endpoint: str = `

`'https://acasessions.io/.default'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

ﾉ

**Expand table**



**Name**

**Description**

`**pool_management_endpoint**`

Required*

`**token_endpoint**`

Default value: https://acasessions.io/.default

get_sessions_auth_token

Retrieve a Microsoft Entra Auth Token for a given token endpoint for

the use with an Azure Container App.

The required role for the token is _Azure ContainerApps Session Executor_

_and Contributor_. The token endpoint may be specified as an

environment variable, via the .env file or as an argument. If the token

endpoint is not provided, the default is None. The _token_endpoint_

argument takes precedence over the _token_endpoint_ attribute.

**get_sessions_auth_token**

Retrieve a Microsoft Entra Auth Token for a given token endpoint for the use
with an

Azure Container App.

The required role for the token is _Azure ContainerApps Session Executor and_

_Contributor_. The token endpoint may be specified as an environment variable,
via the

.env file or as an argument. If the token endpoint is not provided, the
default is

None. The _token_endpoint_ argument takes precedence over the _token_endpoint_

attribute.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`get_sessions_auth_token(token_endpoint: str | ``None`` = ``None``) -> str | ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**token_endpoint**`

The token endpoint to use. Defaults to _https://acasessions.io/.default_

.

Default value: None

**Returns**

**Type**

**Description**

The Azure token or None if the token could not be retrieved.

**Exceptions**

**Type**

**Description**

ServiceInitializationError

If the token endpoint is not provided.

**env_prefix**

Python

**pool_management_endpoint**

Python

**token_endpoint**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`env_prefix: ClassVar[str] = ``'ACA_'`

`pool_management_endpoint: Annotated[Url, UrlConstraints(max_length=2083, `

`allowed_schemes=[``'https'``], host_required=``None``, default_host=``None``,
`

`default_port=``None``, default_path=``None``)]`



`token_endpoint: str`



**CodeExecutionType Enum**

Reference

Code execution type.

Synchronous

**Fields**

ﾉ

**Expand table**



**CodeInputType Enum**

Reference

Code input type.

Inline

**Fields**

ﾉ

**Expand table**



**SessionsPythonSettings Class**

Reference

The Sessions Python code interpreter settings.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**identifier**`

Required*

`**codeInputType**`

Default value: CodeInputType.Inline

`**executionType**`

Default value: CodeExecutionType.Synchronous

`**code**`

Required*

`**timeoutInSeconds**`

Default value: 100

`**sanitizeInput**`

Default value: True

`SessionsPythonSettings(*, identifier: str | ``None`` = ``None``, codeInputType: `

`CodeInputType | ``None`` = CodeInputType.Inline, executionType: `

`CodeExecutionType | ``None`` = CodeExecutionType.Synchronous, code: str | ``None`` = `

`None``, timeoutInSeconds: int | ``None`` = 100, sanitizeInput: bool | ``None`` = ``True``)`

ﾉ

**Expand table**

**Attributes**



**code_input_type**

Python

**execution_type**

Python

**python_code**

Python

**sanitize_input**

Python

**session_id**

Python

**timeout_in_sec**

Python

`code_input_type: CodeInputType | ``None`

`execution_type: CodeExecutionType | ``None`

`python_code: str | ``None`

`sanitize_input: bool | ``None`

`session_id: str | ``None`

`timeout_in_sec: int | ``None`



**sessions_remote_file_metadata Module**

Reference

**Classes**

SessionsRemoteFileMetadata

Metadata for a file in the session.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**SessionsRemoteFileMetadata Class**

Reference

Metadata for a file in the session.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filename**`

Required*

`**size_in_bytes**`

Required*

from_dict

Create a SessionsRemoteFileMetadata object from a dictionary of file data
values.

**from_dict**

Create a SessionsRemoteFileMetadata object from a dictionary of file data
values.

`SessionsRemoteFileMetadata(*, filename: str, size_in_bytes: int)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

dict[str,<xref: Any>]

The file data values.

**Returns**

**Type**

**Description**

SessionsRemoteFileMetadata

The metadata for the file.

**filename**

The size of the file in bytes.

Python

**full_path**

Get the full path of the file.

**size_in_bytes**

Python

`static from_dict(data: dict[str, Any]) -> SessionsRemoteFileMetadata`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`filename: str`



`size_in_bytes: int`



**SessionsPythonSettings Class**

Reference

The Sessions Python code interpreter settings.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**identifier**`

Required*

`**codeInputType**`

Default value: CodeInputType.Inline

`**executionType**`

Default value: CodeExecutionType.Synchronous

`**code**`

Required*

`**timeoutInSeconds**`

Default value: 100

`**sanitizeInput**`

Default value: True

`SessionsPythonSettings(*, identifier: str | ``None`` = ``None``, codeInputType: `

`CodeInputType | ``None`` = CodeInputType.Inline, executionType: `

`CodeExecutionType | ``None`` = CodeExecutionType.Synchronous, code: str | ``None`` = `

`None``, timeoutInSeconds: int | ``None`` = 100, sanitizeInput: bool | ``None`` = ``True``)`

ﾉ

**Expand table**

**Attributes**



**code_input_type**

Python

**execution_type**

Python

**python_code**

Python

**sanitize_input**

Python

**session_id**

Python

**timeout_in_sec**

Python

`code_input_type: CodeInputType | ``None`

`execution_type: CodeExecutionType | ``None`

`python_code: str | ``None`

`sanitize_input: bool | ``None`

`session_id: str | ``None`

`timeout_in_sec: int | ``None`



**SessionsPythonTool Class**

Reference

A plugin for running Python code in an Azure Container Apps dynamic sessions
code

interpreter.

Initializes a new instance of the SessionsPythonTool class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**auth_callback**`

Default value: None

`**pool_management_endpoint**`

Default value: None

`**settings**`

Default value: None

`**http_client**`

Default value: None

`**env_file_path**`

Default value: None

`**token_endpoint**`

Default value: None

download_file

Download a file from the session pool.

execute_code

Executes the provided Python code.

`SessionsPythonTool(auth_callback: Callable[[...], Any | Awaitable[Any]] | `

`None`` = ``None``, pool_management_endpoint: str | ``None`` = ``None``, settings: `

`SessionsPythonSettings | ``None`` = ``None``, http_client: AsyncClient | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, token_endpoint: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



list_files

List the files in the session pool.

upload_file

Upload a file to the session pool.

**download_file**

Download a file from the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

The name of the file to download, relative to _/mnt/data_.

`**local_file_path**`

Required*

The path to save the downloaded file to. Should include the extension.

If not provided, the file is returned as a BufferedReader.

**Keyword-Only Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

`**local_file_path**`

Required*

**Returns**

`async`` download_file(*, remote_file_name: Annotated[str, ``'The name of the
`

`file to download, relative to /mnt/data'``], local_file_path: Annotated[str `

`| ``None``, ``'The local file path to save the file to, optional'``] =
``None``) -> `

`Annotated[BytesIO | ``None``, ``'The data of the downloaded file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

<xref:BufferedReader>

The data of the downloaded file.

**execute_code**

Executes the provided Python code.

Python

**Parameters**

**Name**

**Description**

`**code**`

Required*

str

The valid Python code to execute

**Returns**

**Type**

**Description**

str

The result of the Python code execution in the form of Result, Stdout, and
Stderr

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If the provided code is empty.

**list_files**

List the files in the session pool.

`async`` execute_code(code: Annotated[str, ``'The valid Python code to `

`execute'``]) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Returns**

**Type**

**Description**

list[SessionsRemoteFileMetadata]

The metadata for the files in the session pool

**upload_file**

Upload a file to the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_path**`

Required*

str

The path to the file in the session.

`**local_file_path**`

Required*

str

The path to the file on the local machine.

**Keyword-Only Parameters**

**Name**

**Description**

`**local_file_path**`

`async`` list_files() -> list[SessionsRemoteFileMetadata]`

ﾉ

**Expand table**

`async`` upload_file(*, local_file_path: Annotated[str, ``'The path to the `

`local file on the machine'``], remote_file_path: Annotated[str | ``None``, ``'The `

`remote path to the file in the session. Defaults to /mnt/data'``] = ``None``)
-`

`> Annotated[SessionsRemoteFileMetadata, ``'The metadata of the uploaded `

`file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**remote_file_path**`

Required*

**Returns**

**Type**

**Description**

<xref:RemoteFileMetadata>

The metadata of the uploaded file.

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If local_file_path is not provided.

**auth_callback**

Python

**http_client**

Python

**pool_management_endpoint**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`auth_callback: Callable[[...], Any | Awaitable[Any]]`

`http_client: AsyncClient`



**settings**

Python

`pool_management_endpoint: Annotated[Url, UrlConstraints(max_length=2083, `

`allowed_schemes=[``'https'``], host_required=``None``, default_host=``None``,
`

`default_port=``None``, default_path=``None``)]`

`settings: SessionsPythonSettings`



**conversation_summary_plugin Module**

Reference

**Classes**

ConversationSummaryPlugin

Semantic plugin that enables conversations summarization.

Initializes a new instance of the ConversationSummaryPlugin.

The template for this plugin is built-in, and will overwrite any

template passed in the prompt_template_config.

ﾉ

**Expand table**



**ConversationSummaryPlugin Class**

Reference

Semantic plugin that enables conversations summarization.

Initializes a new instance of the ConversationSummaryPlugin.

The template for this plugin is built-in, and will overwrite any template
passed in the

prompt_template_config.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

<xref:semantic_kernel.core_plugins.conversation_summary_plugin.PromptTemplateConfig>

The prompt template configuration.

`**return_key**`

str

The key to use for the return value.

Default value: summary

`****kwargs**`

Required*

Additional keyword arguments, not used only for compatibility.

summarize_conversation

Given a long conversation transcript, summarize the conversation.

**summarize_conversation**

Given a long conversation transcript, summarize the conversation.

Python

`ConversationSummaryPlugin(prompt_template_config: PromptTemplateConfig,
return_key: str `

`= ``'summary'``, **kwargs: Any)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`async`` summarize_conversation(input: Annotated[str, ``'A long conversation `

`transcript.'``], kernel: Annotated[Kernel, ``'The kernel instance.'``],
arguments: `



**Parameters**

**Name**

**Description**

`**input**`

Required*

str

A long conversation transcript.

`**kernel**`

Required*

<xref:semantic_kernel.core_plugins.conversation_summary_plugin.Kernel>

The kernel for function execution.

`**arguments**`

Required*

<xref:semantic_kernel.core_plugins.conversation_summary_plugin.KernelArguments>

Arguments used by the kernel.

**Returns**

**Type**

**Description**

KernelArguments with the summarized conversation result in key
self.return_key.

`Annotated[KernelArguments, ``'Arguments used by the kernel.'``]) -> `

`Annotated[KernelArguments, ``'KernelArguments with the summarized
conversation result `

`in key self.return_key.'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**http_plugin Module**

Reference

**Classes**

HttpPlugin

A plugin that provides HTTP functionality.

Usage: kernel.add_plugin(HttpPlugin(), "http")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**HttpPlugin Class**

Reference

A plugin that provides HTTP functionality.

Usage: kernel.add_plugin(HttpPlugin(), "http")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

{{http.getAsync $url}} {{http.postAsync $url}} {{http.putAsync $url}}
{{http.deleteAsync

$url}}

delete

Sends an HTTP DELETE request to the specified URI and returns the response
body as a

string.

get

Sends an HTTP GET request to the specified URI and returns the response body
as a

string.

post

Sends an HTTP POST request to the specified URI and returns the response body
as a

string.

put

Sends an HTTP PUT request to the specified URI and returns the response body
as a

string.

`HttpPlugin()`

**Methods**

ﾉ

**Expand table**



**delete**

Sends an HTTP DELETE request to the specified URI and returns the response
body

as a string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

**Returns**

**Type**

**Description**

The response body as a string.

**get**

Sends an HTTP GET request to the specified URI and returns the response body
as a

string.

Python

**Parameters**

`async`` delete(url: Annotated[str, ``'The URI to send the request to.'``]) ->
`

`str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get(url: Annotated[str, ``'The URL to send the request to.'``]) ->
str`

ﾉ

**Expand table**



**Name**

**Description**

`**url**`

Required*

The URL to send the request to.

**Returns**

**Type**

**Description**

The response body as a string.

**post**

Sends an HTTP POST request to the specified URI and returns the response body
as

a string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

`**body**`

Required*

Contains the body of the request

Default value: {}

**Returns**

**Type**

**Description**

The response body as a string.

ﾉ

**Expand table**

`async`` post(url: Annotated[str, ``'The URI to send the request to.'``],
body: `

`Annotated[dict[str, Any] | ``None``, ``'The body of the request'``] = {}) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**put**

Sends an HTTP PUT request to the specified URI and returns the response body
as a

string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

`**body**`

Required*

Contains the body of the request

Default value: {}

**Returns**

**Type**

**Description**

The response body as a string.

`async`` put(url: Annotated[str, ``'The URI to send the request to.'``], body:
`

`Annotated[dict[str, Any] | ``None``, ``'The body of the request'``] = {}) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**math_plugin Module**

Reference

**Classes**

MathPlugin

Description: MathPlugin provides a set of functions to make Math calculations.

Usage: kernel.add_plugin(MathPlugin(), plugin_name="math")

ﾉ

**Expand table**



**MathPlugin Class**

Reference

Description: MathPlugin provides a set of functions to make Math calculations.

Usage: kernel.add_plugin(MathPlugin(), plugin_name="math")

**Constructor**

Python

**Examples**

{{math.Add}} => Returns the sum of input and amount (provided in the

KernelArguments) {{math.Subtract}} => Returns the difference of input and
amount

(provided in the KernelArguments)

add

Returns the Addition result of the values provided.

add_or_subtract

Helper function to perform addition or subtraction based on the add flag.

subtract

Returns the difference of numbers provided.

**add**

Returns the Addition result of the values provided.

Python

`MathPlugin()`

**Methods**

ﾉ

**Expand table**

`add(input: Annotated[int, ``'the first number to add'``], amount: `

`Annotated[int, ``'the second number to add'``]) -> Annotated[int, ``'the
output `

`is a number'``]`



**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

**add_or_subtract**

Helper function to perform addition or subtraction based on the add flag.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

`**add**`

Required*

**subtract**

Returns the difference of numbers provided.

Python

ﾉ

**Expand table**

`static add_or_subtract(input: int, amount: int, add: bool) -> int`

ﾉ

**Expand table**

`subtract(input: Annotated[int, ``'the first number'``], amount: `

`Annotated[int, ``'the number to subtract'``]) -> int`



**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

ﾉ

**Expand table**



**text_memory_plugin Module**

Reference

**Classes**

TextMemoryPlugin

A plugin to interact with a Semantic Text Memory.

Initialize a new instance of the TextMemoryPlugin.

ﾉ

**Expand table**



**TextMemoryPlugin Class**

Reference

A plugin to interact with a Semantic Text Memory.

Initialize a new instance of the TextMemoryPlugin.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**memory**`

Required*

<xref:semantic_kernel.core_plugins.text_memory_plugin.SemanticTextMemoryBase>

the underlying Semantic Text Memory to use

`**embeddings_kwargs**`

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

the keyword arguments to pass to the embedding generator

Default value: {}

recall

Recall a fact from the long term memory.

save

Save a fact to the long term memory.

**recall**

Recall a fact from the long term memory.

Python

`TextMemoryPlugin(memory: SemanticTextMemoryBase, embeddings_kwargs: dict[str,
`

`Any] = {})`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**ask**`

Required*

The question to ask the memory

`**collection**`

Required*

The collection to search for information

Default value: generic

`**relevance**`

Required*

The relevance score, from 0.0 to 1.0; 1.0 means perfect match

Default value: 0.75

`**limit**`

Required*

The maximum number of relevant memories to recall

Default value: 1

**Returns**

**Type**

**Description**

The nearest item from the memory store as a string or empty string if not
found.

**Examples**

{{memory.recall $ask}} => "Paris"

**save**

Save a fact to the long term memory.

Python

`async`` recall(ask: Annotated[str, ``'The information to retrieve'``], `

`collection: Annotated[str, ``'The collection to search for information.'``] =
`

`'generic'``, relevance: Annotated[float, ``'The relevance score, from 0.0 to
`

`1.0; 1.0 means perfect match'``] = 0.75, limit: Annotated[int, ``'The maximum
`

`number of relevant memories to recall.'``] = 1) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` save(text: Annotated[str, ``'The information to save.'``], key: `

`Annotated[str, ``'The unique key to associate with the information.'``], `



**Parameters**

**Name**

**Description**

`**text**`

Required*

The text to save to the memory

`**kernel**`

Required*

The kernel instance, that has a memory store

`**collection**`

Required*

The collection to save the information

Default value: generic

`**key**`

Required*

The unique key to associate with the information

**embeddings_kwargs**

Python

**memory**

Python

`collection: Annotated[str, ``'The collection to save the information.'``] = `

`'generic'``) -> ``None`

ﾉ

**Expand table**

**Attributes**

`embeddings_kwargs: dict[str, Any]`

`memory: SemanticTextMemoryBase`



**text_plugin Module**

Reference

**Classes**

TextPlugin

TextPlugin provides a set of functions to manipulate strings.

Usage: kernel.add_plugin(TextPlugin(), plugin_name="text")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextPlugin Class**

Reference

TextPlugin provides a set of functions to manipulate strings.

Usage: kernel.add_plugin(TextPlugin(), plugin_name="text")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

KernelArguments["input"] = " hello world " {{text.trim $input}} => "hello

world"KernelArguments["input"] = " hello world " {{text.trimStart $input}} =>
"hello

world "KernelArguments["input"] = " hello world " {{text.trimEnd $input}} => "
hello

world"KernelArguments["input"] = "hello world" {{text.uppercase $input}} =>
"HELLO

WORLD"KernelArguments["input"] = "HELLO WORLD" {{text.lowercase $input}} =>

"hello world"

lowercase

Convert a string to lowercase.

trim

Trim whitespace from the start and end of a string.

trim_end

Trim whitespace from the end of a string.

trim_start

Trim whitespace from the start of a string.

`TextPlugin()`

**Methods**

ﾉ

**Expand table**



uppercase

Convert a string to uppercase.

**lowercase**

Convert a string to lowercase.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = "HELLO WORLD" {{input.lowercase $input}} => "hello

world"

**trim**

Trim whitespace from the start and end of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`lowercase(input: str) -> str`

ﾉ

**Expand table**

`trim(input: str) -> str`

ﾉ

**Expand table**



**Examples**

KernelArguments["input"] = " hello world " {{text.trim $input}} => "hello
world"

**trim_end**

Trim whitespace from the end of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = " hello world " {{input.trim $input}} => " hello
world"

**trim_start**

Trim whitespace from the start of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`trim_end(input: str) -> str`

ﾉ

**Expand table**

`trim_start(input: str) -> str`

ﾉ

**Expand table**



**Examples**

KernelArguments["input"] = " hello world " {{input.trim $input}} => "hello
world "

**uppercase**

Convert a string to uppercase.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = "hello world" {{input.uppercase $input}} => "HELLO

WORLD"

`uppercase(input: str) -> str`

ﾉ

**Expand table**



**time_plugin Module**

Reference

**Classes**

TimePlugin

TimePlugin provides a set of functions to get the current time and date.

Usage: kernel.add_plugin(TimePlugin(), plugin_name="time")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TimePlugin Class**

Reference

TimePlugin provides a set of functions to get the current time and date.

Usage: kernel.add_plugin(TimePlugin(), plugin_name="time")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

{{time.date}} => Sunday, 12 January, 2031 {{time.today}} => Sunday, 12
January, 2031

{{time.iso_date}} => 2031-01-12 {{time.now}} => Sunday, January 12, 2031 9:15
PM

{{time.utcNow}} => Sunday, January 13, 2031 5:15 AM {{time.time}} => 09:15:07
PM

{{time.year}} => 2031 {{time.month}} => January {{time.monthNumber}} => 01

{{time.day}} => 12 {{time.dayOfWeek}} => Sunday {{time.hour}} => 9 PM

{{time.hourNumber}} => 21 {{time.days_ago $days}} => Sunday, 7 May, 2023

{{time.last_matching_day $dayName}} => Sunday, 7 May, 2023 {{time.minute}} =>
15

{{time.minutes}} => 15 {{time.second}} => 7 {{time.seconds}} => 7

{{time.timeZoneOffset}} => -0800 {{time.timeZoneName}} => PST

date

Get the current date.

date_matching_last_day_name

Get the date of the last day matching the supplied day name.

`TimePlugin()`

**Methods**

ﾉ

**Expand table**



day

Get the current day of the month.

day_of_week

Get the current day of the week.

days_ago

Get the date a provided number of days in the past.

hour

Get the current hour.

hour_number

Get the current hour number.

iso_date

Get the current date in iso format.

minute

Get the current minute.

month

Get the current month.

month_number

Get the current month number.

now

Get the current date and time in the local time zone.

second

Get the seconds on the current minute.

time

Get the current time in the local time zone.

time_zone_name

Get the current time zone name.

time_zone_offset

Get the current time zone offset.

today

Get the current date.

utc_now

Get the current date and time in UTC.

year

Get the current year.

**date**

Get the current date.

Python

**Examples**

{{time.date}} => Sunday, 12 January, 2031

**date_matching_last_day_name**

`date() -> str`



Get the date of the last day matching the supplied day name.

Python

**Parameters**

**Name**

**Description**

`**day_name**`

Required*

The day name to match with.

**Returns**

**Type**

**Description**

The date of the matching day.

**Examples**

{{time.date_matching_last_day_name $input}} => Sunday, 7 May, 2023

**day**

Get the current day of the month.

Python

**Examples**

{{time.day}} => 12

**day_of_week**

`date_matching_last_day_name(day_name: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`day() -> str`



Get the current day of the week.

Python

**Examples**

{{time.dayOfWeek}} => Sunday

**days_ago**

Get the date a provided number of days in the past.

Python

**Parameters**

**Name**

**Description**

`**days**`

Required*

The number of days to offset from today

**Returns**

**Type**

**Description**

The date of the offset day.

**Examples**

{{time.days_ago $input}} => Sunday, 7 May, 2023

**hour**

`day_of_week() -> str`

`days_ago(days: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



Get the current hour.

Python

**Examples**

{{time.hour}} => 9 PM

**hour_number**

Get the current hour number.

Python

**Examples**

{{time.hourNumber}} => 21

**iso_date**

Get the current date in iso format.

Python

**Examples**

{{time.iso_date}} => 2031-01-12

**minute**

Get the current minute.

Python

`hour() -> str`

`hour_number() -> str`

`iso_date() -> str`



**Examples**

{{time.minute}} => 15

**month**

Get the current month.

Python

**Examples**

{{time.month}} => January

**month_number**

Get the current month number.

Python

**Examples**

{{time.monthNumber}} => 01

**now**

Get the current date and time in the local time zone.

Python

**Examples**

`minute() -> str`

`month() -> str`

`month_number() -> str`

`now() -> str`



{{time.now}} => Sunday, January 12, 2031 9:15 PM

**second**

Get the seconds on the current minute.

Python

**Examples**

{{time.second}} => 7

**time**

Get the current time in the local time zone.

Python

**Examples**

{{time.time}} => 09:15:07 PM

**time_zone_name**

Get the current time zone name.

Python

**Examples**

{{time.timeZoneName}} => PST

**time_zone_offset**

`second() -> str`

`time() -> str`

`time_zone_name() -> str`



Get the current time zone offset.

Python

**Examples**

{{time.timeZoneOffset}} => -08:00

**today**

Get the current date.

Python

**Examples**

{{time.today}} => Sunday, 12 January, 2031

**utc_now**

Get the current date and time in UTC.

Python

**Examples**

{{time.utcNow}} => Sunday, January 13, 2031 5:15 AM

**year**

Get the current year.

Python

`time_zone_offset() -> str`

`today() -> str`

`utc_now() -> str`



**Examples**

{{time.year}} => 2031

`year() -> str`



**wait_plugin Module**

Reference

**Classes**

WaitPlugin

WaitPlugin provides a set of functions to wait for a certain amount of time.

Usage: kernel.add_plugin(WaitPlugin(), plugin_name="wait")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**WaitPlugin Class**

Reference

WaitPlugin provides a set of functions to wait for a certain amount of time.

Usage: kernel.add_plugin(WaitPlugin(), plugin_name="wait")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

{{wait.wait 5}} => Wait for 5 seconds

wait

Wait for a certain number of seconds.

**wait**

Wait for a certain number of seconds.

Python

`WaitPlugin()`

**Methods**

ﾉ

**Expand table**

`async`` wait(input: Annotated[float | str, ``'The number of seconds to wait, `

`can be str or float.'``]) -> ``None`



**Parameters**

**Name**

**Description**

`**input**`

Required*

ﾉ

**Expand table**



**web_search_engine_plugin Module**

Reference

**Classes**

WebSearchEnginePlugin

A plugin that provides web search engine functionality.

Usage: connector = BingConnector(bing_search_api_key)

kernel.add_plugin(WebSearchEnginePlugin(connector),

plugin_name="WebSearch")

Initializes a new instance of the WebSearchEnginePlugin class.

ﾉ

**Expand table**



**WebSearchEnginePlugin Class**

Reference

A plugin that provides web search engine functionality.

Usage: connector = BingConnector(bing_search_api_key)

kernel.add_plugin(WebSearchEnginePlugin(connector), plugin_name="WebSearch")

Initializes a new instance of the WebSearchEnginePlugin class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**connector**`

Required*

**Examples**

{{WebSearch.search "What is semantic kernel?"}} => Returns the first
_num_results_

number of results for the given search query

and ignores the first _offset_ number of results.

search

Returns the search results of the query provided.

**search**

`WebSearchEnginePlugin(connector: ConnectorBase)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Returns the search results of the query provided.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**num_results**`

Required*

Default value: 1

`**offset**`

Required*

Default value: 0

`async`` search(query: Annotated[str, ``'The search query'``], num_results: `

`Annotated[int, ``'The number of search results to return'``] = 1, offset: `

`Annotated[int, ``'The number of search results to skip'``] = 0) -> list[str]`

ﾉ

**Expand table**



**ConversationSummaryPlugin Class**

Reference

Semantic plugin that enables conversations summarization.

Initializes a new instance of the ConversationSummaryPlugin.

The template for this plugin is built-in, and will overwrite any template
passed in the

prompt_template_config.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

<xref:semantic_kernel.core_plugins.PromptTemplateConfig>

The prompt template configuration.

`**return_key**`

str

The key to use for the return value.

Default value: summary

`****kwargs**`

Required*

Additional keyword arguments, not used only for compatibility.

summarize_conversation

Given a long conversation transcript, summarize the conversation.

**summarize_conversation**

`ConversationSummaryPlugin(prompt_template_config: PromptTemplateConfig, `

`return_key: str = ``'summary'``, **kwargs: Any)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Given a long conversation transcript, summarize the conversation.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

str

A long conversation transcript.

`**kernel**`

Required*

<xref:semantic_kernel.core_plugins.Kernel>

The kernel for function execution.

`**arguments**`

Required*

<xref:semantic_kernel.core_plugins.KernelArguments>

Arguments used by the kernel.

**Returns**

**Type**

**Description**

KernelArguments with the summarized conversation result in key
self.return_key.

`async`` summarize_conversation(input: Annotated[str, ``'A long conversation `

`transcript.'``], kernel: Annotated[Kernel, ``'The kernel instance.'``], `

`arguments: Annotated[KernelArguments, ``'Arguments used by the kernel.'``])
-`

`> Annotated[KernelArguments, ``'KernelArguments with the summarized `

`conversation result in key self.return_key.'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**HttpPlugin Class**

Reference

A plugin that provides HTTP functionality.

Usage: kernel.add_plugin(HttpPlugin(), "http")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

{{http.getAsync $url}} {{http.postAsync $url}} {{http.putAsync $url}}
{{http.deleteAsync

$url}}

delete

Sends an HTTP DELETE request to the specified URI and returns the response
body as a

string.

get

Sends an HTTP GET request to the specified URI and returns the response body
as a

string.

post

Sends an HTTP POST request to the specified URI and returns the response body
as a

string.

put

Sends an HTTP PUT request to the specified URI and returns the response body
as a

string.

`HttpPlugin()`

**Methods**

ﾉ

**Expand table**



**delete**

Sends an HTTP DELETE request to the specified URI and returns the response
body

as a string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

**Returns**

**Type**

**Description**

The response body as a string.

**get**

Sends an HTTP GET request to the specified URI and returns the response body
as a

string.

Python

**Parameters**

`async`` delete(url: Annotated[str, ``'The URI to send the request to.'``]) ->
`

`str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get(url: Annotated[str, ``'The URL to send the request to.'``]) ->
str`

ﾉ

**Expand table**



**Name**

**Description**

`**url**`

Required*

The URL to send the request to.

**Returns**

**Type**

**Description**

The response body as a string.

**post**

Sends an HTTP POST request to the specified URI and returns the response body
as

a string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

`**body**`

Required*

Contains the body of the request

Default value: {}

**Returns**

**Type**

**Description**

The response body as a string.

ﾉ

**Expand table**

`async`` post(url: Annotated[str, ``'The URI to send the request to.'``],
body: `

`Annotated[dict[str, Any] | ``None``, ``'The body of the request'``] = {}) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**put**

Sends an HTTP PUT request to the specified URI and returns the response body
as a

string.

Python

**Parameters**

**Name**

**Description**

`**url**`

Required*

The URI to send the request to.

`**body**`

Required*

Contains the body of the request

Default value: {}

**Returns**

**Type**

**Description**

The response body as a string.

`async`` put(url: Annotated[str, ``'The URI to send the request to.'``], body:
`

`Annotated[dict[str, Any] | ``None``, ``'The body of the request'``] = {}) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**MathPlugin Class**

Reference

Description: MathPlugin provides a set of functions to make Math calculations.

Usage: kernel.add_plugin(MathPlugin(), plugin_name="math")

**Constructor**

Python

**Examples**

{{math.Add}} => Returns the sum of input and amount (provided in the

KernelArguments) {{math.Subtract}} => Returns the difference of input and
amount

(provided in the KernelArguments)

add

Returns the Addition result of the values provided.

add_or_subtract

Helper function to perform addition or subtraction based on the add flag.

subtract

Returns the difference of numbers provided.

**add**

Returns the Addition result of the values provided.

Python

`MathPlugin()`

**Methods**

ﾉ

**Expand table**

`add(input: Annotated[int, ``'the first number to add'``], amount: `

`Annotated[int, ``'the second number to add'``]) -> Annotated[int, ``'the
output `

`is a number'``]`



**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

**add_or_subtract**

Helper function to perform addition or subtraction based on the add flag.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

`**add**`

Required*

**subtract**

Returns the difference of numbers provided.

Python

ﾉ

**Expand table**

`static add_or_subtract(input: int, amount: int, add: bool) -> int`

ﾉ

**Expand table**

`subtract(input: Annotated[int, ``'the first number'``], amount: `

`Annotated[int, ``'the number to subtract'``]) -> int`



**Parameters**

**Name**

**Description**

`**input**`

Required*

`**amount**`

Required*

ﾉ

**Expand table**



**SessionsPythonTool Class**

Reference

A plugin for running Python code in an Azure Container Apps dynamic sessions
code

interpreter.

Initializes a new instance of the SessionsPythonTool class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**auth_callback**`

Default value: None

`**pool_management_endpoint**`

Default value: None

`**settings**`

Default value: None

`**http_client**`

Default value: None

`**env_file_path**`

Default value: None

`**token_endpoint**`

Default value: None

download_file

Download a file from the session pool.

execute_code

Executes the provided Python code.

`SessionsPythonTool(auth_callback: Callable[[...], Any | Awaitable[Any]] | `

`None`` = ``None``, pool_management_endpoint: str | ``None`` = ``None``, settings: `

`SessionsPythonSettings | ``None`` = ``None``, http_client: AsyncClient | ``None`` = `

`None``, env_file_path: str | ``None`` = ``None``, token_endpoint: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



list_files

List the files in the session pool.

upload_file

Upload a file to the session pool.

**download_file**

Download a file from the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

The name of the file to download, relative to _/mnt/data_.

`**local_file_path**`

Required*

The path to save the downloaded file to. Should include the extension.

If not provided, the file is returned as a BufferedReader.

**Keyword-Only Parameters**

**Name**

**Description**

`**remote_file_name**`

Required*

`**local_file_path**`

Required*

**Returns**

`async`` download_file(*, remote_file_name: Annotated[str, ``'The name of the
`

`file to download, relative to /mnt/data'``], local_file_path: Annotated[str `

`| ``None``, ``'The local file path to save the file to, optional'``] =
``None``) -> `

`Annotated[BytesIO | ``None``, ``'The data of the downloaded file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

<xref:BufferedReader>

The data of the downloaded file.

**execute_code**

Executes the provided Python code.

Python

**Parameters**

**Name**

**Description**

`**code**`

Required*

str

The valid Python code to execute

**Returns**

**Type**

**Description**

str

The result of the Python code execution in the form of Result, Stdout, and
Stderr

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If the provided code is empty.

**list_files**

List the files in the session pool.

`async`` execute_code(code: Annotated[str, ``'The valid Python code to `

`execute'``]) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Returns**

**Type**

**Description**

list[SessionsRemoteFileMetadata]

The metadata for the files in the session pool

**upload_file**

Upload a file to the session pool.

Python

**Parameters**

**Name**

**Description**

`**remote_file_path**`

Required*

str

The path to the file in the session.

`**local_file_path**`

Required*

str

The path to the file on the local machine.

**Keyword-Only Parameters**

**Name**

**Description**

`**local_file_path**`

`async`` list_files() -> list[SessionsRemoteFileMetadata]`

ﾉ

**Expand table**

`async`` upload_file(*, local_file_path: Annotated[str, ``'The path to the `

`local file on the machine'``], remote_file_path: Annotated[str | ``None``, ``'The `

`remote path to the file in the session. Defaults to /mnt/data'``] = ``None``)
-`

`> Annotated[SessionsRemoteFileMetadata, ``'The metadata of the uploaded `

`file'``]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**remote_file_path**`

Required*

**Returns**

**Type**

**Description**

<xref:RemoteFileMetadata>

The metadata of the uploaded file.

**Exceptions**

**Type**

**Description**

FunctionExecutionException

If local_file_path is not provided.

**auth_callback**

Python

**http_client**

Python

**pool_management_endpoint**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`auth_callback: Callable[[...], Any | Awaitable[Any]]`

`http_client: AsyncClient`



**settings**

Python

`pool_management_endpoint: Annotated[Url, UrlConstraints(max_length=2083, `

`allowed_schemes=[``'https'``], host_required=``None``, default_host=``None``,
`

`default_port=``None``, default_path=``None``)]`

`settings: SessionsPythonSettings`



**TextMemoryPlugin Class**

Reference

A plugin to interact with a Semantic Text Memory.

Initialize a new instance of the TextMemoryPlugin.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**memory**`

Required*

<xref:semantic_kernel.core_plugins.SemanticTextMemoryBase>

the underlying Semantic Text Memory to use

`**embeddings_kwargs**`

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

the keyword arguments to pass to the embedding generator

Default value: {}

recall

Recall a fact from the long term memory.

save

Save a fact to the long term memory.

**recall**

Recall a fact from the long term memory.

Python

`TextMemoryPlugin(memory: SemanticTextMemoryBase, embeddings_kwargs: `

`dict[str, Any] = {})`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**ask**`

Required*

The question to ask the memory

`**collection**`

Required*

The collection to search for information

Default value: generic

`**relevance**`

Required*

The relevance score, from 0.0 to 1.0; 1.0 means perfect match

Default value: 0.75

`**limit**`

Required*

The maximum number of relevant memories to recall

Default value: 1

**Returns**

**Type**

**Description**

The nearest item from the memory store as a string or empty string if not
found.

**Examples**

{{memory.recall $ask}} => "Paris"

**save**

Save a fact to the long term memory.

Python

`async`` recall(ask: Annotated[str, ``'The information to retrieve'``], `

`collection: Annotated[str, ``'The collection to search for information.'``] =
`

`'generic'``, relevance: Annotated[float, ``'The relevance score, from 0.0 to
`

`1.0; 1.0 means perfect match'``] = 0.75, limit: Annotated[int, ``'The maximum
`

`number of relevant memories to recall.'``] = 1) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` save(text: Annotated[str, ``'The information to save.'``], key: `

`Annotated[str, ``'The unique key to associate with the information.'``], `



**Parameters**

**Name**

**Description**

`**text**`

Required*

The text to save to the memory

`**kernel**`

Required*

The kernel instance, that has a memory store

`**collection**`

Required*

The collection to save the information

Default value: generic

`**key**`

Required*

The unique key to associate with the information

**embeddings_kwargs**

Python

**memory**

Python

`collection: Annotated[str, ``'The collection to save the information.'``] = `

`'generic'``) -> ``None`

ﾉ

**Expand table**

**Attributes**

`embeddings_kwargs: dict[str, Any]`

`memory: SemanticTextMemoryBase`



**TextPlugin Class**

Reference

TextPlugin provides a set of functions to manipulate strings.

Usage: kernel.add_plugin(TextPlugin(), plugin_name="text")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

KernelArguments["input"] = " hello world " {{text.trim $input}} => "hello

world"KernelArguments["input"] = " hello world " {{text.trimStart $input}} =>
"hello

world "KernelArguments["input"] = " hello world " {{text.trimEnd $input}} => "
hello

world"KernelArguments["input"] = "hello world" {{text.uppercase $input}} =>
"HELLO

WORLD"KernelArguments["input"] = "HELLO WORLD" {{text.lowercase $input}} =>

"hello world"

lowercase

Convert a string to lowercase.

trim

Trim whitespace from the start and end of a string.

trim_end

Trim whitespace from the end of a string.

trim_start

Trim whitespace from the start of a string.

`TextPlugin()`

**Methods**

ﾉ

**Expand table**



uppercase

Convert a string to uppercase.

**lowercase**

Convert a string to lowercase.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = "HELLO WORLD" {{input.lowercase $input}} => "hello

world"

**trim**

Trim whitespace from the start and end of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`lowercase(input: str) -> str`

ﾉ

**Expand table**

`trim(input: str) -> str`

ﾉ

**Expand table**



**Examples**

KernelArguments["input"] = " hello world " {{text.trim $input}} => "hello
world"

**trim_end**

Trim whitespace from the end of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = " hello world " {{input.trim $input}} => " hello
world"

**trim_start**

Trim whitespace from the start of a string.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

`trim_end(input: str) -> str`

ﾉ

**Expand table**

`trim_start(input: str) -> str`

ﾉ

**Expand table**



**Examples**

KernelArguments["input"] = " hello world " {{input.trim $input}} => "hello
world "

**uppercase**

Convert a string to uppercase.

Python

**Parameters**

**Name**

**Description**

`**input**`

Required*

**Examples**

KernelArguments["input"] = "hello world" {{input.uppercase $input}} => "HELLO

WORLD"

`uppercase(input: str) -> str`

ﾉ

**Expand table**



**TimePlugin Class**

Reference

TimePlugin provides a set of functions to get the current time and date.

Usage: kernel.add_plugin(TimePlugin(), plugin_name="time")

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Examples**

{{time.date}} => Sunday, 12 January, 2031 {{time.today}} => Sunday, 12
January, 2031

{{time.iso_date}} => 2031-01-12 {{time.now}} => Sunday, January 12, 2031 9:15
PM

{{time.utcNow}} => Sunday, January 13, 2031 5:15 AM {{time.time}} => 09:15:07
PM

{{time.year}} => 2031 {{time.month}} => January {{time.monthNumber}} => 01

{{time.day}} => 12 {{time.dayOfWeek}} => Sunday {{time.hour}} => 9 PM

{{time.hourNumber}} => 21 {{time.days_ago $days}} => Sunday, 7 May, 2023

{{time.last_matching_day $dayName}} => Sunday, 7 May, 2023 {{time.minute}} =>
15

{{time.minutes}} => 15 {{time.second}} => 7 {{time.seconds}} => 7

{{time.timeZoneOffset}} => -0800 {{time.timeZoneName}} => PST

date

Get the current date.

date_matching_last_day_name

Get the date of the last day matching the supplied day name.

`TimePlugin()`

**Methods**

ﾉ

**Expand table**



day

Get the current day of the month.

day_of_week

Get the current day of the week.

days_ago

Get the date a provided number of days in the past.

hour

Get the current hour.

hour_number

Get the current hour number.

iso_date

Get the current date in iso format.

minute

Get the current minute.

month

Get the current month.

month_number

Get the current month number.

now

Get the current date and time in the local time zone.

second

Get the seconds on the current minute.

time

Get the current time in the local time zone.

time_zone_name

Get the current time zone name.

time_zone_offset

Get the current time zone offset.

today

Get the current date.

utc_now

Get the current date and time in UTC.

year

Get the current year.

**date**

Get the current date.

Python

**Examples**

{{time.date}} => Sunday, 12 January, 2031

**date_matching_last_day_name**

`date() -> str`



Get the date of the last day matching the supplied day name.

Python

**Parameters**

**Name**

**Description**

`**day_name**`

Required*

The day name to match with.

**Returns**

**Type**

**Description**

The date of the matching day.

**Examples**

{{time.date_matching_last_day_name $input}} => Sunday, 7 May, 2023

**day**

Get the current day of the month.

Python

**Examples**

{{time.day}} => 12

**day_of_week**

`date_matching_last_day_name(day_name: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`day() -> str`



Get the current day of the week.

Python

**Examples**

{{time.dayOfWeek}} => Sunday

**days_ago**

Get the date a provided number of days in the past.

Python

**Parameters**

**Name**

**Description**

`**days**`

Required*

The number of days to offset from today

**Returns**

**Type**

**Description**

The date of the offset day.

**Examples**

{{time.days_ago $input}} => Sunday, 7 May, 2023

**hour**

`day_of_week() -> str`

`days_ago(days: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



Get the current hour.

Python

**Examples**

{{time.hour}} => 9 PM

**hour_number**

Get the current hour number.

Python

**Examples**

{{time.hourNumber}} => 21

**iso_date**

Get the current date in iso format.

Python

**Examples**

{{time.iso_date}} => 2031-01-12

**minute**

Get the current minute.

Python

`hour() -> str`

`hour_number() -> str`

`iso_date() -> str`



**Examples**

{{time.minute}} => 15

**month**

Get the current month.

Python

**Examples**

{{time.month}} => January

**month_number**

Get the current month number.

Python

**Examples**

{{time.monthNumber}} => 01

**now**

Get the current date and time in the local time zone.

Python

**Examples**

`minute() -> str`

`month() -> str`

`month_number() -> str`

`now() -> str`



{{time.now}} => Sunday, January 12, 2031 9:15 PM

**second**

Get the seconds on the current minute.

Python

**Examples**

{{time.second}} => 7

**time**

Get the current time in the local time zone.

Python

**Examples**

{{time.time}} => 09:15:07 PM

**time_zone_name**

Get the current time zone name.

Python

**Examples**

{{time.timeZoneName}} => PST

**time_zone_offset**

`second() -> str`

`time() -> str`

`time_zone_name() -> str`



Get the current time zone offset.

Python

**Examples**

{{time.timeZoneOffset}} => -08:00

**today**

Get the current date.

Python

**Examples**

{{time.today}} => Sunday, 12 January, 2031

**utc_now**

Get the current date and time in UTC.

Python

**Examples**

{{time.utcNow}} => Sunday, January 13, 2031 5:15 AM

**year**

Get the current year.

Python

`time_zone_offset() -> str`

`today() -> str`

`utc_now() -> str`



**Examples**

{{time.year}} => 2031

`year() -> str`



**WebSearchEnginePlugin Class**

Reference

A plugin that provides web search engine functionality.

Usage: connector = BingConnector(bing_search_api_key)

kernel.add_plugin(WebSearchEnginePlugin(connector), plugin_name="WebSearch")

Initializes a new instance of the WebSearchEnginePlugin class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**connector**`

Required*

**Examples**

{{WebSearch.search "What is semantic kernel?"}} => Returns the first
_num_results_

number of results for the given search query

and ignores the first _offset_ number of results.

search

Returns the search results of the query provided.

**search**

`WebSearchEnginePlugin(connector: ConnectorBase)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Returns the search results of the query provided.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**num_results**`

Required*

Default value: 1

`**offset**`

Required*

Default value: 0

`async`` search(query: Annotated[str, ``'The search query'``], num_results: `

`Annotated[int, ``'The number of search results to return'``] = 1, offset: `

`Annotated[int, ``'The number of search results to skip'``] = 0) -> list[str]`

ﾉ

**Expand table**



**data Package**

Reference

**Packages**

filter_clauses

record_definition

text_search

vector_search

vector_storage

**Modules**

const

kernel_search_results

search_filter

search_options

**Classes**

AnyTagsEqualTo

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags.

value: The value to compare against the list of tags.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

EqualTo

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The

value to compare against the field.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelSearchResults

The result of a kernel search.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

OptionsUpdateFunctionType

Type definition for the options update function in Text Search.

SearchOptions

Options for a search.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TextSearch

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in

the future.



TextSearchFilter

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in

the future.

Initialize a new instance of SearchFilter.

TextSearchOptions

Options for a text search.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TextSearchResult

The result of a text search.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorSearchBase

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorSearchFilter

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in

the future.

Initialize a new instance of VectorSearchFilter.



VectorSearchOptions

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorSearchResult

The result of a vector search.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorStore

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorStoreRecordCollection

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in

the future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorStoreRecordDataField

Memory record data field.



Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordDefinition

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether

the record is in container mode. to_dict: The to_dict function,

should take a record and return a list of dicts. from_dict: The

from_dict function, should take a list of dicts and return a record.

serialize: The serialize function, should take a record and return

the type specific to a datastore. deserialize: The deserialize

function, should take a type specific to a datastore and return a

record.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordKeyField

Memory record key field.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordUtils

Helper class to easily add embeddings to a (set of) vector store

record.

Note: This class is marked as 'experimental' and may change in

the future.

Initializes the VectorStoreRecordUtils with a kernel.

VectorStoreRecordVectorField

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors.

This is the default and all vector stores in SK use this internally.

But in your class you may want to use a numpy array or some

other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and

returns the optimized type, and then also supply a

serialize_function that takes the optimized type and returns a list

of floats.

For instance for numpy, that would be

_serialize_function=np.ndarray.tolist_ and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top

of your file). if you want to set it up with more specific options,

use a lambda, a custom function or a partial.

Args: property_type (str, optional): Property type. For vectors this

should be the inner type of the vector. By default the vector will

be a list of numbers. If you want to use a numpy array or some

other optimized format, set the cast_function with a function that

takes a list of floats and returns a numpy array.



Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreTextSearch

Class that wraps a Vector Store Record Collection to expose as a

Text Search.

Preferably the class methods are used to create an instance of

this class. Otherwise the search executes in the following order

depending on which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorTextSearchMixin

The mixin for text search, to be used in combination with

VectorSearchBase.

Note: This class is marked as 'experimental' and may change in

the future.

VectorizableTextSearchMixin

The mixin for searching with text that get's vectorized

downstream.

` local_embedding (bool, optional): Whether to `

`embed the vector locally. Defaults to True.`

` embedding_settings (dict[str, `

`PromptExecutionSettings], optional): Embedding `

`settings.`

` The key is the name of the embedding `

`service to use, can be multiple ones.`

` serialize_function (Callable[[Any], list[float `

`| int]], optional): Serialize function,`

` should take the vector and return a list of `

`numbers.`

` deserialize_function (Callable[[list[float | `

`int]], Any], optional): Deserialize function,`

` should take a list of numbers and return `

`the vector.`



To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in

the future.

VectorizedSearchMixin

The mixin for searching with vectors. To be used in combination

with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in

the future.

**Enums**

DistanceFunction

Distance functions for similarity search.

Cosine Similarity the cosine (angular) similarity between two vectors measures

only the angle between the two vectors, without taking into account the

length of the vectors Cosine Similarity = 1 - Cosine Distance -1 means vectors

are opposite 0 means vectors are orthogonal 1 means vectors are identical

Cosine Distance the cosine (angular) distance between two vectors measures

only the angle between the two vectors, without taking into account the

length of the vectors Cosine Distance = 1 - Cosine Similarity 2 means vectors

are opposite 1 means vectors are orthogonal 0 means vectors are identical

Dot Product measures both the length and angle between two vectors same

as cosine similarity if the vectors are the same length, but more performant

Euclidean Distance measures the Euclidean distance between two vectors also

known as l2-norm

Euclidean Squared Distance measures the Euclidean squared distance

between two vectors also known as l2-squared

Manhattan measures the Manhattan distance between two vectors

Hamming number of differences between vectors at each dimensions

IndexKind

Index kinds for similarity search.

HNSW Hierarchical Navigable Small World which performs an approximate

nearest neighbor (ANN) search. Lower accuracy than exhaustive k nearest

neighbor, but faster and more efficient.

Flat Does a brute force search to find the nearest neighbors. Calculates the

distances between all pairs of data points, so has a linear time complexity,
that

grows directly proportional to the number of points. Also referred to as

ﾉ

**Expand table**



exhaustive k nearest neighbor in some databases. High recall accuracy, but

slower and more expensive than HNSW. Better with smaller datasets.

IVF Flat Inverted File with Flat Compression. Designed to enhance search

efficiency by narrowing the search area through the use of neighbor partitions

or clusters. Also referred to as approximate nearest neighbor (ANN) search.

Disk ANN Disk-based Approximate Nearest Neighbor algorithm designed for

efficiently searching for approximate nearest neighbors (ANN) in high-

dimensional spaces. The primary focus of DiskANN is to handle large-scale

datasets that cannot fit entirely into memory, leveraging disk storage to
store

the data while maintaining fast search times.

Quantized Flat Index that compresses vectors using DiskANN-based

quantization methods for better efficiency in the kNN search.

Dynamic Dynamic index allows to automatically switch from FLAT to HNSW

indexes.

**create_options**

Create search options.

If options are supplied, they are checked for the right type, and the kwargs
are used

to update the options.

If options are not supplied, they are created from the kwargs. If that fails,
an empty

options object is returned.

Python

**Parameters**

**Name**

**Description**

`**options_class**`

Required*

The class of the options.

**Functions**

`create_options(options_class: type[SearchOptions], options: SearchOptions `

`| ``None``, **kwargs: Any) -> SearchOptions`

ﾉ

**Expand table**



**Name**

**Description**

`**options**`

Required*

The existing options to update.

`****kwargs**`

Required*

The keyword arguments to use to create the options.

**Returns**

**Type**

**Description**

SearchOptions

The options.

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

**default_options_update_function**

The default options update function.

This function is used to update the query and options with the kwargs. You can

supply your own version of this function to customize the behavior.

Python

**Parameters**

ﾉ

**Expand table**

ﾉ

**Expand table**

`default_options_update_function(query: str, options: SearchOptions, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, **kwargs: Any) -`

`> tuple[str, SearchOptions]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

The query.

`**options**`

Required*

The options.

`**parameters**`

Required*

The parameters to use to create the options.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to update the options.

**Returns**

**Type**

**Description**

tuple[str, SearchOptions]

The updated query and options

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

**vectorstoremodel**

Returns the class as a vector store model.

This decorator makes a class a vector store model. There are three things
being

checked:

The class must have at least one field with a annotation,

of type VectorStoreRecordKeyField, VectorStoreRecordDataField or

VectorStoreRecordVectorField.

The class must have exactly one field with the VectorStoreRecordKeyField

annotation.

ﾉ

**Expand table**

ﾉ

**Expand table**



A field with multiple VectorStoreRecordKeyField annotations will be set to the

first one found.

Optionally, when there are VectorStoreRecordDataFields that specify a
embedding

property name, there must be a corresponding VectorStoreRecordVectorField with

the same name.

Args: cls: The class to be decorated.

Raises: VectorStoreModelException: If the class does not implement the
serialize and

deserialize methods. VectorStoreModelException: If there are no fields with a

VectorStoreRecordField annotation. VectorStoreModelException: If there are
fields

with no name. VectorStoreModelException: If there is no key field.

VectorStoreModelException: If there is a field with an embedding property name
but

no corresponding field. VectorStoreModelException: If there is a ndarray field

without a serialize or deserialize function.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**cls**`

Default value: None

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

`vectorstoremodel(cls: Any | ``None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**filter_clauses Package**

Reference

**Modules**

any_tags_equal_to_filter_clause

equal_to_filter_clause

filter_clause_base

**Classes**

AnyTagsEqualTo

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags. value:
The

value to compare against the list of tags.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

EqualTo

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The value to

compare against the field.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**







**any_tags_equal_to_filter_clause Module**

Reference

**Classes**

AnyTagsEqualTo

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags. value:
The

value to compare against the list of tags.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AnyTagsEqualTo Class**

Reference

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags. value:
The value to

compare against the list of tags.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**field_name**

Python

`AnyTagsEqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**filter_clause_type**

Python

**is_experimental**

Python

**stage_status**

Python

**value**

Python

`field_name: str`

`filter_clause_type: ClassVar[str] = ``'any_tags_equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`value: Any`



**equal_to_filter_clause Module**

Reference

**Classes**

EqualTo

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The value to
compare

against the field.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**EqualTo Class**

Reference

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The value to
compare against

the field.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**field_name**

Python

`EqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**filter_clause_type**

Python

**is_experimental**

Python

**stage_status**

Python

**value**

Python

`field_name: str`

`filter_clause_type: ClassVar[str] = ``'equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`value: Any`



**filter_clause_base Module**

Reference

**Classes**

FilterClauseBase

A base for all filter clauses.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FilterClauseBase Class**

Reference

A base for all filter clauses.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**field_name**

Python

`FilterClauseBase(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**

`field_name: str`



**filter_clause_type**

Python

**is_experimental**

Python

**stage_status**

Python

**value**

Python

`filter_clause_type: ClassVar[str] = ``'FilterClauseBase'`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`value: Any`



**AnyTagsEqualTo Class**

Reference

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags. value:
The value to

compare against the list of tags.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**field_name**

Python

`AnyTagsEqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**filter_clause_type**

Python

**is_experimental**

Python

**stage_status**

Python

**value**

Python

`field_name: str`

`filter_clause_type: ClassVar[str] = ``'any_tags_equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`value: Any`



**EqualTo Class**

Reference

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The value to
compare against

the field.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**field_name**

Python

`EqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**filter_clause_type**

Python

**is_experimental**

Python

**stage_status**

Python

**value**

Python

`field_name: str`

`filter_clause_type: ClassVar[str] = ``'equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`value: Any`



**record_definition Package**

Reference

**Modules**

vector_store_model_decorator

vector_store_model_definition

vector_store_model_protocols

vector_store_record_fields

vector_store_record_utils

**Classes**

VectorStoreRecordDataField

Memory record data field.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordDefinition

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether

the record is in container mode. to_dict: The to_dict function,

should take a record and return a list of dicts. from_dict: The

from_dict function, should take a list of dicts and return a record.

serialize: The serialize function, should take a record and return

the type specific to a datastore. deserialize: The deserialize

function, should take a type specific to a datastore and return a

record.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordKeyField

Memory record key field.

Note: This class is marked as 'experimental' and may change in

the future.

ﾉ

**Expand table**

ﾉ

**Expand table**



VectorStoreRecordUtils

Helper class to easily add embeddings to a (set of) vector store

record.

Note: This class is marked as 'experimental' and may change in

the future.

Initializes the VectorStoreRecordUtils with a kernel.

VectorStoreRecordVectorField

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors.

This is the default and all vector stores in SK use this internally.

But in your class you may want to use a numpy array or some

other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and

returns the optimized type, and then also supply a

serialize_function that takes the optimized type and returns a list

of floats.

For instance for numpy, that would be

_serialize_function=np.ndarray.tolist_ and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top

of your file). if you want to set it up with more specific options,

use a lambda, a custom function or a partial.

Args: property_type (str, optional): Property type. For vectors this

should be the inner type of the vector. By default the vector will

be a list of numbers. If you want to use a numpy array or some

other optimized format, set the cast_function with a function that

takes a list of floats and returns a numpy array.

` local_embedding (bool, optional): Whether to `

`embed the vector locally. Defaults to True.`

` embedding_settings (dict[str, `

`PromptExecutionSettings], optional): Embedding `

`settings.`

` The key is the name of the embedding `

`service to use, can be multiple ones.`

` serialize_function (Callable[[Any], list[float `

`| int]], optional): Serialize function,`

` should take the vector and return a list of `

`numbers.`

` deserialize_function (Callable[[list[float | `

`int]], Any], optional): Deserialize function,`

` should take a list of numbers and return `

`the vector.`



Note: This class is marked as 'experimental' and may change in

the future.

**vectorstoremodel**

Returns the class as a vector store model.

This decorator makes a class a vector store model. There are three things
being

checked:

The class must have at least one field with a annotation,

of type VectorStoreRecordKeyField, VectorStoreRecordDataField or

VectorStoreRecordVectorField.

The class must have exactly one field with the VectorStoreRecordKeyField

annotation.

A field with multiple VectorStoreRecordKeyField annotations will be set to the

first one found.

Optionally, when there are VectorStoreRecordDataFields that specify a
embedding

property name, there must be a corresponding VectorStoreRecordVectorField with

the same name.

Args: cls: The class to be decorated.

Raises: VectorStoreModelException: If the class does not implement the
serialize and

deserialize methods. VectorStoreModelException: If there are no fields with a

VectorStoreRecordField annotation. VectorStoreModelException: If there are
fields

with no name. VectorStoreModelException: If there is no key field.

VectorStoreModelException: If there is a field with an embedding property name
but

no corresponding field. VectorStoreModelException: If there is a ndarray field

without a serialize or deserialize function.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Functions**

`vectorstoremodel(cls: Any | ``None`` = ``None``)`



**Parameters**

**Name**

**Description**

`**cls**`

Default value: None

ﾉ

**Expand table**



**vector_store_model_decorator Module**

Reference

**vectorstoremodel**

Returns the class as a vector store model.

This decorator makes a class a vector store model. There are three things
being

checked:

The class must have at least one field with a annotation,

of type VectorStoreRecordKeyField, VectorStoreRecordDataField or

VectorStoreRecordVectorField.

The class must have exactly one field with the VectorStoreRecordKeyField

annotation.

A field with multiple VectorStoreRecordKeyField annotations will be set to the

first one found.

Optionally, when there are VectorStoreRecordDataFields that specify a
embedding

property name, there must be a corresponding VectorStoreRecordVectorField with

the same name.

Args: cls: The class to be decorated.

Raises: VectorStoreModelException: If the class does not implement the
serialize and

deserialize methods. VectorStoreModelException: If there are no fields with a

VectorStoreRecordField annotation. VectorStoreModelException: If there are
fields

with no name. VectorStoreModelException: If there is no key field.

VectorStoreModelException: If there is a field with an embedding property name
but

no corresponding field. VectorStoreModelException: If there is a ndarray field

without a serialize or deserialize function.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Functions**

`vectorstoremodel(cls: Any | ``None`` = ``None``)`



**Parameters**

**Name**

**Description**

`**cls**`

Default value: None

ﾉ

**Expand table**



**vector_store_model_definition Module**

Reference

**Classes**

VectorStoreRecordDefinition

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether the

record is in container mode. to_dict: The to_dict function, should

take a record and return a list of dicts. from_dict: The from_dict

function, should take a list of dicts and return a record. serialize:

The serialize function, should take a record and return the type

specific to a datastore. deserialize: The deserialize function, should

take a type specific to a datastore and return a record.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**VectorStoreRecordDefinition Class**

Reference

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether the record is
in container

mode. to_dict: The to_dict function, should take a record and return a list of
dicts.

from_dict: The from_dict function, should take a list of dicts and return a
record.

serialize: The serialize function, should take a record and return the type
specific to a

datastore. deserialize: The deserialize function, should take a type specific
to a datastore

and return a record.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

`**container_mode**`

Default value: False

`**to_dict**`

Default value: None

`**from_dict**`

Default value: None

`**serialize**`

Default value: None

`**deserialize**`

Default value: None

`VectorStoreRecordDefinition(fields: dict[str, VectorStoreRecordFields], `

`container_mode: bool = ``False``, to_dict: ToDictFunctionProtocol | ``None`` = ``None``, `

`from_dict: FromDictFunctionProtocol | ``None`` = ``None``, serialize: `

`SerializeFunctionProtocol | ``None`` = ``None``, deserialize: `

`DeserializeFunctionProtocol | ``None`` = ``None``)`

ﾉ

**Expand table**



get_field_names

Get the names of the fields.

try_get_vector_field

Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector

fields are present None is returned.

**get_field_names**

Get the names of the fields.

Python

**Parameters**

**Name**

**Description**

`**include_vector_fields**`

Whether to include vector fields.

Default value: True

`**include_key_field**`

Whether to include the key field.

Default value: True

**Returns**

**Type**

**Description**

list[str]

The names of the fields.

**try_get_vector_field**

**Methods**

ﾉ

**Expand table**

`get_field_names(include_vector_fields: bool = ``True``, include_key_field: `

`bool = ``True``) -> list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector fields are

present None is returned.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

The field name.

Default value: None

**Returns**

**Type**

**Description**

VectorStoreRecordVectorField | None

The vector field or None.

**field_names**

Get the names of the fields.

**key_field**

Get the key field.

**non_vector_field_names**

Get the names of all the non-vector fields.

`try_get_vector_field(field_name: str | ``None`` = ``None``) -> `

`VectorStoreRecordVectorField | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**vector_field_names**

Get the names of the vector fields.

**vector_fields**

Get the names of the vector fields.

**container_mode**

Python

**deserialize**

Python

**fields**

Python

**from_dict**

Python

**is_experimental**

Python

`container_mode: bool = ``False`

`deserialize: DeserializeFunctionProtocol | ``None`` = ``None`

`fields: dict[str, VectorStoreRecordFields]`

`from_dict: FromDictFunctionProtocol | ``None`` = ``None`

`is_experimental = ``True`



**key_field_name**

Python

**serialize**

Python

**stage_status**

Python

**to_dict**

Python

`key_field_name: str`

`serialize: SerializeFunctionProtocol | ``None`` = ``None`

`stage_status = ``'experimental'`

`to_dict: ToDictFunctionProtocol | ``None`` = ``None`



**vector_store_model_protocols Module**

Reference

**Classes**

DeserializeFunctionProtocol

Protocol for deserialize function.

Args: records: The serialized record directly from the store. >>**

<<kwargs: Additional keyword arguments.

Returns: The deserialized record in the format expected by the

application.

Note: This class is marked as 'experimental' and may change in the

future.

FromDictFunctionProtocol

Protocol for from_dict function.

Args: records: A list of dictionaries. >>**<<kwargs: Additional

keyword arguments.

Returns: A record or list thereof.

Note: This class is marked as 'experimental' and may change in the

future.

SerializeFunctionProtocol

Protocol for serialize function.

Args: record: The record to be serialized. >>**<<kwargs: Additional

keyword arguments.

Returns: The serialized record, ready to be consumed by the specific

store.

Note: This class is marked as 'experimental' and may change in the

future.

SerializeMethodProtocol

Data model serialization protocol.

This can optionally be implemented to allow single step

serialization and deserialization for using your data model with a

specific datastore.

Note: This class is marked as 'experimental' and may change in the

future.

ToDictFunctionProtocol

Protocol for to_dict function.

ﾉ

**Expand table**



Args: record: The record to be serialized. >>**<<kwargs: Additional

keyword arguments.

Returns: A list of dictionaries.

Note: This class is marked as 'experimental' and may change in the

future.

ToDictMethodProtocol

Class used internally to check if a model has a to_dict method.

Note: This class is marked as 'experimental' and may change in the

future.



**DeserializeFunctionProtocol Class**

Reference

Protocol for deserialize function.

Args: records: The serialized record directly from the store. >>**<<kwargs:
Additional

keyword arguments.

Returns: The deserialized record in the format expected by the application.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**is_experimental**

Python

**stage_status**

Python

`DeserializeFunctionProtocol(*args, **kwargs)`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**FromDictFunctionProtocol Class**

Reference

Protocol for from_dict function.

Args: records: A list of dictionaries. >>**<<kwargs: Additional keyword
arguments.

Returns: A record or list thereof.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**is_experimental**

Python

**stage_status**

Python

`FromDictFunctionProtocol(*args, **kwargs)`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**SerializeFunctionProtocol Class**

Reference

Protocol for serialize function.

Args: record: The record to be serialized. >>**<<kwargs: Additional keyword
arguments.

Returns: The serialized record, ready to be consumed by the specific store.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**is_experimental**

Python

**stage_status**

Python

`SerializeFunctionProtocol(*args, **kwargs)`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**SerializeMethodProtocol Class**

Reference

Data model serialization protocol.

This can optionally be implemented to allow single step serialization and
deserialization

for using your data model with a specific datastore.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

serialize

Serialize the object to the format required by the data store.

**serialize**

Serialize the object to the format required by the data store.

Python

**is_experimental**

Python

`SerializeMethodProtocol(*args, **kwargs)`

**Methods**

ﾉ

**Expand table**

`serialize(**kwargs: Any) -> Any`

**Attributes**

`is_experimental = ``True`



**stage_status**

Python

`stage_status = ``'experimental'`



**ToDictFunctionProtocol Class**

Reference

Protocol for to_dict function.

Args: record: The record to be serialized. >>**<<kwargs: Additional keyword
arguments.

Returns: A list of dictionaries.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**is_experimental**

Python

**stage_status**

Python

`ToDictFunctionProtocol(*args, **kwargs)`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**ToDictMethodProtocol Class**

Reference

Class used internally to check if a model has a to_dict method.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

to_dict

Serialize the object to the format required by the data store.

**to_dict**

Serialize the object to the format required by the data store.

Python

**is_experimental**

Python

**stage_status**

`ToDictMethodProtocol(*args, **kwargs)`

**Methods**

ﾉ

**Expand table**

`to_dict(*args: Any, **kwargs: Any) -> dict[str, Any]`

**Attributes**

`is_experimental = ``True`



Python

`stage_status = ``'experimental'`



**vector_store_record_fields Module**

Reference

**Classes**

VectorStoreRecordDataField

Memory record data field.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordKeyField

Memory record key field.

Note: This class is marked as 'experimental' and may change in

the future.

VectorStoreRecordVectorField

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors.

This is the default and all vector stores in SK use this internally.

But in your class you may want to use a numpy array or some

other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and

returns the optimized type, and then also supply a

serialize_function that takes the optimized type and returns a list

of floats.

For instance for numpy, that would be

_serialize_function=np.ndarray.tolist_ and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top

of your file). if you want to set it up with more specific options,

use a lambda, a custom function or a partial.

Args: property_type (str, optional): Property type. For vectors this

should be the inner type of the vector. By default the vector will

be a list of numbers. If you want to use a numpy array or some

other optimized format, set the cast_function with a function that

takes a list of floats and returns a numpy array.

ﾉ

**Expand table**

` local_embedding (bool, optional): Whether to `

`embed the vector locally. Defaults to True.`

` embedding_settings (dict[str, `

`PromptExecutionSettings], optional): Embedding `

`settings.`



Note: This class is marked as 'experimental' and may change in

the future.

` The key is the name of the embedding `

`service to use, can be multiple ones.`

` serialize_function (Callable[[Any], list[float `

`| int]], optional): Serialize function,`

` should take the vector and return a list of `

`numbers.`

` deserialize_function (Callable[[list[float | `

`int]], Any], optional): Deserialize function,`

` should take a list of numbers and return `

`the vector.`



**VectorStoreRecordDataField Class**

Reference

Memory record data field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**has_embedding**`

Default value: False

`**embedding_property_name**`

Default value: None

`**is_filterable**`

Default value: None

`**is_full_text_searchable**`

Default value: None

**embedding_property_name**

Python

`VectorStoreRecordDataField(name: str = ``''``, property_type: str | ``None`` = ``None``, `

`has_embedding: bool = ``False``, embedding_property_name: str | ``None`` = ``None``, `

`is_filterable: bool | ``None`` = ``None``, is_full_text_searchable: bool | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Attributes**

`embedding_property_name: str | ``None`` = ``None`



**has_embedding**

Python

**is_experimental**

Python

**is_filterable**

Python

**is_full_text_searchable**

Python

**stage_status**

Python

`has_embedding: bool = ``False`

`is_experimental = ``True`

`is_filterable: bool | ``None`` = ``None`

`is_full_text_searchable: bool | ``None`` = ``None`

`stage_status = ``'experimental'`



**VectorStoreRecordKeyField Class**

Reference

Memory record key field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

**is_experimental**

Python

**stage_status**

Python

`VectorStoreRecordKeyField(name: str = ``''``, property_type: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordVectorField Class**

Reference

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors. This is
the default and all

vector stores in SK use this internally. But in your class you may want to use
a numpy

array or some other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and returns the
optimized

type, and then also supply a serialize_function that takes the optimized type
and returns

a list of floats.

For instance for numpy, that would be _serialize_function=np.ndarray.tolist_
and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top of
your file). if you

want to set it up with more specific options, use a lambda, a custom function
or a

partial.

Args: property_type (str, optional): Property type. For vectors this should be
the inner

type of the vector. By default the vector will be a list of numbers. If you
want to use a

numpy array or some other optimized format, set the cast_function with a
function that

takes a list of floats and returns a numpy array.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

` local_embedding (bool, optional): Whether to embed the vector locally. `

`Defaults to True.`

` embedding_settings (dict[str, PromptExecutionSettings], optional): `

`Embedding settings.`

` The key is the name of the embedding service to use, can be multiple `

`ones.`

` serialize_function (Callable[[Any], list[float | int]], optional): `

`Serialize function,`

` should take the vector and return a list of numbers.`

` deserialize_function (Callable[[list[float | int]], Any], optional): `

`Deserialize function,`

` should take a list of numbers and return the vector.`



Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**local_embedding**`

Default value: True

`**dimensions**`

Default value: None

`**index_kind**`

Default value: None

`**distance_function**`

Default value: None

`**embedding_settings**`

Default value: <factory>

`**serialize_function**`

Default value: None

`**deserialize_function**`

Default value: None

**deserialize_function**

Python

`VectorStoreRecordVectorField(name: str = ``''``, property_type: str | ``None`` = `

`None``, local_embedding: bool = ``True``, dimensions: int | ``None`` = ``None``, `

`index_kind: ~semantic_kernel.data.const.IndexKind | ``None`` = ``None``, `

`distance_function: ~semantic_kernel.data.const.DistanceFunction | ``None`` = `

`None``, embedding_settings: dict[str, `

`~semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSett`

`ings] = <factory>, serialize_function: `

`~collections.abc.Callable[[~typing.Any], list[float | int]] | ``None`` = ``None``, `

`deserialize_function: ~collections.abc.Callable[[list[float | int]], `

`~typing.Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`deserialize_function: Callable[[list[float | int]], Any] | ``None`` = ``None`



**dimensions**

Python

**distance_function**

Python

**embedding_settings**

Python

**index_kind**

Python

**is_experimental**

Python

**local_embedding**

Python

`dimensions: int | ``None`` = ``None`

`distance_function: DistanceFunction | ``None`` = ``None`

`embedding_settings: dict[str, PromptExecutionSettings] = `

`FieldInfo(annotation=dict[str, PromptExecutionSettings], required=``False``,
`

`default_factory=dict)`

`index_kind: IndexKind | ``None`` = ``None`

`is_experimental = ``True`

`local_embedding: bool = ``True`



**serialize_function**

Python

**stage_status**

Python

`serialize_function: Callable[[Any], list[float | int]] | ``None`` = ``None`

`stage_status = ``'experimental'`



**vector_store_record_utils Module**

Reference

**Classes**

VectorStoreRecordUtils

Helper class to easily add embeddings to a (set of) vector store record.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes the VectorStoreRecordUtils with a kernel.

ﾉ

**Expand table**



**VectorStoreRecordUtils Class**

Reference

Helper class to easily add embeddings to a (set of) vector store record.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the VectorStoreRecordUtils with a kernel.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

add_vector_to_records

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data

fields, if they have a vector field, looks up that vector field and checks if

is a local embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings

to make.

`VectorStoreRecordUtils(kernel: Kernel)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Optional arguments are passed onto the Kernel

add_embedding_to_object call.

**add_vector_to_records**

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data fields,
if they

have a vector field, looks up that vector field and checks if is a local
embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings to
make.

Optional arguments are passed onto the Kernel add_embedding_to_object call.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

`**data_model_type**`

Required*

Default value: None

`**data_model_definition**`

Required*

Default value: None

**is_experimental**

`async`` add_vector_to_records(records: TModel | Sequence[TModel], `

`data_model_type: type[TModel] | ``None`` = ``None``, data_model_definition: `

`VectorStoreRecordDefinition | ``None`` = ``None``, **kwargs) -> TModel | `

`Sequence[TModel]`

ﾉ

**Expand table**

**Attributes**



Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordDataField Class**

Reference

Memory record data field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**has_embedding**`

Default value: False

`**embedding_property_name**`

Default value: None

`**is_filterable**`

Default value: None

`**is_full_text_searchable**`

Default value: None

**embedding_property_name**

Python

`VectorStoreRecordDataField(name: str = ``''``, property_type: str | ``None`` = ``None``, `

`has_embedding: bool = ``False``, embedding_property_name: str | ``None`` = ``None``, `

`is_filterable: bool | ``None`` = ``None``, is_full_text_searchable: bool | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Attributes**

`embedding_property_name: str | ``None`` = ``None`



**has_embedding**

Python

**is_experimental**

Python

**is_filterable**

Python

**is_full_text_searchable**

Python

**stage_status**

Python

`has_embedding: bool = ``False`

`is_experimental = ``True`

`is_filterable: bool | ``None`` = ``None`

`is_full_text_searchable: bool | ``None`` = ``None`

`stage_status = ``'experimental'`



**VectorStoreRecordDefinition Class**

Reference

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether the record is
in container

mode. to_dict: The to_dict function, should take a record and return a list of
dicts.

from_dict: The from_dict function, should take a list of dicts and return a
record.

serialize: The serialize function, should take a record and return the type
specific to a

datastore. deserialize: The deserialize function, should take a type specific
to a datastore

and return a record.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

`**container_mode**`

Default value: False

`**to_dict**`

Default value: None

`**from_dict**`

Default value: None

`**serialize**`

Default value: None

`**deserialize**`

Default value: None

`VectorStoreRecordDefinition(fields: dict[str, VectorStoreRecordFields], `

`container_mode: bool = ``False``, to_dict: ToDictFunctionProtocol | ``None`` = ``None``, `

`from_dict: FromDictFunctionProtocol | ``None`` = ``None``, serialize: `

`SerializeFunctionProtocol | ``None`` = ``None``, deserialize: `

`DeserializeFunctionProtocol | ``None`` = ``None``)`

ﾉ

**Expand table**



get_field_names

Get the names of the fields.

try_get_vector_field

Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector

fields are present None is returned.

**get_field_names**

Get the names of the fields.

Python

**Parameters**

**Name**

**Description**

`**include_vector_fields**`

Whether to include vector fields.

Default value: True

`**include_key_field**`

Whether to include the key field.

Default value: True

**Returns**

**Type**

**Description**

list[str]

The names of the fields.

**try_get_vector_field**

**Methods**

ﾉ

**Expand table**

`get_field_names(include_vector_fields: bool = ``True``, include_key_field: `

`bool = ``True``) -> list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector fields are

present None is returned.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

The field name.

Default value: None

**Returns**

**Type**

**Description**

VectorStoreRecordVectorField | None

The vector field or None.

**field_names**

Get the names of the fields.

**key_field**

Get the key field.

**non_vector_field_names**

Get the names of all the non-vector fields.

`try_get_vector_field(field_name: str | ``None`` = ``None``) -> `

`VectorStoreRecordVectorField | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**vector_field_names**

Get the names of the vector fields.

**vector_fields**

Get the names of the vector fields.

**container_mode**

Python

**deserialize**

Python

**fields**

Python

**from_dict**

Python

**is_experimental**

Python

`container_mode: bool = ``False`

`deserialize: DeserializeFunctionProtocol | ``None`` = ``None`

`fields: dict[str, VectorStoreRecordFields]`

`from_dict: FromDictFunctionProtocol | ``None`` = ``None`

`is_experimental = ``True`



**key_field_name**

Python

**serialize**

Python

**stage_status**

Python

**to_dict**

Python

`key_field_name: str`

`serialize: SerializeFunctionProtocol | ``None`` = ``None`

`stage_status = ``'experimental'`

`to_dict: ToDictFunctionProtocol | ``None`` = ``None`



**VectorStoreRecordKeyField Class**

Reference

Memory record key field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

**is_experimental**

Python

**stage_status**

Python

`VectorStoreRecordKeyField(name: str = ``''``, property_type: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordUtils Class**

Reference

Helper class to easily add embeddings to a (set of) vector store record.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the VectorStoreRecordUtils with a kernel.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

add_vector_to_records

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data

fields, if they have a vector field, looks up that vector field and checks if

is a local embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings

to make.

`VectorStoreRecordUtils(kernel: Kernel)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Optional arguments are passed onto the Kernel

add_embedding_to_object call.

**add_vector_to_records**

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data fields,
if they

have a vector field, looks up that vector field and checks if is a local
embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings to
make.

Optional arguments are passed onto the Kernel add_embedding_to_object call.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

`**data_model_type**`

Required*

Default value: None

`**data_model_definition**`

Required*

Default value: None

**is_experimental**

`async`` add_vector_to_records(records: TModel | Sequence[TModel], `

`data_model_type: type[TModel] | ``None`` = ``None``, data_model_definition: `

`VectorStoreRecordDefinition | ``None`` = ``None``, **kwargs) -> TModel | `

`Sequence[TModel]`

ﾉ

**Expand table**

**Attributes**



Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordVectorField Class**

Reference

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors. This is
the default and all

vector stores in SK use this internally. But in your class you may want to use
a numpy

array or some other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and returns the
optimized

type, and then also supply a serialize_function that takes the optimized type
and returns

a list of floats.

For instance for numpy, that would be _serialize_function=np.ndarray.tolist_
and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top of
your file). if you

want to set it up with more specific options, use a lambda, a custom function
or a

partial.

Args: property_type (str, optional): Property type. For vectors this should be
the inner

type of the vector. By default the vector will be a list of numbers. If you
want to use a

numpy array or some other optimized format, set the cast_function with a
function that

takes a list of floats and returns a numpy array.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

` local_embedding (bool, optional): Whether to embed the vector locally. `

`Defaults to True.`

` embedding_settings (dict[str, PromptExecutionSettings], optional): `

`Embedding settings.`

` The key is the name of the embedding service to use, can be multiple `

`ones.`

` serialize_function (Callable[[Any], list[float | int]], optional): `

`Serialize function,`

` should take the vector and return a list of numbers.`

` deserialize_function (Callable[[list[float | int]], Any], optional): `

`Deserialize function,`

` should take a list of numbers and return the vector.`



Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**local_embedding**`

Default value: True

`**dimensions**`

Default value: None

`**index_kind**`

Default value: None

`**distance_function**`

Default value: None

`**embedding_settings**`

Default value: <factory>

`**serialize_function**`

Default value: None

`**deserialize_function**`

Default value: None

**deserialize_function**

Python

`VectorStoreRecordVectorField(name: str = ``''``, property_type: str | ``None`` = `

`None``, local_embedding: bool = ``True``, dimensions: int | ``None`` = ``None``, `

`index_kind: ~semantic_kernel.data.const.IndexKind | ``None`` = ``None``, `

`distance_function: ~semantic_kernel.data.const.DistanceFunction | ``None`` = `

`None``, embedding_settings: dict[str, `

`~semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSett`

`ings] = <factory>, serialize_function: `

`~collections.abc.Callable[[~typing.Any], list[float | int]] | ``None`` = ``None``, `

`deserialize_function: ~collections.abc.Callable[[list[float | int]], `

`~typing.Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`deserialize_function: Callable[[list[float | int]], Any] | ``None`` = ``None`



**dimensions**

Python

**distance_function**

Python

**embedding_settings**

Python

**index_kind**

Python

**is_experimental**

Python

**local_embedding**

Python

`dimensions: int | ``None`` = ``None`

`distance_function: DistanceFunction | ``None`` = ``None`

`embedding_settings: dict[str, PromptExecutionSettings] = `

`FieldInfo(annotation=dict[str, PromptExecutionSettings], required=``False``,
`

`default_factory=dict)`

`index_kind: IndexKind | ``None`` = ``None`

`is_experimental = ``True`

`local_embedding: bool = ``True`



**serialize_function**

Python

**stage_status**

Python

`serialize_function: Callable[[Any], list[float | int]] | ``None`` = ``None`

`stage_status = ``'experimental'`



**text_search Package**

Reference

**Modules**

text_search

text_search_filter

text_search_options

text_search_result

utils

vector_store_text_search

**Classes**

OptionsUpdateFunctionType

Type definition for the options update function in Text Search.

TextSearch

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in the

future.

TextSearchFilter

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize a new instance of SearchFilter.

TextSearchOptions

Options for a text search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**



Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

TextSearchResult

The result of a text search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorStoreTextSearch

Class that wraps a Vector Store Record Collection to expose as a

Text Search.

Preferably the class methods are used to create an instance of this

class. Otherwise the search executes in the following order

depending on which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**create_options**

Create search options.

If options are supplied, they are checked for the right type, and the kwargs
are used

to update the options.

If options are not supplied, they are created from the kwargs. If that fails,
an empty

options object is returned.

**Functions**



Python

**Parameters**

**Name**

**Description**

`**options_class**`

Required*

The class of the options.

`**options**`

Required*

The existing options to update.

`****kwargs**`

Required*

The keyword arguments to use to create the options.

**Returns**

**Type**

**Description**

SearchOptions

The options.

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

**default_options_update_function**

The default options update function.

This function is used to update the query and options with the kwargs. You can

supply your own version of this function to customize the behavior.

`create_options(options_class: type[SearchOptions], options: SearchOptions `

`| ``None``, **kwargs: Any) -> SearchOptions`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

The query.

`**options**`

Required*

The options.

`**parameters**`

Required*

The parameters to use to create the options.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to update the options.

**Returns**

**Type**

**Description**

tuple[str, SearchOptions]

The updated query and options

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

`default_options_update_function(query: str, options: SearchOptions, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, **kwargs: Any) -`

`> tuple[str, SearchOptions]`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**text_search Module**

Reference

**Classes**

TextSearch

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**TextSearch Class**

Reference

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

create_get_search_results

Create a kernel function from a get_search_results function.

create_get_text_search_results

Create a kernel function from a get_text_search_results function.

create_search

Create a kernel function from a search function.

get_search_results

Search for text, returning a KernelSearchResult with the results

directly from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with

TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of

strings.

**create_get_search_results**

Create a kernel function from a get_search_results function.

Python

`TextSearch()`

**Methods**

ﾉ

**Expand table**

`create_get_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `



**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**

ﾉ

**Expand table**



**create_get_text_search_results**

Create a kernel function from a get_text_search_results function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`create_get_text_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**create_search**

Create a kernel function from a search function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

ﾉ

**Expand table**

`create_search(options: SearchOptions | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, options_update_function: `

`OptionsUpdateFunctionType | ``None`` = ``None``, return_parameter: `

`KernelParameterMetadata | ``None`` = ``None``, function_name: str = ``'search'``, `

`description: str = ``'Perform a search for content related to the specified `

`query and return string results'``, string_mapper: Callable[[TMapInput], `

`str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

ﾉ

**Expand table**

`abstract ``async`` get_search_results(query: str, options: SearchOptions | `

`None`` = ``None``, **kwargs: Any) -> KernelSearchResults[Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

`abstract ``async`` get_text_search_results(query: str, options: SearchOptions
`

`| ``None`` = ``None``, **kwargs: Any) ->
KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**

`abstract ``async`` search(query: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

The query to search for.

`**options**`

Required*

The search options.

Default value: None

`****kwargs**`

Required*

If options is None, the search options can be passed as keyword arguments.
They

are then used to create a search options object.

**options_class**

The options class for the search.

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**text_search_filter Module**

Reference

**Classes**

TextSearchFilter

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

ﾉ

**Expand table**



**TextSearchFilter Class**

Reference

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

**Constructor**

Python

**filters**

Python

**is_experimental**

Python

**stage_status**

Python

`TextSearchFilter()`

**Attributes**

`filters: list[FilterClauseBase]`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**text_search_options Module**

Reference

**Classes**

TextSearchOptions

Options for a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextSearchOptions Class**

Reference

Options for a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**top**`

Default value: 5

`**skip**`

Required*

**filter**

`TextSearchOptions(*, filter: TextSearchFilter = ``None``,
include_total_count: `

`bool = ``False``, top: Annotated[int, Gt(gt=0)] = 5, skip: Annotated[int, `

`Ge(ge=0)] = 0)`

ﾉ

**Expand table**

**Attributes**



Python

**include_total_count**

Python

**is_experimental**

Python

**skip**

Python

**stage_status**

Python

**top**

Python

`filter: TextSearchFilter`

`include_total_count: bool`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`

`stage_status = ``'experimental'`

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`



**text_search_result Module**

Reference

**Classes**

TextSearchResult

The result of a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextSearchResult Class**

Reference

The result of a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**value**`

Required*

`**link**`

Required*

**is_experimental**

Python

`TextSearchResult(*, name: str | ``None`` = ``None``, value: str | ``None`` = ``None``, link: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**link**

Python

**name**

Python

**stage_status**

Python

**value**

Python

`is_experimental = ``True`

`link: str | ``None`

`name: str | ``None`

`stage_status = ``'experimental'`

`value: str | ``None`



**utils Module**

Reference

**Classes**

OptionsUpdateFunctionType

Type definition for the options update function in Text Search.

**create_options**

Create search options.

If options are supplied, they are checked for the right type, and the kwargs
are used

to update the options.

If options are not supplied, they are created from the kwargs. If that fails,
an empty

options object is returned.

Python

**Parameters**

**Name**

**Description**

`**options_class**`

Required*

The class of the options.

`**options**`

Required*

The existing options to update.

`****kwargs**`

Required*

The keyword arguments to use to create the options.

ﾉ

**Expand table**

**Functions**

`create_options(options_class: type[SearchOptions], options: SearchOptions `

`| ``None``, **kwargs: Any) -> SearchOptions`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

SearchOptions

The options.

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

**default_options_update_function**

The default options update function.

This function is used to update the query and options with the kwargs. You can

supply your own version of this function to customize the behavior.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

The query.

`**options**`

Required*

The options.

`**parameters**`

Required*

The parameters to use to create the options.

Default value: None

ﾉ

**Expand table**

ﾉ

**Expand table**

`default_options_update_function(query: str, options: SearchOptions, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, **kwargs: Any) -`

`> tuple[str, SearchOptions]`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

The keyword arguments to use to update the options.

**Returns**

**Type**

**Description**

tuple[str, SearchOptions]

The updated query and options

**Exceptions**

**Type**

**Description**

ValidationError

If the options are not valid.

ﾉ

**Expand table**

ﾉ

**Expand table**



**OptionsUpdateFunctionType Class**

Reference

Type definition for the options update function in Text Search.

**Constructor**

Python

`OptionsUpdateFunctionType(*args, **kwargs)`



**vector_store_text_search Module**

Reference

**Classes**

VectorStoreTextSearch

Class that wraps a Vector Store Record Collection to expose as a Text

Search.

Preferably the class methods are used to create an instance of this class.

Otherwise the search executes in the following order depending on

which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorStoreTextSearch Class**

Reference

Class that wraps a Vector Store Record Collection to expose as a Text Search.

Preferably the class methods are used to create an instance of this class.
Otherwise the

search executes in the following order depending on which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

A vector store record collection with a method to search for

vectorizable text.

`**vectorized_search**`

Required*

A vector store record collection with a method to search for

vectors.

`**vector_text_search**`

`VectorStoreTextSearch(*, vectorizable_text_search: `

`VectorizableTextSearchMixin | ``None`` = ``None``, vectorized_search: `

`VectorizedSearchMixin | ``None`` = ``None``, vector_text_search: `

`VectorTextSearchMixin | ``None`` = ``None``, embedding_service: `

`EmbeddingGeneratorBase | ``None`` = ``None``, string_mapper: Callable[[TModel], str] `

`| ``None`` = ``None``, text_search_results_mapper: Callable[[TModel], `

`TextSearchResult] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

A vector store record collection with a method to search for

text.

`**embedding_service**`

Required*

An embedding service to use for vectorized search.

`**string_mapper**`

Required*

A function to map a record to a string.

`**text_search_results_mapper**`

Required*

A function to map a record to a TextSearchResult.

**Keyword-Only Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**vectorized_search**`

Required*

`**vector_text_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

`**text_search_results_mapper**`

Required*

from_vector_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

from_vectorizable_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



from_vectorized_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

get_search_results

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

get_text_search_results

Search for a query, returning a KernelSearchResult with a

TextSearchResult as the results type.

search

Search for a query, returning a KernelSearchResult with a string as

the results type.

**from_vector_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vector_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorizable_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

`from_vector_text_search(vector_text_search: VectorTextSearchMixin, `

`string_mapper: Callable | ``None`` = ``None``, text_search_results_mapper: `

`Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorizable_text_search(vectorizable_text_search: `

`VectorizableTextSearchMixin, string_mapper: Callable | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorized_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vectorized_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

`text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorized_search(vectorized_search: VectorizedSearchMixin, `

`embedding_service: EmbeddingGeneratorBase, string_mapper: Callable | ``None`` `

`= ``None``, text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: `

`Any) -> _T`

ﾉ

**Expand table**



**get_search_results**

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for a query, returning a KernelSearchResult with a TextSearchResult as
the

results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**search**

Search for a query, returning a KernelSearchResult with a string as the
results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**options_class**

Get the options class.

**embedding_service**

Python

**string_mapper**

Python

**text_search_results_mapper**

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`embedding_service: EmbeddingGeneratorBase | ``None`

`string_mapper: Callable[[TModel], str] | ``None`



Python

**vector_text_search**

Python

**vectorizable_text_search**

Python

**vectorized_search**

Python

`text_search_results_mapper: Callable[[TModel], TextSearchResult] | ``None`

`vector_text_search: VectorTextSearchMixin | ``None`

`vectorizable_text_search: VectorizableTextSearchMixin | ``None`

`vectorized_search: VectorizedSearchMixin | ``None`



**OptionsUpdateFunctionType Class**

Reference

Type definition for the options update function in Text Search.

**Constructor**

Python

`OptionsUpdateFunctionType(*args, **kwargs)`



**TextSearch Class**

Reference

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

create_get_search_results

Create a kernel function from a get_search_results function.

create_get_text_search_results

Create a kernel function from a get_text_search_results function.

create_search

Create a kernel function from a search function.

get_search_results

Search for text, returning a KernelSearchResult with the results

directly from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with

TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of

strings.

**create_get_search_results**

Create a kernel function from a get_search_results function.

Python

`TextSearch()`

**Methods**

ﾉ

**Expand table**

`create_get_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `



**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**

ﾉ

**Expand table**



**create_get_text_search_results**

Create a kernel function from a get_text_search_results function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`create_get_text_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**create_search**

Create a kernel function from a search function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

ﾉ

**Expand table**

`create_search(options: SearchOptions | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, options_update_function: `

`OptionsUpdateFunctionType | ``None`` = ``None``, return_parameter: `

`KernelParameterMetadata | ``None`` = ``None``, function_name: str = ``'search'``, `

`description: str = ``'Perform a search for content related to the specified `

`query and return string results'``, string_mapper: Callable[[TMapInput], `

`str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

ﾉ

**Expand table**

`abstract ``async`` get_search_results(query: str, options: SearchOptions | `

`None`` = ``None``, **kwargs: Any) -> KernelSearchResults[Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

`abstract ``async`` get_text_search_results(query: str, options: SearchOptions
`

`| ``None`` = ``None``, **kwargs: Any) ->
KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**

`abstract ``async`` search(query: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

The query to search for.

`**options**`

Required*

The search options.

Default value: None

`****kwargs**`

Required*

If options is None, the search options can be passed as keyword arguments.
They

are then used to create a search options object.

**options_class**

The options class for the search.

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearchFilter Class**

Reference

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

**Constructor**

Python

**filters**

Python

**is_experimental**

Python

**stage_status**

Python

`TextSearchFilter()`

**Attributes**

`filters: list[FilterClauseBase]`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearchOptions Class**

Reference

Options for a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**top**`

Default value: 5

`**skip**`

Required*

**filter**

`TextSearchOptions(*, filter: TextSearchFilter = ``None``,
include_total_count: `

`bool = ``False``, top: Annotated[int, Gt(gt=0)] = 5, skip: Annotated[int, `

`Ge(ge=0)] = 0)`

ﾉ

**Expand table**

**Attributes**



Python

**include_total_count**

Python

**is_experimental**

Python

**skip**

Python

**stage_status**

Python

**top**

Python

`filter: TextSearchFilter`

`include_total_count: bool`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`

`stage_status = ``'experimental'`

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`



**TextSearchResult Class**

Reference

The result of a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**value**`

Required*

`**link**`

Required*

**is_experimental**

Python

`TextSearchResult(*, name: str | ``None`` = ``None``, value: str | ``None`` = ``None``, link: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**link**

Python

**name**

Python

**stage_status**

Python

**value**

Python

`is_experimental = ``True`

`link: str | ``None`

`name: str | ``None`

`stage_status = ``'experimental'`

`value: str | ``None`



**VectorStoreTextSearch Class**

Reference

Class that wraps a Vector Store Record Collection to expose as a Text Search.

Preferably the class methods are used to create an instance of this class.
Otherwise the

search executes in the following order depending on which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

A vector store record collection with a method to search for

vectorizable text.

`**vectorized_search**`

Required*

A vector store record collection with a method to search for

vectors.

`**vector_text_search**`

`VectorStoreTextSearch(*, vectorizable_text_search: `

`VectorizableTextSearchMixin | ``None`` = ``None``, vectorized_search: `

`VectorizedSearchMixin | ``None`` = ``None``, vector_text_search: `

`VectorTextSearchMixin | ``None`` = ``None``, embedding_service: `

`EmbeddingGeneratorBase | ``None`` = ``None``, string_mapper: Callable[[TModel], str] `

`| ``None`` = ``None``, text_search_results_mapper: Callable[[TModel], `

`TextSearchResult] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

A vector store record collection with a method to search for

text.

`**embedding_service**`

Required*

An embedding service to use for vectorized search.

`**string_mapper**`

Required*

A function to map a record to a string.

`**text_search_results_mapper**`

Required*

A function to map a record to a TextSearchResult.

**Keyword-Only Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**vectorized_search**`

Required*

`**vector_text_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

`**text_search_results_mapper**`

Required*

from_vector_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

from_vectorizable_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



from_vectorized_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

get_search_results

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

get_text_search_results

Search for a query, returning a KernelSearchResult with a

TextSearchResult as the results type.

search

Search for a query, returning a KernelSearchResult with a string as

the results type.

**from_vector_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vector_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorizable_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

`from_vector_text_search(vector_text_search: VectorTextSearchMixin, `

`string_mapper: Callable | ``None`` = ``None``, text_search_results_mapper: `

`Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorizable_text_search(vectorizable_text_search: `

`VectorizableTextSearchMixin, string_mapper: Callable | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorized_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vectorized_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

`text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorized_search(vectorized_search: VectorizedSearchMixin, `

`embedding_service: EmbeddingGeneratorBase, string_mapper: Callable | ``None`` `

`= ``None``, text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: `

`Any) -> _T`

ﾉ

**Expand table**



**get_search_results**

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for a query, returning a KernelSearchResult with a TextSearchResult as
the

results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**search**

Search for a query, returning a KernelSearchResult with a string as the
results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**options_class**

Get the options class.

**embedding_service**

Python

**string_mapper**

Python

**text_search_results_mapper**

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`embedding_service: EmbeddingGeneratorBase | ``None`

`string_mapper: Callable[[TModel], str] | ``None`



Python

**vector_text_search**

Python

**vectorizable_text_search**

Python

**vectorized_search**

Python

`text_search_results_mapper: Callable[[TModel], TextSearchResult] | ``None`

`vector_text_search: VectorTextSearchMixin | ``None`

`vectorizable_text_search: VectorizableTextSearchMixin | ``None`

`vectorized_search: VectorizedSearchMixin | ``None`



**vector_search Package**

Reference

**Modules**

const

vector_search

vector_search_filter

vector_search_options

vector_search_result

vector_text_search

vectorizable_text_search

vectorized_search

**Classes**

VectorSearchBase

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorSearchFilter

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize a new instance of VectorSearchFilter.

ﾉ

**Expand table**

ﾉ

**Expand table**



VectorSearchOptions

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorSearchResult

The result of a vector search.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorTextSearchMixin

The mixin for text search, to be used in combination with

VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

VectorizableTextSearchMixin

The mixin for searching with text that get's vectorized downstream.

To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

VectorizedSearchMixin

The mixin for searching with vectors. To be used in combination

with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

**Enums**

TextSearchFunctions

Text search functions.

ﾉ

**Expand table**







**const Module**

Reference

**Enums**

TextSearchFunctions

Text search functions.

ﾉ

**Expand table**



**TextSearchFunctions Enum**

Reference

Text search functions.

SEARCH

GET_TEXT_SEARCH_RESULT

GET_SEARCH_RESULT

**Fields**

ﾉ

**Expand table**



**vector_search Module**

Reference

**Classes**

VectorSearchBase

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorSearchBase Class**

Reference

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

**options_class**

`VectorSearchBase(*, collection_name: str, data_model_type: type[TModel], `

`data_model_definition: VectorStoreRecordDefinition, managed_client: bool = `

`True``)`

ﾉ

**Expand table**

**Attributes**



The options class for the search.

**options_class**

The options class for the search.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

Python

**managed_client**

Python

**stage_status**

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`

`is_experimental = ``True`

`managed_client: bool`



Python

`stage_status = ``'experimental'`



**vector_search_filter Module**

Reference

**Classes**

VectorSearchFilter

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of VectorSearchFilter.

ﾉ

**Expand table**



**VectorSearchFilter Class**

Reference

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of VectorSearchFilter.

**Constructor**

Python

any_tag_equal_to

Adds a filter clause for a any tags equals comparison.

**any_tag_equal_to**

Adds a filter clause for a any tags equals comparison.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

`VectorSearchFilter()`

**Methods**

ﾉ

**Expand table**

`any_tag_equal_to(field_name: str, value: str) -> Self`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**filters**

Python

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`filters: list[FilterClauseBase]`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**vector_search_options Module**

Reference

**Classes**

VectorSearchOptions

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorSearchOptions Class**

Reference

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**vector_field_name**`

Required*

`**top**`

Default value: 3

`**skip**`

Required*

`**include_vectors**`

`VectorSearchOptions(*, filter: VectorSearchFilter = ``None``, `

`include_total_count: bool = ``False``, vector_field_name: str | ``None`` = ``None``, `

`top: Annotated[int, Gt(gt=0)] = 3, skip: Annotated[int, Ge(ge=0)] = 0, `

`include_vectors: bool = ``False``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**filter**

Python

**include_total_count**

Python

**include_vectors**

Python

**is_experimental**

Python

**skip**

Python

**stage_status**

**Attributes**

`filter: VectorSearchFilter`

`include_total_count: bool`

`include_vectors: bool`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`



Python

**top**

Python

**vector_field_name**

Python

`stage_status = ``'experimental'`

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`

`vector_field_name: str | ``None`



**vector_search_result Module**

Reference

**Classes**

VectorSearchResult

The result of a vector search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorSearchResult Class**

Reference

The result of a vector search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**record**`

Required*

`**score**`

Required*

**is_experimental**

Python

`VectorSearchResult(*, record: TModel, score: float | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**record**

Python

**score**

Python

**stage_status**

Python

`record: TModel`

`score: float | ``None`

`stage_status = ``'experimental'`



**vector_text_search Module**

Reference

**Classes**

VectorTextSearchMixin

The mixin for text search, to be used in combination with

VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**VectorTextSearchMixin Class**

Reference

The mixin for text search, to be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

text_search

Search the vector store for records that match the given text and filters.

**text_search**

Search the vector store for records that match the given text and filters.

Python

**Parameters**

**Name**

**Description**

`**search_text**`

Required*

The query to search for.

`**options**`

Required*

options, should include query_text

Default value: None

`VectorTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` text_search(search_text: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**vectorizable_text_search Module**

Reference

**Classes**

VectorizableTextSearchMixin

The mixin for searching with text that get's vectorized downstream.

To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**VectorizableTextSearchMixin Class**

Reference

The mixin for searching with text that get's vectorized downstream.

To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorizable_text_search

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector

search.

**vectorizable_text_search**

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector search.

Python

**Parameters**

`VectorizableTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorizable_text_search(vectorizable_text: str, options: `

`SearchOptions | ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**vectorizable_text**`

Required*

The text to search for, will be vectorized downstream.

`**options**`

Required*

options for the search

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**vectorized_search Module**

Reference

**Classes**

VectorizedSearchMixin

The mixin for searching with vectors. To be used in combination with

VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**VectorizedSearchMixin Class**

Reference

The mixin for searching with vectors. To be used in combination with
VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorized_search

Search the vector store for records that match the given vector (embedding)

and filter.

**vectorized_search**

Search the vector store for records that match the given vector (embedding)
and

filter.

Python

**Parameters**

**Name**

**Description**

`**vector**`

Required*

The vector to search for.

`VectorizedSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorized_search(vector: list[float | int], options: SearchOptions `

`| ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**options**`

Required*

options, should include query_text

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearchFunctions Enum**

Reference

Text search functions.

SEARCH

GET_TEXT_SEARCH_RESULT

GET_SEARCH_RESULT

**Fields**

ﾉ

**Expand table**



**VectorSearchBase Class**

Reference

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

**options_class**

`VectorSearchBase(*, collection_name: str, data_model_type: type[TModel], `

`data_model_definition: VectorStoreRecordDefinition, managed_client: bool = `

`True``)`

ﾉ

**Expand table**

**Attributes**



The options class for the search.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

Python

**managed_client**

Python

**stage_status**

Python

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`

`is_experimental = ``True`

`managed_client: bool`

`stage_status = ``'experimental'`



**VectorSearchFilter Class**

Reference

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of VectorSearchFilter.

**Constructor**

Python

any_tag_equal_to

Adds a filter clause for a any tags equals comparison.

**any_tag_equal_to**

Adds a filter clause for a any tags equals comparison.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

`VectorSearchFilter()`

**Methods**

ﾉ

**Expand table**

`any_tag_equal_to(field_name: str, value: str) -> Self`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**filters**

Python

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`filters: list[FilterClauseBase]`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorSearchOptions Class**

Reference

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**vector_field_name**`

Required*

`**top**`

Default value: 3

`**skip**`

Required*

`**include_vectors**`

`VectorSearchOptions(*, filter: VectorSearchFilter = ``None``, `

`include_total_count: bool = ``False``, vector_field_name: str | ``None`` = ``None``, `

`top: Annotated[int, Gt(gt=0)] = 3, skip: Annotated[int, Ge(ge=0)] = 0, `

`include_vectors: bool = ``False``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**filter**

Python

**include_total_count**

Python

**include_vectors**

Python

**is_experimental**

Python

**skip**

Python

**stage_status**

**Attributes**

`filter: VectorSearchFilter`

`include_total_count: bool`

`include_vectors: bool`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`



Python

**top**

Python

**vector_field_name**

Python

`stage_status = ``'experimental'`

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`

`vector_field_name: str | ``None`



**VectorSearchResult Class**

Reference

The result of a vector search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**record**`

Required*

`**score**`

Required*

**is_experimental**

Python

`VectorSearchResult(*, record: TModel, score: float | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**record**

Python

**score**

Python

**stage_status**

Python

`record: TModel`

`score: float | ``None`

`stage_status = ``'experimental'`



**VectorTextSearchMixin Class**

Reference

The mixin for text search, to be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

text_search

Search the vector store for records that match the given text and filters.

**text_search**

Search the vector store for records that match the given text and filters.

Python

**Parameters**

**Name**

**Description**

`**search_text**`

Required*

The query to search for.

`**options**`

Required*

options, should include query_text

Default value: None

`VectorTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` text_search(search_text: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorizableTextSearchMixin Class**

Reference

The mixin for searching with text that get's vectorized downstream.

To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorizable_text_search

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector

search.

**vectorizable_text_search**

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector search.

Python

**Parameters**

`VectorizableTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorizable_text_search(vectorizable_text: str, options: `

`SearchOptions | ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**vectorizable_text**`

Required*

The text to search for, will be vectorized downstream.

`**options**`

Required*

options for the search

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorizedSearchMixin Class**

Reference

The mixin for searching with vectors. To be used in combination with
VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorized_search

Search the vector store for records that match the given vector (embedding)

and filter.

**vectorized_search**

Search the vector store for records that match the given vector (embedding)
and

filter.

Python

**Parameters**

**Name**

**Description**

`**vector**`

Required*

The vector to search for.

`VectorizedSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorized_search(vector: list[float | int], options: SearchOptions `

`| ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**options**`

Required*

options, should include query_text

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**vector_storage Package**

Reference

**Modules**

vector_store

vector_store_record_collection

**Classes**

VectorStore

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

VectorStoreRecordCollection

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**

ﾉ

**Expand table**



**vector_store Module**

Reference

**Classes**

VectorStore

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorStore Class**

Reference

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**vector_record_collections**`

Required*

`**managed_client**`

Default value: True

get_collection

Get a vector record store.

list_collection_names

Get the names of all collections.

**get_collection**

`VectorStore(*, vector_record_collections: dict[str, `

`VectorStoreRecordCollection] = ``None``, managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Get a vector record store.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

Default value: None

**list_collection_names**

Get the names of all collections.

Python

**is_experimental**

Python

**managed_client**

`abstract get_collection(collection_name: str, data_model_type: `

`type[object], data_model_definition: VectorStoreRecordDefinition | ``None`` = `

`None``, **kwargs: Any) -> VectorStoreRecordCollection`

ﾉ

**Expand table**

`abstract ``async`` list_collection_names(**kwargs) -> Sequence[str]`

**Attributes**

`is_experimental = ``True`



Python

**stage_status**

Python

**vector_record_collections**

Python

`managed_client: bool`

`stage_status = ``'experimental'`

`vector_record_collections: dict[str, VectorStoreRecordCollection]`



**vector_store_record_collection Module**

Reference

**Classes**

VectorStoreRecordCollection

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VectorStoreRecordCollection Class**

Reference

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

`VectorStoreRecordCollection(*, collection_name: str, data_model_type: `

`type[TModel], data_model_definition: VectorStoreRecordDefinition, `

`managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



create_collection

Create the collection in the service.

This should be overridden by the child class.

create_collection_if_not_exists

Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does

returns False. Otherwise, creates the collection and returns True.

delete

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or the record does not exist.

delete_batch

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or a record does not exist.

delete_collection

Delete the collection.

This should be overridden by the child class.

deserialize

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

2\. Deserialize the store model to a dict, using the store

specific method.

3\. Convert the dict to the data model, using the data model

specific method.

does_collection_exist

Check if the collection exists.

This should be overridden by the child class.

get

Get a record if the key exists.

get_batch

Get a batch of records whose keys exist in the collection, i.e. keys

that do not exist are ignored.

model_post_init

Post init function that sets the key field and container mode

values, and validates the datamodel.

serialize

Serialize the data model to the store model.

This method follows the following steps:



1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model

specific method.

3\. Convert the dict to the store model, using the store specific

method.

If overriding this method, make sure to first try to serialize the

data model to the store model, before doing the store specific

version, the user supplied version should have precedence.

upsert

Upsert a record.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

upsert_batch

Upsert a batch of records.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

**create_collection**

Create the collection in the service.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**create_collection_if_not_exists**

`abstract ``async`` create_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does returns
False. Otherwise,

creates the collection and returns True.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete**

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or the

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`****kwargs**`

Additional arguments.

`async`` create_collection_if_not_exists(**kwargs: Any) -> bool`

ﾉ

**Expand table**

`async`` delete(key: TKey, **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_batch**

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or a

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`****kwargs**`

Required*

Additional arguments.

**Exceptions**

ﾉ

**Expand table**

`async`` delete_batch(keys: Sequence[TKey], **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_collection**

Delete the collection.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**deserialize**

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

ﾉ

**Expand table**

`abstract ``async`` delete_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



2\. Deserialize the store model to a dict, using the store specific method.

3\. Convert the dict to the data model, using the data model specific method.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

**Type**

**Description**

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**does_collection_exist**

Check if the collection exists.

This should be overridden by the child class.

Python

**Exceptions**

`deserialize(records: Any | dict[str, Any] | Sequence[Any | dict[str, `

`Any]], **kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` does_collection_exist(**kwargs: Any) -> bool`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**get**

Get a record if the key exists.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`**include_vectors**`

Required*

Include the vectors in the response, default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

<xref:TModel>

The record. None if the key does not exist.

`async`` get(key: TKey, include_vectors: bool = ``True``, **kwargs: Any) -> `

`TModel | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**get_batch**

Get a batch of records whose keys exist in the collection, i.e. keys that do
not exist

are ignored.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`**include_vectors**`

Required*

Include the vectors in the response. Default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` get_batch(keys: Sequence[TKey], include_vectors: bool = ``True``, `

`**kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The records, either a list of TModel or the container type.

**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**model_post_init**

Post init function that sets the key field and container mode values, and
validates the

datamodel.

Python

**Parameters**

**Name**

**Description**

`**_VectorStoreRecordCollection__context**`

Default value: None

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

ﾉ

**Expand table**

`model_post_init(_VectorStoreRecordCollection__context: object | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

This is different then the _inner_x methods

as this is a public method.

**serialize**

Serialize the data model to the store model.

This method follows the following steps:

1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model specific method.

3\. Convert the dict to the store model, using the store specific method.

If overriding this method, make sure to first try to serialize the data model
to the

store model, before doing the store specific version, the user supplied
version should

have precedence.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

`serialize(records: TModel | Sequence[TModel], **kwargs: Any) -> Any | `

`Sequence[Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

**upsert**

Upsert a record.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**record**`

Required*

The record.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

The key of the upserted record or a list of keys, when a container type is
used.

`async`` upsert(record: TModel, embedding_generation_function: `

`Callable[[TModel, type[TModel] | ``None``, VectorStoreRecordDefinition | `

`None``], Awaitable[TModel]] | ``None`` = ``None``, **kwargs: Any) -> TKey | `

`Sequence[TKey] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**upsert_batch**

Upsert a batch of records.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

The records to upsert, can be a list of records, or a

single container.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` upsert_batch(records: TModel | Sequence[TModel], `

`embedding_generation_function: Callable[[TModel | Sequence[TModel], `

`type[TModel] | ``None``, VectorStoreRecordDefinition | ``None``], `

`Awaitable[TModel | Sequence[TModel]]] | ``None`` = ``None``, **kwargs: Any) -> `

`Sequence[TKey]`

ﾉ

**Expand table**



**Type**

**Description**

Sequence[<xref:

TKey>]

The keys of the upserted records, this is always a list, corresponds to the

input or the items in the container.

**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`



Python

**managed_client**

Python

**stage_status**

Python

**supported_key_types**

Python

**supported_vector_types**

Python

`is_experimental = ``True`

`managed_client: bool`

`stage_status = ``'experimental'`

`supported_key_types: ClassVar[list[str] | ``None``] = ``None`

`supported_vector_types: ClassVar[list[str] | ``None``] = ``None`



**VectorStore Class**

Reference

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**vector_record_collections**`

Required*

`**managed_client**`

Default value: True

get_collection

Get a vector record store.

list_collection_names

Get the names of all collections.

**get_collection**

`VectorStore(*, vector_record_collections: dict[str, `

`VectorStoreRecordCollection] = ``None``, managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Get a vector record store.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

Default value: None

**list_collection_names**

Get the names of all collections.

Python

**is_experimental**

Python

**managed_client**

`abstract get_collection(collection_name: str, data_model_type: `

`type[object], data_model_definition: VectorStoreRecordDefinition | ``None`` = `

`None``, **kwargs: Any) -> VectorStoreRecordCollection`

ﾉ

**Expand table**

`abstract ``async`` list_collection_names(**kwargs) -> Sequence[str]`

**Attributes**

`is_experimental = ``True`



Python

**stage_status**

Python

**vector_record_collections**

Python

`managed_client: bool`

`stage_status = ``'experimental'`

`vector_record_collections: dict[str, VectorStoreRecordCollection]`



**VectorStoreRecordCollection Class**

Reference

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

`VectorStoreRecordCollection(*, collection_name: str, data_model_type: `

`type[TModel], data_model_definition: VectorStoreRecordDefinition, `

`managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



create_collection

Create the collection in the service.

This should be overridden by the child class.

create_collection_if_not_exists

Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does

returns False. Otherwise, creates the collection and returns True.

delete

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or the record does not exist.

delete_batch

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or a record does not exist.

delete_collection

Delete the collection.

This should be overridden by the child class.

deserialize

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

2\. Deserialize the store model to a dict, using the store

specific method.

3\. Convert the dict to the data model, using the data model

specific method.

does_collection_exist

Check if the collection exists.

This should be overridden by the child class.

get

Get a record if the key exists.

get_batch

Get a batch of records whose keys exist in the collection, i.e. keys

that do not exist are ignored.

model_post_init

Post init function that sets the key field and container mode

values, and validates the datamodel.

serialize

Serialize the data model to the store model.

This method follows the following steps:



1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model

specific method.

3\. Convert the dict to the store model, using the store specific

method.

If overriding this method, make sure to first try to serialize the

data model to the store model, before doing the store specific

version, the user supplied version should have precedence.

upsert

Upsert a record.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

upsert_batch

Upsert a batch of records.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

**create_collection**

Create the collection in the service.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**create_collection_if_not_exists**

`abstract ``async`` create_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does returns
False. Otherwise,

creates the collection and returns True.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete**

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or the

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`****kwargs**`

Additional arguments.

`async`` create_collection_if_not_exists(**kwargs: Any) -> bool`

ﾉ

**Expand table**

`async`` delete(key: TKey, **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_batch**

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or a

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`****kwargs**`

Required*

Additional arguments.

**Exceptions**

ﾉ

**Expand table**

`async`` delete_batch(keys: Sequence[TKey], **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_collection**

Delete the collection.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**deserialize**

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

ﾉ

**Expand table**

`abstract ``async`` delete_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



2\. Deserialize the store model to a dict, using the store specific method.

3\. Convert the dict to the data model, using the data model specific method.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

**Type**

**Description**

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**does_collection_exist**

Check if the collection exists.

This should be overridden by the child class.

Python

**Exceptions**

`deserialize(records: Any | dict[str, Any] | Sequence[Any | dict[str, `

`Any]], **kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` does_collection_exist(**kwargs: Any) -> bool`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**get**

Get a record if the key exists.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`**include_vectors**`

Required*

Include the vectors in the response, default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

<xref:TModel>

The record. None if the key does not exist.

`async`` get(key: TKey, include_vectors: bool = ``True``, **kwargs: Any) -> `

`TModel | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**get_batch**

Get a batch of records whose keys exist in the collection, i.e. keys that do
not exist

are ignored.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`**include_vectors**`

Required*

Include the vectors in the response. Default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` get_batch(keys: Sequence[TKey], include_vectors: bool = ``True``, `

`**kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The records, either a list of TModel or the container type.

**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**model_post_init**

Post init function that sets the key field and container mode values, and
validates the

datamodel.

Python

**Parameters**

**Name**

**Description**

`**_VectorStoreRecordCollection__context**`

Default value: None

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

ﾉ

**Expand table**

`model_post_init(_VectorStoreRecordCollection__context: object | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

This is different then the _inner_x methods

as this is a public method.

**serialize**

Serialize the data model to the store model.

This method follows the following steps:

1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model specific method.

3\. Convert the dict to the store model, using the store specific method.

If overriding this method, make sure to first try to serialize the data model
to the

store model, before doing the store specific version, the user supplied
version should

have precedence.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

`serialize(records: TModel | Sequence[TModel], **kwargs: Any) -> Any | `

`Sequence[Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

**upsert**

Upsert a record.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**record**`

Required*

The record.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

The key of the upserted record or a list of keys, when a container type is
used.

`async`` upsert(record: TModel, embedding_generation_function: `

`Callable[[TModel, type[TModel] | ``None``, VectorStoreRecordDefinition | `

`None``], Awaitable[TModel]] | ``None`` = ``None``, **kwargs: Any) -> TKey | `

`Sequence[TKey] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**upsert_batch**

Upsert a batch of records.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

The records to upsert, can be a list of records, or a

single container.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` upsert_batch(records: TModel | Sequence[TModel], `

`embedding_generation_function: Callable[[TModel | Sequence[TModel], `

`type[TModel] | ``None``, VectorStoreRecordDefinition | ``None``], `

`Awaitable[TModel | Sequence[TModel]]] | ``None`` = ``None``, **kwargs: Any) -> `

`Sequence[TKey]`

ﾉ

**Expand table**



**Type**

**Description**

Sequence[<xref:

TKey>]

The keys of the upserted records, this is always a list, corresponds to the

input or the items in the container.

**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`



Python

**managed_client**

Python

**stage_status**

Python

**supported_key_types**

Python

**supported_vector_types**

Python

`is_experimental = ``True`

`managed_client: bool`

`stage_status = ``'experimental'`

`supported_key_types: ClassVar[list[str] | ``None``] = ``None`

`supported_vector_types: ClassVar[list[str] | ``None``] = ``None`



**const Module**

Reference

**Enums**

DistanceFunction

Distance functions for similarity search.

Cosine Similarity the cosine (angular) similarity between two vectors measures

only the angle between the two vectors, without taking into account the

length of the vectors Cosine Similarity = 1 - Cosine Distance -1 means vectors

are opposite 0 means vectors are orthogonal 1 means vectors are identical

Cosine Distance the cosine (angular) distance between two vectors measures

only the angle between the two vectors, without taking into account the

length of the vectors Cosine Distance = 1 - Cosine Similarity 2 means vectors

are opposite 1 means vectors are orthogonal 0 means vectors are identical

Dot Product measures both the length and angle between two vectors same

as cosine similarity if the vectors are the same length, but more performant

Euclidean Distance measures the Euclidean distance between two vectors also

known as l2-norm

Euclidean Squared Distance measures the Euclidean squared distance

between two vectors also known as l2-squared

Manhattan measures the Manhattan distance between two vectors

Hamming number of differences between vectors at each dimensions

IndexKind

Index kinds for similarity search.

HNSW Hierarchical Navigable Small World which performs an approximate

nearest neighbor (ANN) search. Lower accuracy than exhaustive k nearest

neighbor, but faster and more efficient.

Flat Does a brute force search to find the nearest neighbors. Calculates the

distances between all pairs of data points, so has a linear time complexity,
that

grows directly proportional to the number of points. Also referred to as

exhaustive k nearest neighbor in some databases. High recall accuracy, but

slower and more expensive than HNSW. Better with smaller datasets.

IVF Flat Inverted File with Flat Compression. Designed to enhance search

efficiency by narrowing the search area through the use of neighbor partitions

or clusters. Also referred to as approximate nearest neighbor (ANN) search.

ﾉ

**Expand table**



Disk ANN Disk-based Approximate Nearest Neighbor algorithm designed for

efficiently searching for approximate nearest neighbors (ANN) in high-

dimensional spaces. The primary focus of DiskANN is to handle large-scale

datasets that cannot fit entirely into memory, leveraging disk storage to
store

the data while maintaining fast search times.

Quantized Flat Index that compresses vectors using DiskANN-based

quantization methods for better efficiency in the kNN search.

Dynamic Dynamic index allows to automatically switch from FLAT to HNSW

indexes.



**DistanceFunction Enum**

Reference

Distance functions for similarity search.

Cosine Similarity the cosine (angular) similarity between two vectors measures
only the

angle between the two vectors, without taking into account the length of the
vectors

Cosine Similarity = 1 - Cosine Distance -1 means vectors are opposite 0 means
vectors

are orthogonal 1 means vectors are identical

Cosine Distance the cosine (angular) distance between two vectors measures
only the

angle between the two vectors, without taking into account the length of the
vectors

Cosine Distance = 1 - Cosine Similarity 2 means vectors are opposite 1 means
vectors

are orthogonal 0 means vectors are identical

Dot Product measures both the length and angle between two vectors same as
cosine

similarity if the vectors are the same length, but more performant

Euclidean Distance measures the Euclidean distance between two vectors also
known as

l2-norm

Euclidean Squared Distance measures the Euclidean squared distance between two

vectors also known as l2-squared

Manhattan measures the Manhattan distance between two vectors

Hamming number of differences between vectors at each dimensions

COSINE_DISTANCE

COSINE_SIMILARITY

DOT_PROD

EUCLIDEAN_DISTANCE

EUCLIDEAN_SQUARED_DISTANCE

HAMMING

**Fields**

ﾉ

**Expand table**



MANHATTAN



**IndexKind Enum**

Reference

Index kinds for similarity search.

HNSW Hierarchical Navigable Small World which performs an approximate nearest

neighbor (ANN) search. Lower accuracy than exhaustive k nearest neighbor, but
faster

and more efficient.

Flat Does a brute force search to find the nearest neighbors. Calculates the
distances

between all pairs of data points, so has a linear time complexity, that grows
directly

proportional to the number of points. Also referred to as exhaustive k nearest
neighbor

in some databases. High recall accuracy, but slower and more expensive than
HNSW.

Better with smaller datasets.

IVF Flat Inverted File with Flat Compression. Designed to enhance search
efficiency by

narrowing the search area through the use of neighbor partitions or clusters.
Also

referred to as approximate nearest neighbor (ANN) search.

Disk ANN Disk-based Approximate Nearest Neighbor algorithm designed for
efficiently

searching for approximate nearest neighbors (ANN) in high-dimensional spaces.
The

primary focus of DiskANN is to handle large-scale datasets that cannot fit
entirely into

memory, leveraging disk storage to store the data while maintaining fast
search times.

Quantized Flat Index that compresses vectors using DiskANN-based quantization

methods for better efficiency in the kNN search.

Dynamic Dynamic index allows to automatically switch from FLAT to HNSW
indexes.

DISK_ANN

DYNAMIC

FLAT

HNSW

IVF_FLAT

**Fields**

ﾉ

**Expand table**



QUANTIZED_FLAT



**kernel_search_results Module**

Reference

**Classes**

KernelSearchResults

The result of a kernel search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelSearchResults Class**

Reference

The result of a kernel search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**results**`

Required*

`**total_count**`

Required*

`**metadata**`

Required*

**is_experimental**

Python

`KernelSearchResults(*, results: AsyncIterable[T], total_count: int | ``None`` = `

`None``, metadata: Mapping[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**metadata**

Python

**results**

Python

**stage_status**

Python

**total_count**

Python

`is_experimental = ``True`

`metadata: Mapping[str, Any] | ``None`

`results: AsyncIterable[T]`

`stage_status = ``'experimental'`

`total_count: int | ``None`



**search_filter Module**

Reference

**Classes**

SearchFilter

A filter clause for a search.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

ﾉ

**Expand table**



**SearchFilter Class**

Reference

A filter clause for a search.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

**Constructor**

Python

equal_to

Add an equals filter clause.

**equal_to**

Add an equals filter clause.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

`SearchFilter()`

**Methods**

ﾉ

**Expand table**

`equal_to(field_name: str, value: str) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**search_options Module**

Reference

**Classes**

SearchOptions

Options for a search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**SearchOptions Class**

Reference

Options for a search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

**filter**

Python

`SearchOptions(*, filter: SearchFilter = ``None``, include_total_count: bool =
`

`False``)`

ﾉ

**Expand table**

**Attributes**

`filter: SearchFilter`



**include_total_count**

Python

**is_experimental**

Python

**stage_status**

Python

`include_total_count: bool`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**AnyTagsEqualTo Class**

Reference

A filter clause for a any tags equals comparison.

Args: field_name: The name of the field containing the list of tags. value:
The value to

compare against the list of tags.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**filter_clause_type**

Python

`AnyTagsEqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**is_experimental**

Python

**stage_status**

Python

`filter_clause_type: ClassVar[str] = ``'any_tags_equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**DistanceFunction Enum**

Reference

Distance functions for similarity search.

Cosine Similarity the cosine (angular) similarity between two vectors measures
only the

angle between the two vectors, without taking into account the length of the
vectors

Cosine Similarity = 1 - Cosine Distance -1 means vectors are opposite 0 means
vectors

are orthogonal 1 means vectors are identical

Cosine Distance the cosine (angular) distance between two vectors measures
only the

angle between the two vectors, without taking into account the length of the
vectors

Cosine Distance = 1 - Cosine Similarity 2 means vectors are opposite 1 means
vectors

are orthogonal 0 means vectors are identical

Dot Product measures both the length and angle between two vectors same as
cosine

similarity if the vectors are the same length, but more performant

Euclidean Distance measures the Euclidean distance between two vectors also
known as

l2-norm

Euclidean Squared Distance measures the Euclidean squared distance between two

vectors also known as l2-squared

Manhattan measures the Manhattan distance between two vectors

Hamming number of differences between vectors at each dimensions

COSINE_DISTANCE

COSINE_SIMILARITY

DOT_PROD

EUCLIDEAN_DISTANCE

EUCLIDEAN_SQUARED_DISTANCE

HAMMING

**Fields**

ﾉ

**Expand table**



MANHATTAN



**EqualTo Class**

Reference

A filter clause for an equals comparison.

Args: field_name: The name of the field to compare. value: The value to
compare against

the field.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

Required*

**filter_clause_type**

Python

`EqualTo(*, field_name: str, value: Any)`

ﾉ

**Expand table**

**Attributes**



**is_experimental**

Python

**stage_status**

Python

`filter_clause_type: ClassVar[str] = ``'equal_to'`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**IndexKind Enum**

Reference

Index kinds for similarity search.

HNSW Hierarchical Navigable Small World which performs an approximate nearest

neighbor (ANN) search. Lower accuracy than exhaustive k nearest neighbor, but
faster

and more efficient.

Flat Does a brute force search to find the nearest neighbors. Calculates the
distances

between all pairs of data points, so has a linear time complexity, that grows
directly

proportional to the number of points. Also referred to as exhaustive k nearest
neighbor

in some databases. High recall accuracy, but slower and more expensive than
HNSW.

Better with smaller datasets.

IVF Flat Inverted File with Flat Compression. Designed to enhance search
efficiency by

narrowing the search area through the use of neighbor partitions or clusters.
Also

referred to as approximate nearest neighbor (ANN) search.

Disk ANN Disk-based Approximate Nearest Neighbor algorithm designed for
efficiently

searching for approximate nearest neighbors (ANN) in high-dimensional spaces.
The

primary focus of DiskANN is to handle large-scale datasets that cannot fit
entirely into

memory, leveraging disk storage to store the data while maintaining fast
search times.

Quantized Flat Index that compresses vectors using DiskANN-based quantization

methods for better efficiency in the kNN search.

Dynamic Dynamic index allows to automatically switch from FLAT to HNSW
indexes.

DISK_ANN

DYNAMIC

FLAT

HNSW

IVF_FLAT

**Fields**

ﾉ

**Expand table**



QUANTIZED_FLAT



**KernelSearchResults Class**

Reference

The result of a kernel search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**results**`

Required*

`**total_count**`

Required*

`**metadata**`

Required*

**is_experimental**

Python

`KernelSearchResults(*, results: AsyncIterable[T], total_count: int | ``None`` = `

`None``, metadata: Mapping[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**metadata**

Python

**results**

Python

**stage_status**

Python

**total_count**

Python

`is_experimental = ``True`

`metadata: Mapping[str, Any] | ``None`

`results: AsyncIterable[T]`

`stage_status = ``'experimental'`

`total_count: int | ``None`



**OptionsUpdateFunctionType Class**

Reference

Type definition for the options update function in Text Search.

**Constructor**

Python

`OptionsUpdateFunctionType(*args, **kwargs)`



**SearchOptions Class**

Reference

Options for a search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

**filter**

Python

`SearchOptions(*, filter: SearchFilter = ``None``, include_total_count: bool =
`

`False``)`

ﾉ

**Expand table**

**Attributes**

`filter: SearchFilter`



**include_total_count**

Python

**is_experimental**

Python

**stage_status**

Python

`include_total_count: bool`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearch Class**

Reference

The base class for all text searches.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

create_get_search_results

Create a kernel function from a get_search_results function.

create_get_text_search_results

Create a kernel function from a get_text_search_results function.

create_search

Create a kernel function from a search function.

get_search_results

Search for text, returning a KernelSearchResult with the results

directly from the service.

get_text_search_results

Search for text, returning a KernelSearchResult with

TextSearchResults.

search

Search for text, returning a KernelSearchResult with a list of

strings.

**create_get_search_results**

Create a kernel function from a get_search_results function.

Python

`TextSearch()`

**Methods**

ﾉ

**Expand table**

`create_get_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `



**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**

ﾉ

**Expand table**



**create_get_text_search_results**

Create a kernel function from a get_text_search_results function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`create_get_text_search_results(options: SearchOptions | ``None`` = ``None``, `

`parameters: list[KernelParameterMetadata] | ``None`` = ``None``, `

`options_update_function: OptionsUpdateFunctionType | ``None`` = ``None``, `

`return_parameter: KernelParameterMetadata | ``None`` = ``None``, function_name: `

`str = ``'search'``, description: str = ``'Perform a search for content
related `

`to the specified query and return string results'``, string_mapper: `

`Callable[[TMapInput], str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**create_search**

Create a kernel function from a search function.

Python

**Parameters**

**Name**

**Description**

`**options**`

The search options.

Default value: None

`**parameters**`

The parameters for the function, a list of

KernelParameterMetadata.

Default value: None

`**options_update_function**`

A function to update the search options. The function should

return the updated query and options. There is a default

function that can be used, or you can supply your own. The

default function uses the parameters and the kwargs to

update the options. Adding equal to filters to the options for

ﾉ

**Expand table**

`create_search(options: SearchOptions | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, options_update_function: `

`OptionsUpdateFunctionType | ``None`` = ``None``, return_parameter: `

`KernelParameterMetadata | ``None`` = ``None``, function_name: str = ``'search'``, `

`description: str = ``'Perform a search for content related to the specified `

`query and return string results'``, string_mapper: Callable[[TMapInput], `

`str] | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

all parameters that are not "query", "top", or "skip". As well as

adding equal to filters for parameters that have a default

value.

Default value: None

`**return_parameter**`

The return parameter for the function.

Default value: None

`**function_name**`

The name of the function, to be used in the kernel, default is

"search".

Default value: search

`**description**`

The description of the function, a default is provided.

Default value: Perform a search for content related to the

specified query and return string results

`**string_mapper**`

The function to map the search results to strings.

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The kernel function.

**get_search_results**

Search for text, returning a KernelSearchResult with the results directly from
the

service.

Python

**Parameters**

ﾉ

**Expand table**

`abstract ``async`` get_search_results(query: str, options: SearchOptions | `

`None`` = ``None``, **kwargs: Any) -> KernelSearchResults[Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for text, returning a KernelSearchResult with TextSearchResults.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**search**

Search for text, returning a KernelSearchResult with a list of strings.

Python

**Parameters**

`abstract ``async`` get_text_search_results(query: str, options: SearchOptions
`

`| ``None`` = ``None``, **kwargs: Any) ->
KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**

`abstract ``async`` search(query: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**



**Name**

**Description**

`**query**`

Required*

The query to search for.

`**options**`

Required*

The search options.

Default value: None

`****kwargs**`

Required*

If options is None, the search options can be passed as keyword arguments.
They

are then used to create a search options object.

**options_class**

The options class for the search.

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearchFilter Class**

Reference

A filter clause for a text search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SearchFilter.

**Constructor**

Python

**is_experimental**

Python

**stage_status**

Python

`TextSearchFilter()`

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**TextSearchOptions Class**

Reference

Options for a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**top**`

Default value: 5

`**skip**`

Required*

**filter**

`TextSearchOptions(*, filter: TextSearchFilter = ``None``,
include_total_count: `

`bool = ``False``, top: Annotated[int, Gt(gt=0)] = 5, skip: Annotated[int, `

`Ge(ge=0)] = 0)`

ﾉ

**Expand table**

**Attributes**



Python

**is_experimental**

Python

**skip**

Python

**stage_status**

Python

**top**

Python

`filter: TextSearchFilter`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`

`stage_status = ``'experimental'`

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`



**TextSearchResult Class**

Reference

The result of a text search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**value**`

Required*

`**link**`

Required*

**is_experimental**

Python

`TextSearchResult(*, name: str | ``None`` = ``None``, value: str | ``None`` = ``None``, link: `

`str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**link**

Python

**name**

Python

**stage_status**

Python

**value**

Python

`is_experimental = ``True`

`link: str | ``None`

`name: str | ``None`

`stage_status = ``'experimental'`

`value: str | ``None`



**VectorSearchBase Class**

Reference

Method for searching vectors.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

**options_class**

`VectorSearchBase(*, collection_name: str, data_model_type: type[TModel], `

`data_model_definition: VectorStoreRecordDefinition, managed_client: bool = `

`True``)`

ﾉ

**Expand table**

**Attributes**



The options class for the search.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

Python

**managed_client**

Python

**stage_status**

Python

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`

`is_experimental = ``True`

`managed_client: bool`

`stage_status = ``'experimental'`



**VectorSearchFilter Class**

Reference

A filter clause for a vector search query.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of VectorSearchFilter.

**Constructor**

Python

any_tag_equal_to

Adds a filter clause for a any tags equals comparison.

**any_tag_equal_to**

Adds a filter clause for a any tags equals comparison.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

Required*

`**value**`

`VectorSearchFilter()`

**Methods**

ﾉ

**Expand table**

`any_tag_equal_to(field_name: str, value: str) -> Self`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorSearchOptions Class**

Reference

Options for vector search, builds on TextSearchOptions.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**filter**`

Required*

`**include_total_count**`

Required*

`**vector_field_name**`

Required*

`**top**`

Default value: 3

`**skip**`

Required*

`**include_vectors**`

`VectorSearchOptions(*, filter: VectorSearchFilter = ``None``, `

`include_total_count: bool = ``False``, vector_field_name: str | ``None`` = ``None``, `

`top: Annotated[int, Gt(gt=0)] = 3, skip: Annotated[int, Ge(ge=0)] = 0, `

`include_vectors: bool = ``False``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**filter**

Python

**include_vectors**

Python

**is_experimental**

Python

**skip**

Python

**stage_status**

Python

**top**

**Attributes**

`filter: VectorSearchFilter`

`include_vectors: bool`

`is_experimental = ``True`

`skip: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Ge(ge=0)])]`

`stage_status = ``'experimental'`



Python

**vector_field_name**

Python

`top: Annotated[int, FieldInfo(annotation=NoneType, required=``True``, `

`metadata=[Gt(gt=0)])]`

`vector_field_name: str | ``None`



**VectorSearchResult Class**

Reference

The result of a vector search.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**record**`

Required*

`**score**`

Required*

**is_experimental**

Python

`VectorSearchResult(*, record: TModel, score: float | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**record**

Python

**score**

Python

**stage_status**

Python

`record: TModel`

`score: float | ``None`

`stage_status = ``'experimental'`



**VectorStore Class**

Reference

Base class for vector stores.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**vector_record_collections**`

Required*

`**managed_client**`

Default value: True

get_collection

Get a vector record store.

list_collection_names

Get the names of all collections.

**get_collection**

`VectorStore(*, vector_record_collections: dict[str, `

`VectorStoreRecordCollection] = ``None``, managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Get a vector record store.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

Default value: None

**list_collection_names**

Get the names of all collections.

Python

**is_experimental**

Python

**managed_client**

`abstract get_collection(collection_name: str, data_model_type: `

`type[object], data_model_definition: VectorStoreRecordDefinition | ``None`` = `

`None``, **kwargs: Any) -> VectorStoreRecordCollection`

ﾉ

**Expand table**

`abstract ``async`` list_collection_names(**kwargs) -> Sequence[str]`

**Attributes**

`is_experimental = ``True`



Python

**stage_status**

Python

**vector_record_collections**

Python

`managed_client: bool`

`stage_status = ``'experimental'`

`vector_record_collections: dict[str, VectorStoreRecordCollection]`



**VectorStoreRecordCollection Class**

Reference

Base class for a vector store record collection.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**collection_name**`

Required*

`**data_model_type**`

Required*

`**data_model_definition**`

Required*

`**managed_client**`

Default value: True

`VectorStoreRecordCollection(*, collection_name: str, data_model_type: `

`type[TModel], data_model_definition: VectorStoreRecordDefinition, `

`managed_client: bool = ``True``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



create_collection

Create the collection in the service.

This should be overridden by the child class.

create_collection_if_not_exists

Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does

returns False. Otherwise, creates the collection and returns True.

delete

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or the record does not exist.

delete_batch

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs

during deletion or a record does not exist.

delete_collection

Delete the collection.

This should be overridden by the child class.

deserialize

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

2\. Deserialize the store model to a dict, using the store

specific method.

3\. Convert the dict to the data model, using the data model

specific method.

does_collection_exist

Check if the collection exists.

This should be overridden by the child class.

get

Get a record if the key exists.

get_batch

Get a batch of records whose keys exist in the collection, i.e. keys

that do not exist are ignored.

model_post_init

Post init function that sets the key field and container mode

values, and validates the datamodel.

serialize

Serialize the data model to the store model.

This method follows the following steps:



1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model

specific method.

3\. Convert the dict to the store model, using the store specific

method.

If overriding this method, make sure to first try to serialize the

data model to the store model, before doing the store specific

version, the user supplied version should have precedence.

upsert

Upsert a record.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

upsert_batch

Upsert a batch of records.

If the key of the record already exists, the existing record will be

updated. If the key does not exist, a new record will be created.

**create_collection**

Create the collection in the service.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**create_collection_if_not_exists**

`abstract ``async`` create_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



Create the collection in the service if it does not exists.

First uses does_collection_exist to check if it exists, if it does returns
False. Otherwise,

creates the collection and returns True.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete**

Delete a record.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or the

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`****kwargs**`

Additional arguments.

`async`` create_collection_if_not_exists(**kwargs: Any) -> bool`

ﾉ

**Expand table**

`async`` delete(key: TKey, **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_batch**

Delete a batch of records.

An exception will be raised at the end if any record does not exist.

Exceptions: VectorStoreOperationException: If an error occurs during deletion
or a

record does not exist.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`****kwargs**`

Required*

Additional arguments.

**Exceptions**

ﾉ

**Expand table**

`async`` delete_batch(keys: Sequence[TKey], **kwargs: Any) -> ``None`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**delete_collection**

Delete the collection.

This should be overridden by the child class.

Python

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**deserialize**

Deserialize the store model to the data model.

This method follows the following steps:

1\. Check if the data model has a deserialize method.

Use that method to deserialize and return the result.

ﾉ

**Expand table**

`abstract ``async`` delete_collection(**kwargs: Any) -> ``None`

ﾉ

**Expand table**



2\. Deserialize the store model to a dict, using the store specific method.

3\. Convert the dict to the data model, using the data model specific method.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

**Type**

**Description**

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**does_collection_exist**

Check if the collection exists.

This should be overridden by the child class.

Python

**Exceptions**

`deserialize(records: Any | dict[str, Any] | Sequence[Any | dict[str, `

`Any]], **kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` does_collection_exist(**kwargs: Any) -> bool`

ﾉ

**Expand table**



**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

This is different then the _inner_x methods

as this is a public method.

**get**

Get a record if the key exists.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

The key.

`**include_vectors**`

Required*

Include the vectors in the response, default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

<xref:TModel>

The record. None if the key does not exist.

`async`` get(key: TKey, include_vectors: bool = ``True``, **kwargs: Any) -> `

`TModel | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**get_batch**

Get a batch of records whose keys exist in the collection, i.e. keys that do
not exist

are ignored.

Python

**Parameters**

**Name**

**Description**

`**keys**`

Required*

The keys.

`**include_vectors**`

Required*

Include the vectors in the response. Default is True. Some vector stores

do not support retrieving without vectors, even when set to false. Some

vector stores have specific parameters to control that behavior, when

that parameter is set, include_vectors is ignored.

Default value: True

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` get_batch(keys: Sequence[TKey], include_vectors: bool = ``True``, `

`**kwargs: Any) -> TModel | Sequence[TModel] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The records, either a list of TModel or the container type.

**Exceptions**

**Type**

**Description**

VectorStoreOperationException

If an error occurs during the get.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

**model_post_init**

Post init function that sets the key field and container mode values, and
validates the

datamodel.

Python

**Parameters**

**Name**

**Description**

`**_VectorStoreRecordCollection__context**`

Default value: None

**Exceptions**

**Type**

**Description**

Make sure the implementation

this function raises relevant exceptions with good descriptions.

ﾉ

**Expand table**

`model_post_init(_VectorStoreRecordCollection__context: object | ``None`` = `

`None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

This is different then the _inner_x methods

as this is a public method.

**serialize**

Serialize the data model to the store model.

This method follows the following steps:

1\. Check if the data model has a serialize method.

Use that method to serialize and return the result.

2\. Serialize the records into a dict, using the data model specific method.

3\. Convert the dict to the store model, using the store specific method.

If overriding this method, make sure to first try to serialize the data model
to the

store model, before doing the store specific version, the user supplied
version should

have precedence.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

**Exceptions**

`serialize(records: TModel | Sequence[TModel], **kwargs: Any) -> Any | `

`Sequence[Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

**upsert**

Upsert a record.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**record**`

Required*

The record.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

**Type**

**Description**

The key of the upserted record or a list of keys, when a container type is
used.

`async`` upsert(record: TModel, embedding_generation_function: `

`Callable[[TModel, type[TModel] | ``None``, VectorStoreRecordDefinition | `

`None``], Awaitable[TModel]] | ``None`` = ``None``, **kwargs: Any) -> TKey | `

`Sequence[TKey] | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**upsert_batch**

Upsert a batch of records.

If the key of the record already exists, the existing record will be updated.
If the key

does not exist, a new record will be created.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

The records to upsert, can be a list of records, or a

single container.

`**embedding_generation_function**`

Required*

Supply this function to generate embeddings. This will

be called with the data model definition and the

records, should return the records with vectors. This

can be supplied by using the add_vector_to_records

method from the VectorStoreRecordUtils.

Default value: None

`****kwargs**`

Required*

Additional arguments.

**Returns**

ﾉ

**Expand table**

`async`` upsert_batch(records: TModel | Sequence[TModel], `

`embedding_generation_function: Callable[[TModel | Sequence[TModel], `

`type[TModel] | ``None``, VectorStoreRecordDefinition | ``None``], `

`Awaitable[TModel | Sequence[TModel]]] | ``None`` = ``None``, **kwargs: Any) -> `

`Sequence[TKey]`

ﾉ

**Expand table**



**Type**

**Description**

Sequence[<xref:

TKey>]

The keys of the upserted records, this is always a list, corresponds to the

input or the items in the container.

**Exceptions**

**Type**

**Description**

VectorStoreModelSerializationException

If an error occurs during serialization.

VectorStoreOperationException

If an error occurs during upserting.

**collection_name**

Python

**data_model_definition**

Python

**data_model_type**

Python

**is_experimental**

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**

`collection_name: str`

`data_model_definition: VectorStoreRecordDefinition`

`data_model_type: type[TModel]`



Python

**managed_client**

Python

**stage_status**

Python

**supported_key_types**

Python

**supported_vector_types**

Python

`is_experimental = ``True`

`managed_client: bool`

`stage_status = ``'experimental'`

`supported_key_types: ClassVar[list[str] | ``None``] = ``None`

`supported_vector_types: ClassVar[list[str] | ``None``] = ``None`



**VectorStoreRecordDataField Class**

Reference

Memory record data field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**has_embedding**`

Default value: False

`**embedding_property_name**`

Default value: None

`**is_filterable**`

Default value: None

`**is_full_text_searchable**`

Default value: None

**embedding_property_name**

Python

`VectorStoreRecordDataField(name: str = ``''``, property_type: str | ``None`` = ``None``, `

`has_embedding: bool = ``False``, embedding_property_name: str | ``None`` = ``None``, `

`is_filterable: bool | ``None`` = ``None``, is_full_text_searchable: bool | ``None`` = `

`None``)`

ﾉ

**Expand table**

**Attributes**

`embedding_property_name: str | ``None`` = ``None`



**has_embedding**

Python

**is_experimental**

Python

**is_filterable**

Python

**is_full_text_searchable**

Python

**stage_status**

Python

`has_embedding: bool = ``False`

`is_experimental = ``True`

`is_filterable: bool | ``None`` = ``None`

`is_full_text_searchable: bool | ``None`` = ``None`

`stage_status = ``'experimental'`



**VectorStoreRecordDefinition Class**

Reference

Memory record definition.

Args: fields: The fields of the record. container_mode: Whether the record is
in container

mode. to_dict: The to_dict function, should take a record and return a list of
dicts.

from_dict: The from_dict function, should take a list of dicts and return a
record.

serialize: The serialize function, should take a record and return the type
specific to a

datastore. deserialize: The deserialize function, should take a type specific
to a datastore

and return a record.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

`**container_mode**`

Default value: False

`**to_dict**`

Default value: None

`**from_dict**`

Default value: None

`**serialize**`

Default value: None

`**deserialize**`

Default value: None

`VectorStoreRecordDefinition(fields: dict[str, VectorStoreRecordFields], `

`container_mode: bool = ``False``, to_dict: ToDictFunctionProtocol | ``None`` = ``None``, `

`from_dict: FromDictFunctionProtocol | ``None`` = ``None``, serialize: `

`SerializeFunctionProtocol | ``None`` = ``None``, deserialize: `

`DeserializeFunctionProtocol | ``None`` = ``None``)`

ﾉ

**Expand table**



get_field_names

Get the names of the fields.

try_get_vector_field

Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector

fields are present None is returned.

**get_field_names**

Get the names of the fields.

Python

**Parameters**

**Name**

**Description**

`**include_vector_fields**`

Whether to include vector fields.

Default value: True

`**include_key_field**`

Whether to include the key field.

Default value: True

**Returns**

**Type**

**Description**

list[str]

The names of the fields.

**try_get_vector_field**

**Methods**

ﾉ

**Expand table**

`get_field_names(include_vector_fields: bool = ``True``, include_key_field: `

`bool = ``True``) -> list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Try to get the vector field.

If the field_name is None, then the first vector field is returned. If no
vector fields are

present None is returned.

Python

**Parameters**

**Name**

**Description**

`**field_name**`

The field name.

Default value: None

**Returns**

**Type**

**Description**

VectorStoreRecordVectorField | None

The vector field or None.

**field_names**

Get the names of the fields.

**key_field**

Get the key field.

**non_vector_field_names**

Get the names of all the non-vector fields.

`try_get_vector_field(field_name: str | ``None`` = ``None``) -> `

`VectorStoreRecordVectorField | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**vector_field_names**

Get the names of the vector fields.

**vector_fields**

Get the names of the vector fields.

**container_mode**

Python

**deserialize**

Python

**fields**

Python

**from_dict**

Python

**is_experimental**

Python

`container_mode: bool = ``False`

`deserialize: DeserializeFunctionProtocol | ``None`` = ``None`

`fields: dict[str, VectorStoreRecordFields]`

`from_dict: FromDictFunctionProtocol | ``None`` = ``None`

`is_experimental = ``True`



**key_field_name**

Python

**serialize**

Python

**stage_status**

Python

**to_dict**

Python

`key_field_name: str`

`serialize: SerializeFunctionProtocol | ``None`` = ``None`

`stage_status = ``'experimental'`

`to_dict: ToDictFunctionProtocol | ``None`` = ``None`



**VectorStoreRecordKeyField Class**

Reference

Memory record key field.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

**is_experimental**

Python

**stage_status**

Python

`VectorStoreRecordKeyField(name: str = ``''``, property_type: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordUtils Class**

Reference

Helper class to easily add embeddings to a (set of) vector store record.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the VectorStoreRecordUtils with a kernel.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

add_vector_to_records

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data

fields, if they have a vector field, looks up that vector field and checks if

is a local embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings

to make.

`VectorStoreRecordUtils(kernel: Kernel)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Optional arguments are passed onto the Kernel

add_embedding_to_object call.

**add_vector_to_records**

Vectorize the vector record.

This function can be passed to upsert or upsert batch of a

VectorStoreRecordCollection.

Loops through the fields of the data model definition, looks at data fields,
if they

have a vector field, looks up that vector field and checks if is a local
embedding.

If so adds that to a list of embeddings to make.

Finally calls Kernel add_embedding_to_object with the list of embeddings to
make.

Optional arguments are passed onto the Kernel add_embedding_to_object call.

Python

**Parameters**

**Name**

**Description**

`**records**`

Required*

`**data_model_type**`

Required*

Default value: None

`**data_model_definition**`

Required*

Default value: None

**is_experimental**

`async`` add_vector_to_records(records: TModel | Sequence[TModel], `

`data_model_type: type[TModel] | ``None`` = ``None``, data_model_definition: `

`VectorStoreRecordDefinition | ``None`` = ``None``, **kwargs) -> TModel | `

`Sequence[TModel]`

ﾉ

**Expand table**

**Attributes**



Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorStoreRecordVectorField Class**

Reference

Memory record vector field.

Most vectors stores use a _list[float]_ as the data type for vectors. This is
the default and all

vector stores in SK use this internally. But in your class you may want to use
a numpy

array or some other optimized type, in order to support that, you can set the

deserialize_function to a function that takes a list of floats and returns the
optimized

type, and then also supply a serialize_function that takes the optimized type
and returns

a list of floats.

For instance for numpy, that would be _serialize_function=np.ndarray.tolist_
and

_deserialize_function=np.array_ , (with _import numpy as np_ at the top of
your file). if you

want to set it up with more specific options, use a lambda, a custom function
or a

partial.

Args: property_type (str, optional): Property type. For vectors this should be
the inner

type of the vector. By default the vector will be a list of numbers. If you
want to use a

numpy array or some other optimized format, set the cast_function with a
function that

takes a list of floats and returns a numpy array.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

` local_embedding (bool, optional): Whether to embed the vector locally. `

`Defaults to True.`

` embedding_settings (dict[str, PromptExecutionSettings], optional): `

`Embedding settings.`

` The key is the name of the embedding service to use, can be multiple `

`ones.`

` serialize_function (Callable[[Any], list[float | int]], optional): `

`Serialize function,`

` should take the vector and return a list of numbers.`

` deserialize_function (Callable[[list[float | int]], Any], optional): `

`Deserialize function,`

` should take a list of numbers and return the vector.`



Python

**Parameters**

**Name**

**Description**

`**name**`

`**property_type**`

Default value: None

`**local_embedding**`

Default value: True

`**dimensions**`

Default value: None

`**index_kind**`

Default value: None

`**distance_function**`

Default value: None

`**embedding_settings**`

Default value: <factory>

`**serialize_function**`

Default value: None

`**deserialize_function**`

Default value: None

**deserialize_function**

Python

`VectorStoreRecordVectorField(name: str = ``''``, property_type: str | ``None`` = `

`None``, local_embedding: bool = ``True``, dimensions: int | ``None`` = ``None``, `

`index_kind: ~semantic_kernel.data.const.IndexKind | ``None`` = ``None``, `

`distance_function: ~semantic_kernel.data.const.DistanceFunction | ``None`` = `

`None``, embedding_settings: dict[str, `

`~semantic_kernel.connectors.ai.prompt_execution_settings.PromptExecutionSett`

`ings] = <factory>, serialize_function: `

`~collections.abc.Callable[[~typing.Any], list[float | int]] | ``None`` = ``None``, `

`deserialize_function: ~collections.abc.Callable[[list[float | int]], `

`~typing.Any] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**

`deserialize_function: Callable[[list[float | int]], Any] | ``None`` = ``None`



**dimensions**

Python

**distance_function**

Python

**embedding_settings**

Python

**index_kind**

Python

**is_experimental**

Python

**local_embedding**

Python

`dimensions: int | ``None`` = ``None`

`distance_function: DistanceFunction | ``None`` = ``None`

`embedding_settings: dict[str, PromptExecutionSettings] = `

`FieldInfo(annotation=dict[str, PromptExecutionSettings], required=``False``,
`

`default_factory=dict)`

`index_kind: IndexKind | ``None`` = ``None`

`is_experimental = ``True`

`local_embedding: bool = ``True`



**serialize_function**

Python

**stage_status**

Python

`serialize_function: Callable[[Any], list[float | int]] | ``None`` = ``None`

`stage_status = ``'experimental'`



**VectorStoreTextSearch Class**

Reference

Class that wraps a Vector Store Record Collection to expose as a Text Search.

Preferably the class methods are used to create an instance of this class.
Otherwise the

search executes in the following order depending on which store was set:

1\. vectorizable_text_search

2\. vector_text_search

3\. vectorized_search (after calling the embedding service)

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

A vector store record collection with a method to search for

vectorizable text.

`**vectorized_search**`

Required*

A vector store record collection with a method to search for

vectors.

`**vector_text_search**`

`VectorStoreTextSearch(*, vectorizable_text_search: `

`VectorizableTextSearchMixin | ``None`` = ``None``, vectorized_search: `

`VectorizedSearchMixin | ``None`` = ``None``, vector_text_search: `

`VectorTextSearchMixin | ``None`` = ``None``, embedding_service: `

`EmbeddingGeneratorBase | ``None`` = ``None``, string_mapper: Callable[[TModel], str] `

`| ``None`` = ``None``, text_search_results_mapper: Callable[[TModel], `

`TextSearchResult] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

A vector store record collection with a method to search for

text.

`**embedding_service**`

Required*

An embedding service to use for vectorized search.

`**string_mapper**`

Required*

A function to map a record to a string.

`**text_search_results_mapper**`

Required*

A function to map a record to a TextSearchResult.

**Keyword-Only Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**vectorized_search**`

Required*

`**vector_text_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

`**text_search_results_mapper**`

Required*

from_vector_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

from_vectorizable_text_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



from_vectorized_search

Create a new VectorStoreTextSearch from a

VectorStoreRecordCollection.

get_search_results

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

get_text_search_results

Search for a query, returning a KernelSearchResult with a

TextSearchResult as the results type.

search

Search for a query, returning a KernelSearchResult with a string as

the results type.

**from_vector_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vector_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorizable_text_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

`from_vector_text_search(vector_text_search: VectorTextSearchMixin, `

`string_mapper: Callable | ``None`` = ``None``, text_search_results_mapper: `

`Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorizable_text_search(vectorizable_text_search: `

`VectorizableTextSearchMixin, string_mapper: Callable | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**vectorizable_text_search**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

**from_vectorized_search**

Create a new VectorStoreTextSearch from a VectorStoreRecordCollection.

Python

**Parameters**

**Name**

**Description**

`**vectorized_search**`

Required*

`**embedding_service**`

Required*

`**string_mapper**`

Required*

Default value: None

`**text_search_results_mapper**`

Required*

Default value: None

`text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

`from_vectorized_search(vectorized_search: VectorizedSearchMixin, `

`embedding_service: EmbeddingGeneratorBase, string_mapper: Callable | ``None`` `

`= ``None``, text_search_results_mapper: Callable | ``None`` = ``None``, **kwargs: `

`Any) -> _T`

ﾉ

**Expand table**



**get_search_results**

Search for a query, returning a KernelSearchResult with a

VectorSearchResult[TModel] as the results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**get_text_search_results**

Search for a query, returning a KernelSearchResult with a TextSearchResult as
the

results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

`async`` get_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**

`async`` get_text_search_results(query: str, options: SearchOptions | ``None`` = `

`None``, **kwargs: Any) -> KernelSearchResults[TextSearchResult]`

ﾉ

**Expand table**



**search**

Search for a query, returning a KernelSearchResult with a string as the
results type.

Python

**Parameters**

**Name**

**Description**

`**query**`

Required*

`**options**`

Required*

Default value: None

**options_class**

Get the options class.

**embedding_service**

Python

**string_mapper**

Python

**text_search_results_mapper**

`async`` search(query: str, options: SearchOptions | ``None`` = ``None``, **kwargs: `

`Any) -> KernelSearchResults[str]`

ﾉ

**Expand table**

**Attributes**

`embedding_service: EmbeddingGeneratorBase | ``None`

`string_mapper: Callable[[TModel], str] | ``None`



Python

**vector_text_search**

Python

**vectorizable_text_search**

Python

**vectorized_search**

Python

`text_search_results_mapper: Callable[[TModel], TextSearchResult] | ``None`

`vector_text_search: VectorTextSearchMixin | ``None`

`vectorizable_text_search: VectorizableTextSearchMixin | ``None`

`vectorized_search: VectorizedSearchMixin | ``None`



**VectorTextSearchMixin Class**

Reference

The mixin for text search, to be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

text_search

Search the vector store for records that match the given text and filters.

**text_search**

Search the vector store for records that match the given text and filters.

Python

**Parameters**

**Name**

**Description**

`**search_text**`

Required*

The query to search for.

`**options**`

Required*

options, should include query_text

Default value: None

`VectorTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` text_search(search_text: str, options: SearchOptions | ``None`` = ``None``, `

`**kwargs: Any) -> KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorizableTextSearchMixin Class**

Reference

The mixin for searching with text that get's vectorized downstream.

To be used in combination with VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorizable_text_search

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector

search.

**vectorizable_text_search**

Search the vector store for records that match the given text and filter.

The text string will be vectorized downstream and used for the vector search.

Python

**Parameters**

`VectorizableTextSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorizable_text_search(vectorizable_text: str, options: `

`SearchOptions | ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**vectorizable_text**`

Required*

The text to search for, will be vectorized downstream.

`**options**`

Required*

options for the search

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**VectorizedSearchMixin Class**

Reference

The mixin for searching with vectors. To be used in combination with
VectorSearchBase.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

vectorized_search

Search the vector store for records that match the given vector (embedding)

and filter.

**vectorized_search**

Search the vector store for records that match the given vector (embedding)
and

filter.

Python

**Parameters**

**Name**

**Description**

`**vector**`

Required*

The vector to search for.

`VectorizedSearchMixin()`

**Methods**

ﾉ

**Expand table**

`async`` vectorized_search(vector: list[float | int], options: SearchOptions `

`| ``None`` = ``None``, **kwargs: Any) -> `

`KernelSearchResults[VectorSearchResult[TModel]]`

ﾉ

**Expand table**



**Name**

**Description**

`**options**`

Required*

options, should include query_text

Default value: None

`****kwargs**`

Required*

if options are not set, this is used to create them.

**Exceptions**

**Type**

**Description**

VectorSearchExecutionException

If an error occurs during the search.

VectorStoreModelDeserializationException

If an error occurs during deserialization.

VectorSearchOptionsException

If the search options are invalid.

VectorStoreMixinException

raised when the method is not used in

combination with the VectorSearchBase.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**exceptions Package**

Reference

**Modules**

agent_exceptions

content_exceptions

filter_exceptions

function_exceptions

kernel_exceptions

memory_connector_exceptions

planner_exceptions

process_exceptions

search_exceptions

service_exceptions

template_engine_exceptions

vector_store_exceptions

ﾉ

**Expand table**



**agent_exceptions Module**

Reference

**Classes**

AgentChatException

An error occurred while invoking the agent chat.

AgentChatHistoryReducerException

An error occurred while reducing the chat history.

AgentException

Base class for all agent exceptions.

AgentExecutionException

An error occurred while executing the agent.

AgentFileNotFoundException

The requested file was not found.

AgentInitializationException

An error occurred while initializing the agent.

AgentInvokeException

An error occurred while invoking the agent.

ﾉ

**Expand table**



**AgentChatException Class**

Reference

An error occurred while invoking the agent chat.

**Constructor**

Python

`AgentChatException()`



**AgentChatHistoryReducerException**

**Class**

Reference

An error occurred while reducing the chat history.

**Constructor**

Python

`AgentChatHistoryReducerException()`



**AgentException Class**

Reference

Base class for all agent exceptions.

**Constructor**

Python

`AgentException()`



**AgentExecutionException Class**

Reference

An error occurred while executing the agent.

**Constructor**

Python

`AgentExecutionException()`



**AgentFileNotFoundException Class**

Reference

The requested file was not found.

**Constructor**

Python

`AgentFileNotFoundException()`



**AgentInitializationException Class**

Reference

An error occurred while initializing the agent.

**Constructor**

Python

`AgentInitializationException()`



**AgentInvokeException Class**

Reference

An error occurred while invoking the agent.

**Constructor**

Python

`AgentInvokeException()`



**content_exceptions Module**

Reference

**Classes**

ChatHistoryReducerException

An error occurred while reducing chat history.

ContentAdditionException

An error occurred while adding content.

ContentException

Base class for all content exceptions.

ContentInitializationError

An error occurred while initializing the content.

ContentSerializationError

An error occurred while serializing the content.

FunctionCallInvalidArgumentsException

An error occurred while validating the function

arguments.

FunctionCallInvalidNameException

An error occurred while validating the function name.

ﾉ

**Expand table**



**ChatHistoryReducerException Class**

Reference

An error occurred while reducing chat history.

**Constructor**

Python

`ChatHistoryReducerException()`



**ContentAdditionException Class**

Reference

An error occurred while adding content.

**Constructor**

Python

`ContentAdditionException()`



**ContentException Class**

Reference

Base class for all content exceptions.

**Constructor**

Python

`ContentException()`



**ContentInitializationError Class**

Reference

An error occurred while initializing the content.

**Constructor**

Python

`ContentInitializationError()`



**ContentSerializationError Class**

Reference

An error occurred while serializing the content.

**Constructor**

Python

`ContentSerializationError()`



**FunctionCallInvalidArgumentsException**

**Class**

Reference

An error occurred while validating the function arguments.

**Constructor**

Python

`FunctionCallInvalidArgumentsException()`



**FunctionCallInvalidNameException Class**

Reference

An error occurred while validating the function name.

**Constructor**

Python

`FunctionCallInvalidNameException()`



**filter_exceptions Module**

Reference

**Classes**

FilterException

Base class for all filter exceptions.

FilterManagementException

An error occurred while adding or removing the filter to/from the

kernel.

ﾉ

**Expand table**



**FilterException Class**

Reference

Base class for all filter exceptions.

**Constructor**

Python

`FilterException()`



**FilterManagementException Class**

Reference

An error occurred while adding or removing the filter to/from the kernel.

**Constructor**

Python

`FilterManagementException()`



**function_exceptions Module**

Reference

**Classes**

FunctionException

Base class for all function exceptions.

FunctionExecutionException

Base class for all function execution exceptions.

FunctionInitializationError

An error occurred while initializing the function.

Adds the context of the error to the generic message.

FunctionInvalidNameError

An error occurred while validating the function name.

FunctionInvalidParamNameError

An error occurred while validating the function parameter

name.

FunctionNameNotUniqueError

An error occurred while validating the function name.

FunctionResultError

An error occurred while validating the function result.

FunctionSyntaxError

Base class for all function syntax exceptions.

PluginInitializationError

An error occurred while initializing the plugin.

PluginInvalidNameError

An error occurred while validating the plugin name.

PromptRenderingException

An error occurred while rendering a prompt.

ﾉ

**Expand table**



**FunctionException Class**

Reference

Base class for all function exceptions.

**Constructor**

Python

`FunctionException()`



**FunctionExecutionException Class**

Reference

Base class for all function execution exceptions.

**Constructor**

Python

`FunctionExecutionException()`



**FunctionInitializationError Class**

Reference

An error occurred while initializing the function.

Adds the context of the error to the generic message.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

`FunctionInitializationError(message: str)`

ﾉ

**Expand table**



**FunctionInvalidNameError Class**

Reference

An error occurred while validating the function name.

**Constructor**

Python

`FunctionInvalidNameError()`



**FunctionInvalidParamNameError Class**

Reference

An error occurred while validating the function parameter name.

**Constructor**

Python

`FunctionInvalidParamNameError()`



**FunctionNameNotUniqueError Class**

Reference

An error occurred while validating the function name.

**Constructor**

Python

`FunctionNameNotUniqueError()`



**FunctionResultError Class**

Reference

An error occurred while validating the function result.

**Constructor**

Python

`FunctionResultError()`



**FunctionSyntaxError Class**

Reference

Base class for all function syntax exceptions.

**Constructor**

Python

`FunctionSyntaxError()`



**PluginInitializationError Class**

Reference

An error occurred while initializing the plugin.

**Constructor**

Python

`PluginInitializationError()`



**PluginInvalidNameError Class**

Reference

An error occurred while validating the plugin name.

**Constructor**

Python

`PluginInvalidNameError()`



**PromptRenderingException Class**

Reference

An error occurred while rendering a prompt.

**Constructor**

Python

`PromptRenderingException()`



**kernel_exceptions Module**

Reference

**Classes**

KernelException

The base class for all Semantic Kernel exceptions.

KernelFunctionAlreadyExistsError

Raised when a function is already registered in the kernel.

KernelFunctionNotFoundError

Raised when a function is not found in the kernel.

KernelInvokeException

Raised when an error occurs while invoking a function in

the kernel.

KernelPluginInvalidConfigurationError

Raised when a plugin configuration is invalid.

KernelPluginNotFoundError

Raised when a plugin is not found in the kernel.

KernelServiceNotFoundError

Raised when a service is not found in the kernel.

OperationCancelledException

Raised when an operation is cancelled.

ﾉ

**Expand table**



**KernelException Class**

Reference

The base class for all Semantic Kernel exceptions.

**Constructor**

Python

`KernelException()`



**KernelFunctionAlreadyExistsError Class**

Reference

Raised when a function is already registered in the kernel.

**Constructor**

Python

`KernelFunctionAlreadyExistsError()`



**KernelFunctionNotFoundError Class**

Reference

Raised when a function is not found in the kernel.

**Constructor**

Python

`KernelFunctionNotFoundError()`



**KernelInvokeException Class**

Reference

Raised when an error occurs while invoking a function in the kernel.

**Constructor**

Python

`KernelInvokeException()`



**KernelPluginInvalidConfigurationError**

**Class**

Reference

Raised when a plugin configuration is invalid.

**Constructor**

Python

`KernelPluginInvalidConfigurationError()`



**KernelPluginNotFoundError Class**

Reference

Raised when a plugin is not found in the kernel.

**Constructor**

Python

`KernelPluginNotFoundError()`



**KernelServiceNotFoundError Class**

Reference

Raised when a service is not found in the kernel.

**Constructor**

Python

`KernelServiceNotFoundError()`



**OperationCancelledException Class**

Reference

Raised when an operation is cancelled.

**Constructor**

Python

`OperationCancelledException()`



**memory_connector_exceptions Module**

Reference

**Classes**

MemoryConnectorConnectionException

An error occurred while connecting to the memory

connector.

MemoryConnectorException

Base class for all memory connector exceptions.

MemoryConnectorInitializationError

An error occurred while initializing the memory

connector.

MemoryConnectorResourceNotFound

The requested resource was not found in the memory

connector.

ﾉ

**Expand table**



**MemoryConnectorConnectionException**

**Class**

Reference

An error occurred while connecting to the memory connector.

**Constructor**

Python

`MemoryConnectorConnectionException()`



**MemoryConnectorException Class**

Reference

Base class for all memory connector exceptions.

**Constructor**

Python

`MemoryConnectorException()`



**MemoryConnectorInitializationError**

**Class**

Reference

An error occurred while initializing the memory connector.

**Constructor**

Python

`MemoryConnectorInitializationError()`



**MemoryConnectorResourceNotFound**

**Class**

Reference

The requested resource was not found in the memory connector.

**Constructor**

Python

`MemoryConnectorResourceNotFound()`



**planner_exceptions Module**

Reference

**Classes**

PlannerCreatePlanError

An error occurred while creating the plan.

PlannerException

Base class for all planner exceptions.

PlannerExecutionException

Base class for all planner execution exceptions.

PlannerInvalidConfigurationError

An error occurred while validating the configuration.

PlannerInvalidGoalError

An error occurred while validating the goal.

PlannerInvalidPlanError

An error occurred while validating the plan.

ﾉ

**Expand table**



**PlannerCreatePlanError Class**

Reference

An error occurred while creating the plan.

**Constructor**

Python

`PlannerCreatePlanError()`



**PlannerException Class**

Reference

Base class for all planner exceptions.

**Constructor**

Python

`PlannerException()`



**PlannerExecutionException Class**

Reference

Base class for all planner execution exceptions.

**Constructor**

Python

`PlannerExecutionException()`



**PlannerInvalidConfigurationError Class**

Reference

An error occurred while validating the configuration.

**Constructor**

Python

`PlannerInvalidConfigurationError()`



**PlannerInvalidGoalError Class**

Reference

An error occurred while validating the goal.

**Constructor**

Python

`PlannerInvalidGoalError()`



**PlannerInvalidPlanError Class**

Reference

An error occurred while validating the plan.

**Constructor**

Python

`PlannerInvalidPlanError()`



**process_exceptions Module**

Reference

**Classes**

ProcessEventUndefinedException

An event was not defined in the process.

ProcessException

Base class for all process exceptions.

ProcessFunctionNotFoundException

A function was not found in the process.

ProcessInvalidConfigurationException

An invalid configuration was provided for the

process.

ProcessTargetFunctionNameMismatchException

A message targeting a function has resulted in

a different function becoming invocable.

ﾉ

**Expand table**



**ProcessEventUndefinedException Class**

Reference

An event was not defined in the process.

**Constructor**

Python

`ProcessEventUndefinedException()`



**ProcessException Class**

Reference

Base class for all process exceptions.

**Constructor**

Python

`ProcessException()`



**ProcessFunctionNotFoundException**

**Class**

Reference

A function was not found in the process.

**Constructor**

Python

`ProcessFunctionNotFoundException()`



**ProcessInvalidConfigurationException**

**Class**

Reference

An invalid configuration was provided for the process.

**Constructor**

Python

`ProcessInvalidConfigurationException()`



**ProcessTargetFunctionNameMismatch**

**Exception Class**

Reference

A message targeting a function has resulted in a different function becoming
invocable.

**Constructor**

Python

`ProcessTargetFunctionNameMismatchException()`



**search_exceptions Module**

Reference

**Classes**

SearchException

Base class for all Search related exceptions.

SearchResultEmptyError

Raised when there are no hits in the search results.

TextSearchException

An error occurred while executing a text search function.

TextSearchOptionsException

Raised when invalid options are given to a TextSearch function.

ﾉ

**Expand table**



**SearchException Class**

Reference

Base class for all Search related exceptions.

**Constructor**

Python

`SearchException()`



**SearchResultEmptyError Class**

Reference

Raised when there are no hits in the search results.

**Constructor**

Python

`SearchResultEmptyError()`



**TextSearchException Class**

Reference

An error occurred while executing a text search function.

**Constructor**

Python

`TextSearchException()`



**TextSearchOptionsException Class**

Reference

Raised when invalid options are given to a TextSearch function.

**Constructor**

Python

`TextSearchOptionsException()`



**service_exceptions Module**

Reference

**Classes**

ServiceContentFilterException

An error was raised by the content filter of the service.

ServiceException

Base class for all service exceptions.

ServiceInitializationError

An error occurred while initializing the service.

ServiceInvalidAuthError

An error occurred while authenticating the service.

ServiceInvalidExecutionSettingsError

An error occurred while validating the execution settings of

the service.

ServiceInvalidRequestError

An error occurred while validating the request to the

service.

ServiceInvalidResponseError

An error occurred while validating the response from the

service.

ServiceInvalidTypeError

An error occurred while validating the type of the service

request.

ServiceResourceNotFoundError

The request service could not be found.

ServiceResponseException

Base class for all service response exceptions.

ﾉ

**Expand table**



**ServiceContentFilterException Class**

Reference

An error was raised by the content filter of the service.

**Constructor**

Python

`ServiceContentFilterException()`



**ServiceException Class**

Reference

Base class for all service exceptions.

**Constructor**

Python

`ServiceException()`



**ServiceInitializationError Class**

Reference

An error occurred while initializing the service.

**Constructor**

Python

`ServiceInitializationError()`



**ServiceInvalidAuthError Class**

Reference

An error occurred while authenticating the service.

**Constructor**

Python

`ServiceInvalidAuthError()`



**ServiceInvalidExecutionSettingsError**

**Class**

Reference

An error occurred while validating the execution settings of the service.

**Constructor**

Python

`ServiceInvalidExecutionSettingsError()`



**ServiceInvalidRequestError Class**

Reference

An error occurred while validating the request to the service.

**Constructor**

Python

`ServiceInvalidRequestError()`



**ServiceInvalidResponseError Class**

Reference

An error occurred while validating the response from the service.

**Constructor**

Python

`ServiceInvalidResponseError()`



**ServiceInvalidTypeError Class**

Reference

An error occurred while validating the type of the service request.

**Constructor**

Python

`ServiceInvalidTypeError()`



**ServiceResourceNotFoundError Class**

Reference

The request service could not be found.

**Constructor**

Python

`ServiceResourceNotFoundError()`



**ServiceResponseException Class**

Reference

Base class for all service response exceptions.

**Constructor**

Python

`ServiceResponseException()`



**template_engine_exceptions Module**

Reference

**Classes**

BlockException

Base class for all block exceptions.

BlockRenderException

An error occurred while rendering a block.

BlockSyntaxError

A invalid block syntax was found.

CodeBlockRenderException

An error occurred while rendering a CodeBlock.

CodeBlockSyntaxError

A invalid CodeBlock syntax was found.

CodeBlockTokenError

An error occurred while tokenizing a CodeBlock.

FunctionIdBlockSyntaxError

A invalid FunctionIdBlock syntax was found.

Adds the context of the error to the generic message.

HandlebarsTemplateRenderException

An error occurred while rendering a HandlebarsTemplate.

HandlebarsTemplateSyntaxError

A invalid HandlebarsTemplate syntax was found.

Jinja2TemplateRenderException

An error occurred while rendering a Jinja2Template.

Jinja2TemplateSyntaxError

A invalid Jinja2Template syntax was found.

NamedArgBlockSyntaxError

A invalid NamedArgBlock syntax was found.

Adds the context of the error to the generic message.

TemplateRenderException

An error occurred while rendering a Template.

TemplateSyntaxError

A invalid Template syntax was found.

ValBlockSyntaxError

A invalid ValBlock syntax was found.

Adds the context of the error to the generic message.

VarBlockRenderError

An error occurred while rendering a VarBlock.

VarBlockSyntaxError

A invalid VarBlock syntax was found.

Adds the context of the error to the generic message.

ﾉ

**Expand table**



**BlockException Class**

Reference

Base class for all block exceptions.

**Constructor**

Python

`BlockException()`



**BlockRenderException Class**

Reference

An error occurred while rendering a block.

**Constructor**

Python

`BlockRenderException()`



**BlockSyntaxError Class**

Reference

A invalid block syntax was found.

**Constructor**

Python

`BlockSyntaxError()`



**CodeBlockRenderException Class**

Reference

An error occurred while rendering a CodeBlock.

**Constructor**

Python

`CodeBlockRenderException()`



**CodeBlockSyntaxError Class**

Reference

A invalid CodeBlock syntax was found.

**Constructor**

Python

`CodeBlockSyntaxError()`



**CodeBlockTokenError Class**

Reference

An error occurred while tokenizing a CodeBlock.

**Constructor**

Python

`CodeBlockTokenError()`



**FunctionIdBlockSyntaxError Class**

Reference

A invalid FunctionIdBlock syntax was found.

Adds the context of the error to the generic message.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

`FunctionIdBlockSyntaxError(content: str)`

ﾉ

**Expand table**



**HandlebarsTemplateRenderException**

**Class**

Reference

An error occurred while rendering a HandlebarsTemplate.

**Constructor**

Python

`HandlebarsTemplateRenderException()`



**HandlebarsTemplateSyntaxError Class**

Reference

A invalid HandlebarsTemplate syntax was found.

**Constructor**

Python

`HandlebarsTemplateSyntaxError()`



**Jinja2TemplateRenderException Class**

Reference

An error occurred while rendering a Jinja2Template.

**Constructor**

Python

`Jinja2TemplateRenderException()`



**Jinja2TemplateSyntaxError Class**

Reference

A invalid Jinja2Template syntax was found.

**Constructor**

Python

`Jinja2TemplateSyntaxError()`



**NamedArgBlockSyntaxError Class**

Reference

A invalid NamedArgBlock syntax was found.

Adds the context of the error to the generic message.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

`NamedArgBlockSyntaxError(content: str)`

ﾉ

**Expand table**



**TemplateRenderException Class**

Reference

An error occurred while rendering a Template.

**Constructor**

Python

`TemplateRenderException()`



**TemplateSyntaxError Class**

Reference

A invalid Template syntax was found.

**Constructor**

Python

`TemplateSyntaxError()`



**ValBlockSyntaxError Class**

Reference

A invalid ValBlock syntax was found.

Adds the context of the error to the generic message.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

`ValBlockSyntaxError(content: str)`

ﾉ

**Expand table**



**VarBlockRenderError Class**

Reference

An error occurred while rendering a VarBlock.

**Constructor**

Python

`VarBlockRenderError()`



**VarBlockSyntaxError Class**

Reference

A invalid VarBlock syntax was found.

Adds the context of the error to the generic message.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

`VarBlockSyntaxError(content: str)`

ﾉ

**Expand table**



**vector_store_exceptions Module**

Reference

**Classes**

VectorSearchExecutionException

Raised when there is an error executing a

VectorSearch function.

VectorSearchOptionsException

Raised when invalid options are given to a

VectorSearch function.

VectorStoreException

Base class for all vector store exceptions.

VectorStoreInitializationException

Class for all vector store initialization exceptions.

VectorStoreMixinException

Raised when a mixin is used without the

VectorSearchBase Class.

VectorStoreModelDeserializationException

An error occurred while deserializing the vector store

model.

VectorStoreModelException

Base class for all vector store model exceptions.

VectorStoreModelSerializationException

An error occurred while serializing the vector store

model.

VectorStoreModelValidationError

An error occurred while validating the vector store

model.

VectorStoreOperationException

An error occurred while performing an operation on

the vector store record collection.

ﾉ

**Expand table**



**VectorSearchExecutionException Class**

Reference

Raised when there is an error executing a VectorSearch function.

**Constructor**

Python

`VectorSearchExecutionException()`



**VectorSearchOptionsException Class**

Reference

Raised when invalid options are given to a VectorSearch function.

**Constructor**

Python

`VectorSearchOptionsException()`



**VectorStoreException Class**

Reference

Base class for all vector store exceptions.

**Constructor**

Python

`VectorStoreException()`



**VectorStoreInitializationException Class**

Reference

Class for all vector store initialization exceptions.

**Constructor**

Python

`VectorStoreInitializationException()`



**VectorStoreMixinException Class**

Reference

Raised when a mixin is used without the VectorSearchBase Class.

**Constructor**

Python

`VectorStoreMixinException()`



**VectorStoreModelDeserialization**

**Exception Class**

Reference

An error occurred while deserializing the vector store model.

**Constructor**

Python

`VectorStoreModelDeserializationException()`



**VectorStoreModelException Class**

Reference

Base class for all vector store model exceptions.

**Constructor**

Python

`VectorStoreModelException()`



**VectorStoreModelSerializationException**

**Class**

Reference

An error occurred while serializing the vector store model.

**Constructor**

Python

`VectorStoreModelSerializationException()`



**VectorStoreModelValidationError Class**

Reference

An error occurred while validating the vector store model.

**Constructor**

Python

`VectorStoreModelValidationError()`



**VectorStoreOperationException Class**

Reference

An error occurred while performing an operation on the vector store record
collection.

**Constructor**

Python

`VectorStoreOperationException()`



**filters Package**

Reference

**Packages**

auto_function_invocation

functions

prompts

**Modules**

filter_context_base

filter_types

kernel_filters_extension

**Classes**

AutoFunctionInvocationContext

The context for auto function invocation filtering.

This is the context supplied to the auto function invocation

filters.

Common use case are to alter the function_result, for instance

filling it with a pre-computed value, in order to skip a step, for

instance when doing caching.

Another option is to terminate, this can be done by setting

terminate to True.

Create a new model by parsing and validating input data from

keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

FunctionInvocationContext

The context for function invocation filtering.

This filter can be used to monitor which functions are called. To

log what function was called with which parameters and what

output. Finally it can be used for caching by setting the result

value.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

PromptRenderContext

The context for prompt rendering filtering.

When prompt rendering is expensive (for instance when there

are expensive functions being called.) This filter can be used to

set the rendered_prompt or function result directly and

returning.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

FilterTypes

Enum for the filter types.

ﾉ

**Expand table**



**auto_function_invocation Package**

Reference

**Modules**

auto_function_invocation_context

ﾉ

**Expand table**



**auto_function_invocation_context**

**Module**

Reference

**Classes**

AutoFunctionInvocationContext

The context for auto function invocation filtering.

This is the context supplied to the auto function invocation

filters.

Common use case are to alter the function_result, for instance

filling it with a pre-computed value, in order to skip a step, for

instance when doing caching.

Another option is to terminate, this can be done by setting

terminate to True.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the

input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AutoFunctionInvocationContext Class**

Reference

The context for auto function invocation filtering.

This is the context supplied to the auto function invocation filters.

Common use case are to alter the function_result, for instance filling it with
a pre-

computed value, in order to skip a step, for instance when doing caching.

Another option is to terminate, this can be done by setting terminate to True.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**chat_history**`

The chat history or None.

`AutoFunctionInvocationContext()`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**function_result**`

Required*

The function result or None.

`**request_sequence_index**`

Required*

The request sequence index.

`**function_sequence_index**`

Required*

The function sequence index.

`**function_count**`

Required*

The function count.

`**terminate**`

Required*

The flag to terminate.

**arguments**

Python

**chat_history**

Python

**execution_settings**

Python

**function**

Python

**Attributes**

`arguments: KernelArguments`

`chat_history: ChatHistory | ``None`

`execution_settings: PromptExecutionSettings | ``None`



**function_count**

Python

**function_result**

Python

**function_sequence_index**

Python

**is_streaming**

Python

**kernel**

Python

**request_sequence_index**

Python

`function: KernelFunction`

`function_count: int`

`function_result: FunctionResult | ``None`

`function_sequence_index: int`

`is_streaming: bool`

`kernel: Kernel`

`request_sequence_index: int`



**terminate**

Python

`terminate: bool`



**functions Package**

Reference

**Modules**

function_invocation_context

ﾉ

**Expand table**



**function_invocation_context Module**

Reference

**Classes**

FunctionInvocationContext

The context for function invocation filtering.

This filter can be used to monitor which functions are called. To log

what function was called with which parameters and what output.

Finally it can be used for caching by setting the result value.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FunctionInvocationContext Class**

Reference

The context for function invocation filtering.

This filter can be used to monitor which functions are called. To log what
function was

called with which parameters and what output. Finally it can be used for
caching by

setting the result value.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**result**`

Required*

The result of the function, or None.

`FunctionInvocationContext()`

ﾉ

**Expand table**



**arguments**

Python

**function**

Python

**is_streaming**

Python

**kernel**

Python

**result**

Python

**Attributes**

`arguments: KernelArguments`

`function: KernelFunction`

`is_streaming: bool`

`kernel: Kernel`

`result: FunctionResult | ``None`



**prompts Package**

Reference

**Modules**

prompt_render_context

ﾉ

**Expand table**



**prompt_render_context Module**

Reference

**Classes**

PromptRenderContext

The context for prompt rendering filtering.

When prompt rendering is expensive (for instance when there are

expensive functions being called.) This filter can be used to set the

rendered_prompt or function result directly and returning.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PromptRenderContext Class**

Reference

The context for prompt rendering filtering.

When prompt rendering is expensive (for instance when there are expensive
functions

being called.) This filter can be used to set the rendered_prompt or function
result

directly and returning.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**rendered_prompt**`

Required*

The result of the prompt rendering.

`**function_result**`

The result of the function that used the prompt.

`PromptRenderContext()`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**arguments**

Python

**function**

Python

**function_result**

Python

**is_streaming**

Python

**kernel**

Python

**rendered_prompt**

**Attributes**

`arguments: KernelArguments`

`function: KernelFunction`

`function_result: FunctionResult | ``None`

`is_streaming: bool`

`kernel: Kernel`



Python

`rendered_prompt: str | ``None`



**filter_context_base Module**

Reference

**Classes**

FilterContextBase

Base class for Kernel Filter Contexts.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FilterContextBase Class**

Reference

Base class for Kernel Filter Contexts.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**arguments**

Python

**function**

Python

**is_streaming**

Python

`FilterContextBase()`

**Attributes**

`arguments: KernelArguments`

`function: KernelFunction`

`is_streaming: bool`



**kernel**

Python

`kernel: Kernel`



**filter_types Module**

Reference

**Enums**

FilterTypes

Enum for the filter types.

ﾉ

**Expand table**



**FilterTypes Enum**

Reference

Enum for the filter types.

AUTO_FUNCTION_INVOCATION

FUNCTION_INVOCATION

PROMPT_RENDERING

**Fields**

ﾉ

**Expand table**



**kernel_filters_extension Module**

Reference

**Classes**

KernelFilterExtension

KernelFilterExtension.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFilterExtension Class**

Reference

KernelFilterExtension.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**function_invocation_filters**`

Required*

`**prompt_rendering_filters**`

Required*

`**auto_function_invocation_filters**`

Required*

`KernelFilterExtension(*, function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``,
prompt_rendering_filters: `

`list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]], Awaitable[``None``]]]]
= `

`None``, auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``)`

ﾉ

**Expand table**

**Methods**



add_filter

Add a filter to the Kernel.

Each filter is added to the beginning of the list of filters, this is because
the

filters are executed in the order they are added, so the first filter added,
will

be the first to be executed, but it will also be the last executed for the
part

after _await next(context)_.

construct_call_stack

Construct the call stack for the given filter type.

filter

Decorator to add a filter to the Kernel.

remove_filter

Remove a filter from the Kernel.

**add_filter**

Add a filter to the Kernel.

Each filter is added to the beginning of the list of filters, this is because
the filters are

executed in the order they are added, so the first filter added, will be the
first to be

executed, but it will also be the last executed for the part after _await
next(context)_.

Python

**Parameters**

**Name**

**Description**

`**filter_type**`

Required*

str

The type of the filter to add (function_invocation, prompt_rendering)

`**filter**`

Required*

object

The filter to add

**Exceptions**

ﾉ

**Expand table**

`add_filter(filter_type: Literal[FilterTypes.AUTO_FUNCTION_INVOCATION, `

`FilterTypes.FUNCTION_INVOCATION, FilterTypes.PROMPT_RENDERING] | `

`FilterTypes, filter: Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]], Awaitable[``None``]])
-> `

`None`

ﾉ

**Expand table**



**Type**

**Description**

FilterDefinitionException

If an error occurs while adding the filter to the kernel

**construct_call_stack**

Construct the call stack for the given filter type.

Python

**Parameters**

**Name**

**Description**

`**filter_type**`

Required*

`**inner_function**`

Required*

**Exceptions**

**Type**

**Description**

FilterDefinitionException

If an error occurs while adding the filter to the kernel

**filter**

Decorator to add a filter to the Kernel.

Python

ﾉ

**Expand table**

`construct_call_stack(filter_type: FilterTypes, inner_function: `

`Callable[[FILTER_CONTEXT_TYPE], Coroutine[Any, Any, ``None``]]) -> `

`Callable[[FILTER_CONTEXT_TYPE], Coroutine[Any, Any, ``None``]]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**filter_type**`

Required*

**Exceptions**

**Type**

**Description**

FilterDefinitionException

If an error occurs while adding the filter to the kernel

**remove_filter**

Remove a filter from the Kernel.

Python

**Parameters**

`filter(filter_type: Literal[FilterTypes.AUTO_FUNCTION_INVOCATION, `

`FilterTypes.FUNCTION_INVOCATION, FilterTypes.PROMPT_RENDERING] | `

`FilterTypes) -> Callable[[Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]], Awaitable[``None``]]],
`

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`remove_filter(filter_type: Literal[FilterTypes.AUTO_FUNCTION_INVOCATION, `

`FilterTypes.FUNCTION_INVOCATION, FilterTypes.PROMPT_RENDERING] | `

`FilterTypes | ``None`` = ``None``, filter_id: int | ``None`` = ``None``, position: int | `

`None`` = ``None``) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**filter_type**`

<xref:<xref:semantic_kernel.filters.kernel_filters_extension.str | FilterTypes |

None>>

The type of the filter to remove.

Default value: None

`**filter_id**`

int

The id of the hook to remove

Default value: None

`**position**`

int

The position of the filter in the list

Default value: None

**Exceptions**

**Type**

**Description**

FilterDefinitionException

If an error occurs while adding the filter to the kernel

**auto_function_invocation_filters**

Python

**function_invocation_filters**

Python

**prompt_rendering_filters**

ﾉ

**Expand table**

**Attributes**

`auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`

`function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`



Python

`prompt_rendering_filters: list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]],
Awaitable[``None``]]]]`



**AutoFunctionInvocationContext Class**

Reference

The context for auto function invocation filtering.

This is the context supplied to the auto function invocation filters.

Common use case are to alter the function_result, for instance filling it with
a pre-

computed value, in order to skip a step, for instance when doing caching.

Another option is to terminate, this can be done by setting terminate to True.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**chat_history**`

The chat history or None.

`AutoFunctionInvocationContext()`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**function_result**`

Required*

The function result or None.

`**request_sequence_index**`

Required*

The request sequence index.

`**function_sequence_index**`

Required*

The function sequence index.

`**function_count**`

Required*

The function count.

`**terminate**`

Required*

The flag to terminate.

**chat_history**

Python

**execution_settings**

Python

**function_count**

Python

**function_result**

Python

**Attributes**

`chat_history: ChatHistory | ``None`

`execution_settings: PromptExecutionSettings | ``None`

`function_count: int`



**function_sequence_index**

Python

**request_sequence_index**

Python

**terminate**

Python

`function_result: FunctionResult | ``None`

`function_sequence_index: int`

`request_sequence_index: int`

`terminate: bool`



**FilterTypes Enum**

Reference

Enum for the filter types.

AUTO_FUNCTION_INVOCATION

FUNCTION_INVOCATION

PROMPT_RENDERING

**Fields**

ﾉ

**Expand table**



**FunctionInvocationContext Class**

Reference

The context for function invocation filtering.

This filter can be used to monitor which functions are called. To log what
function was

called with which parameters and what output. Finally it can be used for
caching by

setting the result value.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**result**`

Required*

The result of the function, or None.

`FunctionInvocationContext()`

ﾉ

**Expand table**



**result**

Python

**Attributes**

`result: FunctionResult | ``None`



**PromptRenderContext Class**

Reference

The context for prompt rendering filtering.

When prompt rendering is expensive (for instance when there are expensive
functions

being called.) This filter can be used to set the rendered_prompt or function
result

directly and returning.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The function invoked.

`**kernel**`

Required*

The kernel used.

`**arguments**`

Required*

The arguments used to call the function.

`**is_streaming**`

Required*

Whether the function is streaming.

`**rendered_prompt**`

Required*

The result of the prompt rendering.

`**function_result**`

The result of the function that used the prompt.

`PromptRenderContext()`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**function_result**

Python

**rendered_prompt**

Python

**Attributes**

`function_result: FunctionResult | ``None`

`rendered_prompt: str | ``None`



**functions Package**

Reference

**Modules**

kernel_function

function_result

kernel_arguments

kernel_function

kernel_function_decorator

kernel_function_extension

kernel_function_from_method

kernel_function_from_prompt

kernel_function_log_messages

kernel_function_metadata

kernel_parameter_metadata

kernel_plugin

prompt_rendering_result

types

**Classes**

FunctionResult

The result of a function.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

ﾉ

**Expand table**

ﾉ

**Expand table**



_self_ is explicitly positional-only to allow _self_ as a field name.

KernelArguments

The arguments sent to the KernelFunction.

Initializes a new instance of the KernelArguments class.

This is a dict-like class with the additional field for the

execution_settings.

This class is derived from a dict, hence behaves the same way, just

adds the execution_settings as a dict, with service_id and the

settings.

KernelFunction

Semantic Kernel function.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelFunctionFromMethod

Semantic Kernel Function from a method.

Initializes a new instance of the KernelFunctionFromMethod class.

KernelFunctionFromPrompt

Semantic Kernel Function from a prompt.

Initializes a new instance of the KernelFunctionFromPrompt class.

KernelFunctionMetadata

The kernel function metadata.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelParameterMetadata

The kernel parameter metadata.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelPlugin

Represents a Kernel Plugin with functions.



This class behaves mostly like a dictionary, with functions as values

and their names as keys. When you add a function, through _.set_ or

**_setitem_** , the function is copied, the metadata is deep-copied and

the name of the plugin is set in the metadata and added to the dict

of functions. This is done in the same way as a normal dict, so a

existing key will be overwritten.

Class methods: from_object(plugin_name: str, plugin_instance: Any |

dict[str, Any], description: str | None = None): Create a plugin from

a existing object, like a custom class with annotated functions.

from_directory(plugin_name: str, parent_directory: str, description:

str | None = None): Create a plugin from a directory, parsing: .py

files, .yaml files and directories with skprompt.txt and config.json

files.

from_openapi( plugin_name: str, openapi_document_path: str,

execution_settings: OpenAPIFunctionExecutionParameters | None =

None, description: str | None = None):

Create a KernelPlugin.

**kernel_function**

Decorator for kernel functions.

Can be used directly as @kernel_function or with parameters

@kernel_function(name='function', description='I am a function.').

This decorator is used to mark a function as a kernel function. It also
provides

metadata for the function. The name and description can be left empty, and
then the

function name and docstring will be used.

The parameters are parsed from the function signature, use typing.Annotated to

provide a description for the parameter.

To parse the type, first it checks if the parameter is annotated, and get's
the

description from there. After that it checks recursively until it reaches the
lowest

` Create a plugin from an OpenAPI document.`

**Functions**



level, and it combines the types into a single comma-separated string, a
forwardRef

is also supported. All of this is are stored in
**kernel_function_parameters**.

The return type and description are parsed from the function signature, and
that is

stored in **kernel_function_return_type** ,
**kernel_function_return_description** and

**kernel_function_return_required**.

It also checks if the function is a streaming type (generator or iterable,
async or not),

and that is stored as a bool in **kernel_function_streaming**.

Python

**Parameters**

**Name**

**Description**

`**func**`

<xref:Callable>[<xref:...>,object]<xref: | None>

The function to decorate, can be None (if used as @kernel_function

Default value: None

`**name**`

<xref:<xref:semantic_kernel.functions.str | None>>

The name of the function, if not supplied, the function name will be used.

Default value: None

`**description**`

<xref:<xref:semantic_kernel.functions.str | None>>

The description of the function, if not supplied, the function docstring will
be

used, can be None.

Default value: None

**kernel_function**

Decorator for kernel functions.

Can be used directly as @kernel_function or with parameters

@kernel_function(name='function', description='I am a function.').

This decorator is used to mark a function as a kernel function. It also
provides

metadata for the function. The name and description can be left empty, and
then the

function name and docstring will be used.

`kernel_function(func: Callable[[...], object] | ``None`` = ``None``, name: str | `

`None`` = ``None``, description: str | ``None`` = ``None``) -> Callable[[...], Any]`

ﾉ

**Expand table**



The parameters are parsed from the function signature, use typing.Annotated to

provide a description for the parameter.

To parse the type, first it checks if the parameter is annotated, and get's
the

description from there. After that it checks recursively until it reaches the
lowest

level, and it combines the types into a single comma-separated string, a
forwardRef

is also supported. All of this is are stored in
**kernel_function_parameters**.

The return type and description are parsed from the function signature, and
that is

stored in **kernel_function_return_type** ,
**kernel_function_return_description** and

**kernel_function_return_required**.

It also checks if the function is a streaming type (generator or iterable,
async or not),

and that is stored as a bool in **kernel_function_streaming**.

Python

**Parameters**

**Name**

**Description**

`**func**`

<xref:Callable>[<xref:...>,object]<xref: | None>

The function to decorate, can be None (if used as @kernel_function

Default value: None

`**name**`

<xref:<xref:semantic_kernel.functions.str | None>>

The name of the function, if not supplied, the function name will be used.

Default value: None

`**description**`

<xref:<xref:semantic_kernel.functions.str | None>>

The description of the function, if not supplied, the function docstring will
be

used, can be None.

Default value: None

`kernel_function(func: Callable[[...], object] | ``None`` = ``None``, name: str | `

`None`` = ``None``, description: str | ``None`` = ``None``) -> Callable[[...], Any]`

ﾉ

**Expand table**



**function_result Module**

Reference

**Classes**

FunctionResult

The result of a function.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FunctionResult Class**

Reference

The result of a function.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The metadata of the function that was invoked.

`**value**`

Required*

The value of the result.

`**rendered_prompt**`

Required*

The rendered prompt of the result.

`**metadata**`

Required*

The metadata of the result.

**Keyword-Only Parameters**

`FunctionResult(*, function: KernelFunctionMetadata, value: Any, `

`rendered_prompt: str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function**`

Required*

`**value**`

Required*

`**rendered_prompt**`

Required*

`**metadata**`

Required*

get_inner_content

Get the inner content of the function result when that is a KernelContent or

subclass of the first item of the value if it is a list.

**get_inner_content**

Get the inner content of the function result when that is a KernelContent or
subclass

of the first item of the value if it is a list.

Python

**Parameters**

**Name**

**Description**

`**index**`

Default value: 0

**function**

**Methods**

ﾉ

**Expand table**

`get_inner_content()`

ﾉ

**Expand table**

**Attributes**



Python

**metadata**

Python

**rendered_prompt**

Python

**value**

Python

`function: KernelFunctionMetadata`

`metadata: dict[str, Any]`

`rendered_prompt: str | ``None`

`value: Any`



**kernel_arguments Module**

Reference

**Classes**

KernelArguments

The arguments sent to the KernelFunction.

Initializes a new instance of the KernelArguments class.

This is a dict-like class with the additional field for the
execution_settings.

This class is derived from a dict, hence behaves the same way, just adds the

execution_settings as a dict, with service_id and the settings.

ﾉ

**Expand table**



**KernelArguments Class**

Reference

The arguments sent to the KernelFunction.

Initializes a new instance of the KernelArguments class.

This is a dict-like class with the additional field for the
execution_settings.

This class is derived from a dict, hence behaves the same way, just adds the

execution_settings as a dict, with service_id and the settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**settings**`

<xref:PromptExecutionSettings | List>[<xref:PromptExecutionSettings>]<xref: |

None>

The settings for the execution. If a list is given, make sure all items in the
list have a

unique service_id as that is used as the key for the dict.

Default value: None

`****kwargs**`

Required*

dict[str,<xref: Any>]

The arguments for the function invocation, works similar to a regular dict.

`KernelArguments(settings: PromptExecutionSettings | `

`list[PromptExecutionSettings] | dict[str, PromptExecutionSettings] | ``None`` = `

`None``, **kwargs: Any)`

ﾉ

**Expand table**



**kernel_function Module**

Reference

**Classes**

KernelFunction

Semantic Kernel function.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFunction Class**

Reference

Semantic Kernel function.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**metadata**`

Required*

`**invocation_duration_histogram**`

Required*

`**streaming_duration_histogram**`

Required*

from_method

Create a new instance of the KernelFunctionFromMethod class.

from_prompt

Create a new instance of the KernelFunctionFromPrompt class.

`KernelFunction(*, metadata: KernelFunctionMetadata, `

`invocation_duration_histogram: Histogram = ``None``, `

`streaming_duration_histogram: Histogram = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



function_copy

Copy the function, can also override the plugin_name.

invoke

Invoke the function with the given arguments.

invoke_stream

Invoke a stream async function with the given arguments.

**from_method**

Create a new instance of the KernelFunctionFromMethod class.

Python

**Parameters**

**Name**

**Description**

`**method**`

Required*

`**plugin_name**`

Required*

Default value: None

`**stream_method**`

Required*

Default value: None

**from_prompt**

Create a new instance of the KernelFunctionFromPrompt class.

Python

`from_method(method: Callable[[...], Any], plugin_name: str | ``None`` = ``None``, `

`stream_method: Callable[[...], Any] | ``None`` = ``None``) -> `

`KernelFunctionFromMethod`

ﾉ

**Expand table**

`from_prompt(function_name: str, plugin_name: str, description: str | ``None`` `

`= ``None``, prompt: str | ``None`` = ``None``, template_format: Literal[``'semantic-`

`kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-kernel'``,
prompt_template: `

`PromptTemplateBase | ``None`` = ``None``, prompt_template_config: `

`PromptTemplateConfig | ``None`` = ``None``, prompt_execution_settings: `

`PromptExecutionSettings | Sequence[PromptExecutionSettings] | `

`Mapping[str, PromptExecutionSettings] | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`



**Parameters**

**Name**

**Description**

`**function_name**`

Required*

`**plugin_name**`

Required*

`**description**`

Required*

Default value: None

`**prompt**`

Required*

Default value: None

`**template_format**`

Required*

Default value: semantic-kernel

`**prompt_template**`

Required*

Default value: None

`**prompt_template_config**`

Required*

Default value: None

`**prompt_execution_settings**`

Required*

Default value: None

**function_copy**

Copy the function, can also override the plugin_name.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

str

The new plugin name.

ﾉ

**Expand table**

`function_copy(plugin_name: str | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The copied function.

**invoke**

Invoke the function with the given arguments.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.functions.kernel_function.Kernel>

The kernel

`**arguments**`

Required*

<xref:semantic_kernel.functions.kernel_function.KernelArguments>

The Kernel arguments

Default value: None

`**metadata**`

Required*

<xref:Dict>[str,<xref: Any>]

Additional metadata.

Default value: {}

`**kwargs**`

Required*

Any

Additional keyword arguments that will be added to the KernelArguments.

**Returns**

ﾉ

**Expand table**

`async`` invoke(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``, `

`metadata: dict[str, Any] = {}, **kwargs: Any) -> FunctionResult | ``None`

ﾉ

**Expand table**



**Type**

**Description**

FunctionResult

The result of the function

**invoke_stream**

Invoke a stream async function with the given arguments.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.functions.kernel_function.Kernel>

The kernel

`**arguments**`

Required*

<xref:semantic_kernel.functions.kernel_function.KernelArguments>

The Kernel arguments

Default value: None

`**metadata**`

Required*

<xref:Dict>[str,<xref: Any>]

Additional metadata.

Default value: {}

`**kwargs**`

Required*

Any

Additional keyword arguments that will be added to the KernelArguments.

**description**

The description of the function.

**fully_qualified_name**

ﾉ

**Expand table**

`async`` invoke_stream(kernel: Kernel, arguments: KernelArguments | ``None`` = `

`None``, metadata: dict[str, Any] = {}, **kwargs: Any) -> `

`AsyncGenerator[FunctionResult | list[StreamingContentMixin | Any], Any]`

ﾉ

**Expand table**

**Attributes**



The fully qualified name of the function.

**is_prompt**

Whether the function is based on a prompt.

**name**

The name of the function.

**parameters**

The parameters for the function.

**plugin_name**

The name of the plugin that contains this function.

**return_parameter**

The return parameter for the function.

**stream_function**

The stream function for the function.

**function**

The function to call.

**prompt_execution_settings**

The AI prompt execution settings.

**prompt_template_config**

The prompt template configuration.

**metadata**



The metadata for the function.

Python

**invocation_duration_histogram**

Python

**streaming_duration_histogram**

Python

`metadata: KernelFunctionMetadata`

`invocation_duration_histogram: Histogram`

`streaming_duration_histogram: Histogram`



**kernel_function_decorator Module**

Reference

**kernel_function**

Decorator for kernel functions.

Can be used directly as @kernel_function or with parameters

@kernel_function(name='function', description='I am a function.').

This decorator is used to mark a function as a kernel function. It also
provides

metadata for the function. The name and description can be left empty, and
then the

function name and docstring will be used.

The parameters are parsed from the function signature, use typing.Annotated to

provide a description for the parameter.

To parse the type, first it checks if the parameter is annotated, and get's
the

description from there. After that it checks recursively until it reaches the
lowest

level, and it combines the types into a single comma-separated string, a
forwardRef

is also supported. All of this is are stored in
**kernel_function_parameters**.

The return type and description are parsed from the function signature, and
that is

stored in **kernel_function_return_type** ,
**kernel_function_return_description** and

**kernel_function_return_required**.

It also checks if the function is a streaming type (generator or iterable,
async or not),

and that is stored as a bool in **kernel_function_streaming**.

Python

**Parameters**

**Functions**

`kernel_function(func: Callable[[...], object] | ``None`` = ``None``, name: str | `

`None`` = ``None``, description: str | ``None`` = ``None``) -> Callable[[...], Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**func**`

<xref:Callable>[<xref:...>,object]<xref: | None>

The function to decorate, can be None (if used as @kernel_function

Default value: None

`**name**`

<xref:<xref:semantic_kernel.functions.kernel_function_decorator.str | None>>

The name of the function, if not supplied, the function name will be used.

Default value: None

`**description**`

<xref:<xref:semantic_kernel.functions.kernel_function_decorator.str | None>>

The description of the function, if not supplied, the function docstring will
be

used, can be None.

Default value: None



**kernel_function_extension Module**

Reference

**Classes**

KernelFunctionExtension

Kernel function extension.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFunctionExtension Class**

Reference

Kernel function extension.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to form

a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**plugins**`

Required*

add_function

Adds a function to the specified plugin.

add_functions

Adds a list of functions to the specified plugin.

add_plugin

Adds a plugin to the kernel's collection of plugins.

If a plugin is provided, it uses that instance instead of

creating a new KernelPlugin. See

KernelPlugin.from_directory for more details on how the

directory is parsed.

add_plugin_from_openapi

Add a plugin from the OpenAPI manifest.

add_plugins

Adds a list of plugins to the kernel's collection of plugins.

get_full_list_of_function_metadata

Get a list of all function metadata in the plugins.

`KernelFunctionExtension(*, plugins: dict[str, KernelPlugin] = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_function

Get a function by plugin_name and function_name.

get_function_from_fully_qualified_function_name

Get a function by its fully qualified name (<plugin_name>-

<function_name>).

get_list_of_function_metadata

Get a list of all function metadata in the plugin collection.

get_list_of_function_metadata_bool

Get a list of the function metadata in the plugin collection.

get_list_of_function_metadata_filters

Get a list of Kernel Function Metadata based on filters.

get_plugin

Get a plugin by name.

rewrite_plugins

Rewrite plugins to a dictionary.

**add_function**

Adds a function to the specified plugin.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin to add the function to

`**function**`

Required*

<xref:KernelFunction | Callable>[<xref:...>,<xref: Any>]

The function to add

Default value: None

`**function_name**`

Required*

str

The name of the function

Default value: None

`**plugin_name**`

Required*

The name of the plugin

`**description**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_function_extension.str | None>>

The description of the function

Default value: None

`add_function(plugin_name: str, function: KERNEL_FUNCTION_TYPE | ``None`` = ``None``, `

`function_name: str | ``None`` = ``None``, description: str | ``None`` = ``None``, prompt: str | ``None`` `

`= ``None``, prompt_template_config: PromptTemplateConfig | ``None`` = ``None``, `

`prompt_execution_settings: PromptExecutionSettings | `

`Sequence[PromptExecutionSettings] | Mapping[str, PromptExecutionSettings] | ``None`` = `

`None``, template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``] = `

`'semantic-kernel'``, prompt_template: PromptTemplateBase | ``None`` = ``None``, return_plugin: `

`bool = ``False``, **kwargs: Any) -> KernelFunction | KernelPlugin`

ﾉ

**Expand table**



**Name**

**Description**

`**prompt**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_function_extension.str | None>>

The prompt template.

Default value: None

`**prompt_template_config**`

Required*

<xref:

<xref:semantic_kernel.functions.kernel_function_extension.PromptTemplateConfig

| None>>

The prompt template configuration

Default value: None

`**prompt_execution_settings**`

Required*

The execution settings, will be parsed into a dict.

Default value: None

`**template_format**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_function_extension.str | None>>

The format of the prompt template

Default value: semantic-kernel

`**prompt_template**`

Required*

<xref:

<xref:semantic_kernel.functions.kernel_function_extension.PromptTemplateBase |

None>>

The prompt template

Default value: None

`**return_plugin**`

Required*

bool

If True, the plugin is returned instead of the function

Default value: False

`**kwargs**`

Required*

Any

Additional arguments

**Returns**

**Type**

**Description**

KernelFunction | KernelPlugin

The function that was added, or the plugin if return_plugin is True

**add_functions**

Adds a list of functions to the specified plugin.

Python

**Parameters**

ﾉ

**Expand table**

`add_functions(plugin_name: str, functions: list[KERNEL_FUNCTION_TYPE] | dict[str, `

`KERNEL_FUNCTION_TYPE]) -> KernelPlugin`



**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin to add the functions to

`**functions**`

Required*

list[<xref:KernelFunction>]<xref: | dict>[str,<xref: KernelFunction>]

The functions to add

**Returns**

**Type**

**Description**

KernelPlugin

The plugin that the functions were added to.

**add_plugin**

Adds a plugin to the kernel's collection of plugins.

If a plugin is provided, it uses that instance instead of creating a new
KernelPlugin. See

KernelPlugin.from_directory for more details on how the directory is parsed.

Python

**Parameters**

**Name**

**Description**

`**plugin**`

The plugin to add. This can be a KernelPlugin, in which case it is added

straightaway and other parameters are ignored, a custom class that contains

methods with the kernel_function decorator or a dictionary of functions with

the kernel_function decorator for one or several methods.

Default value: None

`**plugin_name**`

The name of the plugin, used if the plugin is not a KernelPlugin, if the
plugin is

None and the parent_directory is set, KernelPlugin.from_directory is called
with

those parameters, see _KernelPlugin.from_directory_ for details.

Default value: None

`**parent_directory**`

The parent directory path where the plugin directory resides

ﾉ

**Expand table**

ﾉ

**Expand table**

`add_plugin(plugin: KernelPlugin | object | dict[str, Any] | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, parent_directory: str | ``None`` = ``None``, description: `

`str | ``None`` = ``None``, class_init_arguments: dict[str, dict[str, Any]] | ``None`` = ``None``) -> `

`KernelPlugin`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

`**description**`

The description of the plugin, used if the plugin is not a KernelPlugin.

Default value: None

`**class_init_arguments**`

The class initialization arguments

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The plugin that was added.

**Exceptions**

**Type**

**Description**

ValidationError

If a KernelPlugin needs to be created, but it is not valid.

**add_plugin_from_openapi**

Add a plugin from the OpenAPI manifest.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

The name of the plugin

`**openapi_document_path**`

Required*

The path to the OpenAPI document

Default value: None

ﾉ

**Expand table**

ﾉ

**Expand table**

`add_plugin_from_openapi(plugin_name: str, openapi_document_path: str | ``None`` = ``None``, `

`openapi_parsed_spec: dict[str, Any] | ``None`` = ``None``, execution_settings: `

`OpenAPIFunctionExecutionParameters | ``None`` = ``None``, description: str | ``None`` = ``None``) -> `

`KernelPlugin`

ﾉ

**Expand table**



**Name**

**Description**

`**openapi_parsed_spec**`

Required*

The parsed OpenAPI spec

Default value: None

`**execution_settings**`

Required*

The execution parameters

Default value: None

`**description**`

Required*

The description of the plugin

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The imported plugin

**Exceptions**

**Type**

**Description**

PluginInitializationError

if the plugin URL or plugin JSON/YAML is not provided

**add_plugins**

Adds a list of plugins to the kernel's collection of plugins.

Python

**Parameters**

**Name**

**Description**

`**plugins**`

Required*

list[<xref:KernelPlugin>]<xref: | dict>[str,<xref: KernelPlugin>]

The plugins to add to the kernel

**get_full_list_of_function_metadata**

Get a list of all function metadata in the plugins.

ﾉ

**Expand table**

ﾉ

**Expand table**

`add_plugins(plugins: list[KernelPlugin] | dict[str, KernelPlugin | object]) -> ``None`

ﾉ

**Expand table**



Python

**get_function**

Get a function by plugin_name and function_name.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_function_extension.str | None>>

The name of the plugin

`**function_name**`

Required*

str

The name of the function

**Returns**

**Type**

**Description**

KernelFunction

The function

**Exceptions**

**Type**

**Description**

KernelPluginNotFoundError

If the plugin is not found

KernelFunctionNotFoundError

If the function is not found

**get_function_from_fully_qualified_function_name**

Get a function by its fully qualified name (<plugin_name>-<function_name>).

`get_full_list_of_function_metadata() -> list[KernelFunctionMetadata]`

`get_function(plugin_name: str | ``None``, function_name: str) -> KernelFunction`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**fully_qualified_function_name**`

Required*

str

The fully qualified name of the function, if there is no '-' in the name,

it is assumed that it is only a function_name.

**Returns**

**Type**

**Description**

KernelFunction

The function

**Exceptions**

**Type**

**Description**

KernelPluginNotFoundError

If the plugin is not found

KernelFunctionNotFoundError

If the function is not found

**get_list_of_function_metadata**

Get a list of all function metadata in the plugin collection.

Python

**get_list_of_function_metadata_bool**

Get a list of the function metadata in the plugin collection.

`get_function_from_fully_qualified_function_name(fully_qualified_function_name:
str) `

`-> KernelFunction`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

`get_list_of_function_metadata(*args: Any, **kwargs: Any) -> `

`list[KernelFunctionMetadata]`



Python

**Parameters**

**Name**

**Description**

`**include_prompt**`

bool

Whether to include semantic functions in the list.

Default value: True

`**include_native**`

bool

Whether to include native functions in the list.

Default value: True

**Returns**

**Type**

**Description**

A list of KernelFunctionMetadata objects in the collection.

**get_list_of_function_metadata_filters**

Get a list of Kernel Function Metadata based on filters.

Python

**Parameters**

**Name**

**Description**

`**filters**`

Required*

dict[str,list[str]]

The filters to apply to the function list. The keys are:

included_plugins: A list of plugin names to include.

excluded_plugins: A list of plugin names to exclude.

`get_list_of_function_metadata_bool(include_prompt: bool = ``True``,
include_native: bool `

`= ``True``) -> list[KernelFunctionMetadata]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`get_list_of_function_metadata_filters(filters:
dict[Literal[``'excluded_plugins'``, `

`'included_plugins'``, ``'excluded_functions'``, ``'included_functions'``],
list[str]]) -> `

`list[KernelFunctionMetadata]`

ﾉ

**Expand table**



**Name**

**Description**

included_functions: A list of function names to include.

excluded_functions: A list of function names to exclude.

The included and excluded parameters are mutually exclusive. The function
names are checked

against the fully qualified name of a function.

**Returns**

**Type**

**Description**

list[KernelFunctionMetadata]

The list of Kernel Function Metadata that match the filters.

**get_plugin**

Get a plugin by name.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin

**Returns**

**Type**

**Description**

KernelPlugin

The plugin

**Exceptions**

ﾉ

**Expand table**

`get_plugin(plugin_name: str) -> KernelPlugin`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

KernelPluginNotFoundError

If the plugin is not found

**rewrite_plugins**

Rewrite plugins to a dictionary.

Python

**Parameters**

**Name**

**Description**

`**plugins**`

Default value: None

**plugins**

Python

`rewrite_plugins(plugins: KernelPlugin | list[KernelPlugin] | dict[str, KernelPlugin] `

`| ``None`` = ``None``) -> dict[str, KernelPlugin]`

ﾉ

**Expand table**

**Attributes**

`plugins: dict[str, KernelPlugin]`



**kernel_function_from_method Module**

Reference

**Classes**

KernelFunctionFromMethod

Semantic Kernel Function from a method.

Initializes a new instance of the KernelFunctionFromMethod class.

ﾉ

**Expand table**



**KernelFunctionFromMethod Class**

Reference

Semantic Kernel Function from a method.

Initializes a new instance of the KernelFunctionFromMethod class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**method**`

Required*

<xref:Callable>[<xref:...>,<xref: Any>]

The method to be called

`**plugin_name**`

<xref:<xref:semantic_kernel.functions.kernel_function_from_method.str | None>>

The name of the plugin

Default value: None

`**stream_method**`

<xref:Callable>[<xref:...>,<xref: Any>]<xref: | None>

The stream method for the function

Default value: None

`**parameters**`

list[<xref:KernelParameterMetadata>]<xref: | None>

The parameters of the function

Default value: None

`**return_parameter**`

<xref:

<xref:semantic_kernel.functions.kernel_function_from_method.KernelParameterMetadata

| None>>

The return parameter of the function

Default value: None

`**additional_metadata**`

dict[str,<xref: Any>]<xref: | None>

Additional metadata for the function

Default value: None

`KernelFunctionFromMethod(method: Callable[[...], Any], plugin_name: str | ``None`` = `

`None``, stream_method: Callable[[...], Any] | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, return_parameter: `

`KernelParameterMetadata | ``None`` = ``None``, additional_metadata: dict[str, Any] | ``None`` = `

`None``)`

ﾉ

**Expand table**



gather_function_parameters

Gathers the function parameters from the arguments.

**gather_function_parameters**

Gathers the function parameters from the arguments.

Python

**Parameters**

**Name**

**Description**

`**context**`

Required*

**invocation_duration_histogram**

Python

**metadata**

Python

**method**

Python

**Methods**

ﾉ

**Expand table**

`gather_function_parameters(context: FunctionInvocationContext) -> dict[str,
Any]`

ﾉ

**Expand table**

**Attributes**

`invocation_duration_histogram: metrics.Histogram`

`metadata: KernelFunctionMetadata`



**stream_method**

Python

**streaming_duration_histogram**

Python

`method: Callable[[...], Any]`

`stream_method: Callable[[...], Any] | ``None`

`streaming_duration_histogram: metrics.Histogram`



**kernel_function_from_prompt Module**

Reference

**Classes**

KernelFunctionFromPrompt

Semantic Kernel Function from a prompt.

Initializes a new instance of the KernelFunctionFromPrompt class.

ﾉ

**Expand table**



**KernelFunctionFromPrompt Class**

Reference

Semantic Kernel Function from a prompt.

Initializes a new instance of the KernelFunctionFromPrompt class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

str

The name of the function

`**plugin_name**`

str

The name of the plugin

Default value: None

`**description**`

str

The description for the function

Default value: None

`**prompt**`

<xref:Optional>[str]

The prompt

Default value: None

`**template_format**`

<xref:Optional>[str]

The template format, default is "semantic-kernel"

Default value: semantic-kernel

`**prompt_template**`

<xref:Optional>[<xref:KernelPromptTemplate>]

`KernelFunctionFromPrompt(function_name: str, plugin_name: str | ``None`` = ``None``, `

`description: str | ``None`` = ``None``, prompt: str | ``None`` = ``None``, template_format: `

`Literal[``'semantic-kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-
kernel'``, `

`prompt_template: PromptTemplateBase | ``None`` = ``None``, prompt_template_config: `

`PromptTemplateConfig | ``None`` = ``None``, prompt_execution_settings: `

`PromptExecutionSettings | Sequence[PromptExecutionSettings] | Mapping[str, `

`PromptExecutionSettings] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The prompt template

Default value: None

`**prompt_template_config**`

<xref:Optional>[<xref:PromptTemplateConfig>]

The prompt template configuration

Default value: None

`**prompt_execution_settings**`

Optional

instance, list or dict of PromptExecutionSettings to be used by

the function, can also be supplied through

prompt_template_config, but the supplied one is used if both

are present. prompt_template_config

(Optional[PromptTemplateConfig]): the prompt template config.

Default value: None

from_directory

Creates a new instance of the KernelFunctionFromPrompt

class from a directory.

The directory needs to contain:

A prompt file named _skprompt.txt_

A config file named _config.json_

from_yaml

Creates a new instance of the KernelFunctionFromPrompt

class from a YAML string.

rewrite_execution_settings

Rewrite execution settings to a dictionary.

If the prompt_execution_settings is not a dictionary, it is

converted to a dictionary. If it is not supplied, but

prompt_template is, the prompt_template's execution settings

are used.

update_arguments_with_defaults

Update any missing values with their defaults.

**from_directory**

Creates a new instance of the KernelFunctionFromPrompt class from a directory.

The directory needs to contain:

**Methods**

ﾉ

**Expand table**



A prompt file named _skprompt.txt_

A config file named _config.json_

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

`**plugin_name**`

Required*

Default value: None

**Returns**

**Type**

**Description**

KernelFunctionFromPrompt

The kernel function from prompt

**from_yaml**

Creates a new instance of the KernelFunctionFromPrompt class from a YAML
string.

Python

**Parameters**

`from_directory(path: str, plugin_name: str | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_yaml(yaml_str: str, plugin_name: str | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`

ﾉ

**Expand table**



**Name**

**Description**

`**yaml_str**`

Required*

`**plugin_name**`

Required*

Default value: None

**rewrite_execution_settings**

Rewrite execution settings to a dictionary.

If the prompt_execution_settings is not a dictionary, it is converted to a
dictionary. If

it is not supplied, but prompt_template is, the prompt_template's execution
settings

are used.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**update_arguments_with_defaults**

Update any missing values with their defaults.

Python

**Parameters**

`rewrite_execution_settings(data: Any) -> dict[str, `

`PromptExecutionSettings]`

ﾉ

**Expand table**

`update_arguments_with_defaults(arguments: KernelArguments) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

**invocation_duration_histogram**

Python

**metadata**

Python

**prompt_execution_settings**

Python

**prompt_template**

Python

**streaming_duration_histogram**

Python

**Attributes**

`invocation_duration_histogram: metrics.Histogram`

`metadata: KernelFunctionMetadata`

`prompt_execution_settings: dict[str, PromptExecutionSettings]`

`prompt_template: PromptTemplateBase`

`streaming_duration_histogram: metrics.Histogram`



**kernel_function_log_messages Module**

Reference

**Classes**

KernelFunctionLogMessages

Kernel function log messages.

This class contains static methods to log messages related to

kernel functions.

ﾉ

**Expand table**



**KernelFunctionLogMessages Class**

Reference

Kernel function log messages.

This class contains static methods to log messages related to kernel
functions.

**Constructor**

Python

log_function_arguments

Log message when a kernel function is invoked.

log_function_completed

Log message when a kernel function is completed.

log_function_error

Log message when a kernel function fails.

log_function_invoked_success

Log message when a kernel function is invoked successfully.

log_function_invoking

Log message when a kernel function is invoked.

log_function_result_value

Log message when a kernel function result is returned.

log_function_streaming_completed

Log message when a kernel function is completed via

streaming.

log_function_streaming_invoking

Log message when a kernel function is invoked via

streaming.

**log_function_arguments**

Log message when a kernel function is invoked.

Python

`KernelFunctionLogMessages()`

**Methods**

ﾉ

**Expand table**

`static log_function_arguments(logger: Logger, arguments: KernelArguments)`



**Parameters**

**Name**

**Description**

`**logger**`

Required*

`**arguments**`

Required*

**log_function_completed**

Log message when a kernel function is completed.

Python

**Parameters**

**Name**

**Description**

`**logger**`

Required*

`**duration**`

Required*

**log_function_error**

Log message when a kernel function fails.

Python

**Parameters**

ﾉ

**Expand table**

`static log_function_completed(logger: Logger, duration: float)`

ﾉ

**Expand table**

`static log_function_error(logger: Logger, error: Exception)`



**Name**

**Description**

`**logger**`

Required*

`**error**`

Required*

**log_function_invoked_success**

Log message when a kernel function is invoked successfully.

Python

**Parameters**

**Name**

**Description**

`**logger**`

Required*

`**kernel_function_name**`

Required*

**log_function_invoking**

Log message when a kernel function is invoked.

Python

**Parameters**

ﾉ

**Expand table**

`static log_function_invoked_success(logger: Logger, kernel_function_name: `

`str)`

ﾉ

**Expand table**

`static log_function_invoking(logger: Logger, kernel_function_name: str)`

ﾉ

**Expand table**



**Name**

**Description**

`**logger**`

Required*

`**kernel_function_name**`

Required*

**log_function_result_value**

Log message when a kernel function result is returned.

Python

**Parameters**

**Name**

**Description**

`**logger**`

Required*

`**function_result**`

Required*

**log_function_streaming_completed**

Log message when a kernel function is completed via streaming.

Python

**Parameters**

`static log_function_result_value(logger: Logger, function_result: `

`FunctionResult | ``None``)`

ﾉ

**Expand table**

`static log_function_streaming_completed(logger: Logger, duration: float)`

ﾉ

**Expand table**



**Name**

**Description**

`**logger**`

Required*

`**duration**`

Required*

**log_function_streaming_invoking**

Log message when a kernel function is invoked via streaming.

Python

**Parameters**

**Name**

**Description**

`**logger**`

Required*

`**kernel_function_name**`

Required*

`static log_function_streaming_invoking(logger: Logger, `

`kernel_function_name: str)`

ﾉ

**Expand table**



**kernel_function_metadata Module**

Reference

**Classes**

KernelFunctionMetadata

The kernel function metadata.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelFunctionMetadata Class**

Reference

The kernel function metadata.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**plugin_name**`

Required*

`**description**`

Required*

`**parameters**`

Required*

`**is_prompt**`

Required*

`KernelFunctionMetadata(*, name: Annotated[str, `

`_PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)], plugin_name: `

`Annotated[str | ``None``, _PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)] = `

`None``, description: str | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] = ``None``, is_prompt: bool, is_asynchronous:
bool `

`| ``None`` = ``True``, return_parameter: KernelParameterMetadata | ``None`` = ``None``, `

`additional_properties: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**is_asynchronous**`

Default value: True

`**return_parameter**`

Required*

`**additional_properties**`

Required*

custom_fully_qualified_name

Get the fully qualified name of the function with a custom

separator.

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**Methods**

ﾉ

**Expand table**

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**fully_qualified_name**

Get the fully qualified name of the function.

A fully qualified name is the name of the combination of the plugin name and
the

function name, separated by a hyphen, if the plugin name is present.
Otherwise, it is

just the function name.

**Returns**

**Type**

**Description**

The fully qualified name of the function.

**additional_properties**

Python

**description**

Python

**is_asynchronous**

Python

**is_prompt**

Python

**Attributes**

ﾉ

**Expand table**

`additional_properties: dict[str, Any] | ``None`

`description: str | ``None`

`is_asynchronous: bool | ``None`



**name**

Python

**parameters**

Python

**plugin_name**

Python

**return_parameter**

Python

`is_prompt: bool`

`name: str`

`parameters: list[KernelParameterMetadata]`

`plugin_name: str | ``None`

`return_parameter: KernelParameterMetadata | ``None`



**kernel_parameter_metadata Module**

Reference

**Classes**

KernelParameterMetadata

The kernel parameter metadata.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelParameterMetadata Class**

Reference

The kernel parameter metadata.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**default_value**`

Required*

`**type**`

Default value: str

`**is_required**`

Required*

`**type_object**`

Required*

`KernelParameterMetadata(*, name: Annotated[str | ``None``, `

`_PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)], description: str | `

`None`` = ``None``, default_value: Any | ``None`` = ``None``, type: str | ``None`` = ``'str'``, `

`is_required: bool | ``None`` = ``False``, type_object: Any | ``None`` = ``None``, `

`schema_data: dict[str, Any] | ``None`` = ``None``, include_in_function_choices: bool `

`= ``True``)`

ﾉ

**Expand table**



**Name**

**Description**

`**schema_data**`

Required*

`**include_in_function_choices**`

Default value: True

form_schema

Create a schema for the parameter metadata.

infer_schema

Infer the schema for the parameter metadata.

**form_schema**

Create a schema for the parameter metadata.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**infer_schema**

Infer the schema for the parameter metadata.

Python

**Methods**

ﾉ

**Expand table**

`form_schema(data: Any) -> Any`

ﾉ

**Expand table**

`infer_schema(type_object: type | ``None`` = ``None``, parameter_type: str | ``None`` `

`= ``None``, default_value: Any | ``None`` = ``None``, description: str | ``None`` = ``None``, `

`structured_output: bool = ``False``) -> dict[str, Any] | ``None`



**Parameters**

**Name**

**Description**

`**type_object**`

Default value: None

`**parameter_type**`

Default value: None

`**default_value**`

Default value: None

`**description**`

Default value: None

`**structured_output**`

Default value: False

**default_value**

Python

**description**

Python

**include_in_function_choices**

Python

**is_required**

Python

ﾉ

**Expand table**

**Attributes**

`default_value: Any | ``None`

`description: str | ``None`

`include_in_function_choices: bool`

`is_required: bool | ``None`



**name**

Python

**schema_data**

Python

**type_**

Python

**type_object**

Python

`name: str | ``None`

`schema_data: dict[str, Any] | ``None`

`type_: str | ``None`

`type_object: Any | ``None`



**kernel_plugin Module**

Reference

**Classes**

KernelPlugin

Represents a Kernel Plugin with functions.

This class behaves mostly like a dictionary, with functions as values and
their

names as keys. When you add a function, through _.set_ or **_setitem_** , the
function is

copied, the metadata is deep-copied and the name of the plugin is set in the

metadata and added to the dict of functions. This is done in the same way as a

normal dict, so a existing key will be overwritten.

Class methods: from_object(plugin_name: str, plugin_instance: Any | dict[str, Any],

description: str | None = None): Create a plugin from a existing object, like a

custom class with annotated functions.

from_directory(plugin_name: str, parent_directory: str, description: str | None =

None): Create a plugin from a directory, parsing: .py files, .yaml files and
directories

with skprompt.txt and config.json files.

from_openapi( plugin_name: str, openapi_document_path: str,
execution_settings:

OpenAPIFunctionExecutionParameters | None = None, description: str | None =

None):

Create a KernelPlugin.

ﾉ

**Expand table**

` Create a plugin from an OpenAPI document.`



**KernelPlugin Class**

Reference

Represents a Kernel Plugin with functions.

This class behaves mostly like a dictionary, with functions as values and
their names as

keys. When you add a function, through _.set_ or **_setitem_** , the function
is copied, the

metadata is deep-copied and the name of the plugin is set in the metadata and
added

to the dict of functions. This is done in the same way as a normal dict, so a
existing key

will be overwritten.

Class methods: from_object(plugin_name: str, plugin_instance: Any | dict[str, Any],

description: str | None = None): Create a plugin from a existing object, like a custom

class with annotated functions.

from_directory(plugin_name: str, parent_directory: str, description: str | None = None):

Create a plugin from a directory, parsing: .py files, .yaml files and
directories with

skprompt.txt and config.json files.

from_openapi( plugin_name: str, openapi_document_path: str,
execution_settings:

OpenAPIFunctionExecutionParameters | None = None, description: str | None = None):

Create a KernelPlugin.

**Constructor**

Python

**Parameters**

` Create a plugin from an OpenAPI document.`

`KernelPlugin(name: str, description: str | ``None`` = ``None``, functions: `

`KernelFunction | Callable[[...], Any] | KernelPlugin | `

`Sequence[KernelFunction | Callable[[...], Any] | KernelPlugin] | ``None`` | `

`dict[str, KernelFunction | Callable[[...], Any]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**name**`

Required*

The name of the plugin. The name can be upper/lower case letters and

underscores.

`**description**`

The description of the plugin.

Default value: None

`**functions**`

The functions in the plugin, will be rewritten to a dictionary of functions.

Default value: None

add

Add functions to the plugin.

add_dict

Add a dictionary of functions to the plugin.

add_list

Add a list of functions to the plugin.

from_directory

Create a plugin from a specified directory.

This method does not recurse into subdirectories

beyond one level deep from the specified plugin

directory. For YAML files, function names are

extracted from the content of the YAML files

themselves (the name property). For directories,

the function name is assumed to be the name of

the directory. Each KernelFunction object is

initialized with data parsed from the associated

files and added to a list of functions that are then

assigned to the created KernelPlugin object. A .py

file is parsed and a plugin created, the functions

within as then combined with any other functions

found. The python file needs to contain a class

with one or more kernel_function decorated

methods. If this class has a **_init_** method, it will be

called with the arguments provided in the

_class_init_arguments_ dictionary, the key needs to

be the same as the name of the class, with the

value being a dictionary of arguments to pass to

the class (using kwargs).

MyPlugins/

|<<-- pluginA.yaml |<<-- pluginB.yaml |

<<-- native_function.py |<<--

Directory1/

**Methods**

ﾉ

**Expand table**



|<<-- Directory2/ >>|<<-- skprompt.txt

>>|<<-- config.json

Calling _KernelPlugin.from_directory( "MyPlugins",_

_" /path/to")_ will create a KernelPlugin object

named "MyPlugins", containing KernelFunction

objects for _pluginA.yaml_ , _pluginB.yaml_ ,

_Directory1_ , and _Directory2_ , each initialized with

their respective configurations. And functions for

anything within native_function.py.

from_object

Creates a plugin that wraps the specified target

object and imports it into the kernel's plugin

collection.

from_openapi

Create a plugin from an OpenAPI document.

from_python_file

Create a plugin from a Python file.

from_text_search_with_get_search_results

Creates a plugin that wraps the text search

"get_search_results" function.

from_text_search_with_get_text_search_results

Creates a plugin that wraps the text search

"get_text_search_results" function.

from_text_search_with_search

Creates a plugin that wraps the text search

"search" function.

get

Get a function from the plugin.

get_functions_metadata

Get the metadata for the functions in the plugin.

set

Set a function in the plugin.

setdefault

Set a default value for a key.

update

Update the plugin with the functions from

another.

**add**

Add functions to the plugin.

` >>|<<-- skprompt.txt`

` >>|<<-- config.json`



Python

**add_dict**

Add a dictionary of functions to the plugin.

Python

**Parameters**

**Name**

**Description**

`**functions**`

Required*

**add_list**

Add a list of functions to the plugin.

Python

**Parameters**

**Name**

**Description**

`**functions**`

Required*

**from_directory**

`add(functions: Any) -> ``None`

`add_dict(functions: dict[str, KernelFunction | Callable[[...], Any]]) -> `

`None`

ﾉ

**Expand table**

`add_list(functions: list[KernelFunction | Callable[[...], Any] | `

`KernelPlugin]) -> ``None`

ﾉ

**Expand table**



Create a plugin from a specified directory.

This method does not recurse into subdirectories beyond one level deep from
the

specified plugin directory. For YAML files, function names are extracted from
the

content of the YAML files themselves (the name property). For directories, the

function name is assumed to be the name of the directory. Each KernelFunction

object is initialized with data parsed from the associated files and added to
a list of

functions that are then assigned to the created KernelPlugin object. A .py
file is

parsed and a plugin created, the functions within as then combined with any
other

functions found. The python file needs to contain a class with one or more

kernel_function decorated methods. If this class has a **_init_** method, it
will be called

with the arguments provided in the _class_init_arguments_ dictionary, the key
needs to

be the same as the name of the class, with the value being a dictionary of
arguments

to pass to the class (using kwargs).

MyPlugins/

|<<-- pluginA.yaml |<<-- pluginB.yaml |<<-- native_function.py |<<--

Directory1/

|<<-- Directory2/ >>|<<-- skprompt.txt >>|<<-- config.json

Calling _KernelPlugin.from_directory( "MyPlugins", "/path/to")_ will create a
KernelPlugin

object named "MyPlugins", containing KernelFunction objects for _pluginA.yaml_
,

_pluginB.yaml_ , _Directory1_ , and _Directory2_ , each initialized with their
respective

configurations. And functions for anything within native_function.py.

Python

**Parameters**

` >>|<<-- skprompt.txt`

` >>|<<-- config.json`

`from_directory(plugin_name: str, parent_directory: str, description: str `

`| ``None`` = ``None``, class_init_arguments: dict[str, dict[str, Any]] | ``None`` = `

`None``) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin, this is the name of the directory within

the parent directory

`**parent_directory**`

Required*

str

The parent directory path where the plugin directory resides

`**description**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_plugin.str | None>>

The description of the plugin

Default value: None

`**class_init_arguments**`

Required*

dict[str,dict[str,<xref: Any>]]<xref: | None>

The class initialization arguments

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The created plugin of type KernelPlugin.

**Exceptions**

**Type**

**Description**

PluginInitializationError

If the plugin directory does not exist.

PluginInvalidNameError

If the plugin name is invalid.

**Examples**

Assuming a plugin directory structure as follows:

**from_object**

Creates a plugin that wraps the specified target object and imports it into
the

kernel's plugin collection.

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin. Allows chars: upper, lower ASCII and

underscores.

`**plugin_instance**`

Required*

<xref:Any | dict>[str,<xref: Any>]

The plugin instance. This can be a custom class or a dictionary of classes

that contains methods with the kernel_function decorator for one or

several methods. See _TextMemoryPlugin_ as an example.

`**description**`

Required*

<xref:<xref:semantic_kernel.functions.kernel_plugin.str | None>>

The description of the plugin.

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The imported plugin of type KernelPlugin.

**from_openapi**

Create a plugin from an OpenAPI document.

Python

`from_object(plugin_name: str, plugin_instance: Any | dict[str, Any], `

`description: str | ``None`` = ``None``) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_openapi(plugin_name: str, openapi_document_path: str | ``None`` = ``None``, `

`openapi_parsed_spec: dict[str, Any] | ``None`` = ``None``, execution_settings: `

`OpenAPIFunctionExecutionParameters | ``None`` = ``None``, description: str | ``None`` `

`= ``None``) -> _T`



**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

The name of the plugin

`**openapi_document_path**`

Required*

The path to the OpenAPI document (optional)

Default value: None

`**openapi_parsed_spec**`

Required*

The parsed OpenAPI spec (optional)

Default value: None

`**execution_settings**`

Required*

The execution parameters

Default value: None

`**description**`

Required*

The description of the plugin

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The created plugin

**Exceptions**

**Type**

**Description**

PluginInitializationError

if the plugin URL or plugin JSON/YAML is not provided

**from_python_file**

Create a plugin from a Python file.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_python_file(plugin_name: str, py_file: str, description: str | ``None`` `

`= ``None``, class_init_arguments: dict[str, dict[str, Any]] | ``None`` = ``None``) -> `



**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

`**py_file**`

Required*

`**description**`

Required*

Default value: None

`**class_init_arguments**`

Required*

Default value: None

**from_text_search_with_get_search_results**

Creates a plugin that wraps the text search "get_search_results" function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`_T`

ﾉ

**Expand table**

`from_text_search_with_get_search_results(text_search: TextSearch, `

`plugin_name: str, plugin_description: str | ``None`` = ``None``, **kwargs: Any) -`

`> _T`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

**Type**

**Description**

a KernelPlugin.

**from_text_search_with_get_text_search_results**

Creates a plugin that wraps the text search "get_text_search_results"
function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

ﾉ

**Expand table**

`from_text_search_with_get_text_search_results(text_search: TextSearch, `

`plugin_name: str, plugin_description: str | ``None`` = ``None``, **kwargs: Any) -`

`> _T`

ﾉ

**Expand table**



**Type**

**Description**

a KernelPlugin.

**from_text_search_with_search**

Creates a plugin that wraps the text search "search" function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

**Type**

**Description**

a KernelPlugin.

**get**

ﾉ

**Expand table**

`from_text_search_with_search(text_search: TextSearch, plugin_name: str, `

`plugin_description: str | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**



Get a function from the plugin.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`**default**`

Required*

Default value: None

**get_functions_metadata**

Get the metadata for the functions in the plugin.

Python

**set**

Set a function in the plugin.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`get()`

ﾉ

**Expand table**

`get_functions_metadata()`

`set()`

ﾉ

**Expand table**



**Name**

**Description**

`**value**`

Required*

**setdefault**

Set a default value for a key.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`**value**`

Required*

Default value: None

**update**

Update the plugin with the functions from another.

Python

**name**

The name of the plugin. The name can be upper/lower case letters and
underscores.

Python

`setdefault()`

ﾉ

**Expand table**

`update()`

**Attributes**



**description**

The description of the plugin.

Python

**functions**

The functions in the plugin, indexed by their name.

Python

`name: StringConstraints(strip_whitespace=``None``, to_upper=``None``, `

`to_lower=``None``, strict=``None``, min_length=1, max_length=``None``,
pattern=^[0-`

`9A-Za-z_]+$)]`

`description: str | ``None`

`functions: dict[str, KernelFunction]`



**prompt_rendering_result Module**

Reference

**Classes**

PromptRenderingResult

Represents the result of rendering a prompt template.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PromptRenderingResult Class**

Reference

Represents the result of rendering a prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**rendered_prompt**`

Required*

`**ai_service**`

Required*

`**execution_settings**`

Required*

`**function_result**`

Required*

**rendered_prompt**

`PromptRenderingResult(*, rendered_prompt: str, ai_service: `

`AIServiceClientBase, execution_settings: PromptExecutionSettings, `

`function_result: FunctionResult | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



The rendered prompt.

Python

**ai_service**

The AI service that rendered the prompt.

Python

**execution_settings**

The execution settings for the prompt.

Python

**function_result**

The result of executing the prompt.

Python

`rendered_prompt: str`

`ai_service: AIServiceClientBase`

`execution_settings: PromptExecutionSettings`

`function_result: FunctionResult | ``None`



**types Module**

Reference



**FunctionResult Class**

Reference

The result of a function.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

The metadata of the function that was invoked.

`**value**`

Required*

The value of the result.

`**rendered_prompt**`

Required*

The rendered prompt of the result.

`**metadata**`

Required*

The metadata of the result.

**Keyword-Only Parameters**

`FunctionResult(*, function: KernelFunctionMetadata, value: Any, `

`rendered_prompt: str | ``None`` = ``None``, metadata: dict[str, Any] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**function**`

Required*

`**value**`

Required*

`**rendered_prompt**`

Required*

`**metadata**`

Required*

get_inner_content

Get the inner content of the function result when that is a KernelContent or

subclass of the first item of the value if it is a list.

**get_inner_content**

Get the inner content of the function result when that is a KernelContent or
subclass

of the first item of the value if it is a list.

Python

**Parameters**

**Name**

**Description**

`**index**`

Default value: 0

**function**

**Methods**

ﾉ

**Expand table**

`get_inner_content()`

ﾉ

**Expand table**

**Attributes**



Python

**metadata**

Python

**rendered_prompt**

Python

**value**

Python

`function: KernelFunctionMetadata`

`metadata: dict[str, Any]`

`rendered_prompt: str | ``None`

`value: Any`



**KernelArguments Class**

Reference

The arguments sent to the KernelFunction.

Initializes a new instance of the KernelArguments class.

This is a dict-like class with the additional field for the
execution_settings.

This class is derived from a dict, hence behaves the same way, just adds the

execution_settings as a dict, with service_id and the settings.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**settings**`

<xref:PromptExecutionSettings | List>[<xref:PromptExecutionSettings>]<xref: |

None>

The settings for the execution. If a list is given, make sure all items in the
list have a

unique service_id as that is used as the key for the dict.

Default value: None

`****kwargs**`

Required*

dict[str,<xref: Any>]

The arguments for the function invocation, works similar to a regular dict.

`KernelArguments(settings: PromptExecutionSettings | `

`list[PromptExecutionSettings] | dict[str, PromptExecutionSettings] | ``None`` = `

`None``, **kwargs: Any)`

ﾉ

**Expand table**



**KernelFunction Class**

Reference

Semantic Kernel function.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**metadata**`

Required*

`**invocation_duration_histogram**`

Required*

`**streaming_duration_histogram**`

Required*

from_method

Create a new instance of the KernelFunctionFromMethod class.

from_prompt

Create a new instance of the KernelFunctionFromPrompt class.

`KernelFunction(*, metadata: KernelFunctionMetadata, `

`invocation_duration_histogram: Histogram = ``None``, `

`streaming_duration_histogram: Histogram = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



function_copy

Copy the function, can also override the plugin_name.

invoke

Invoke the function with the given arguments.

invoke_stream

Invoke a stream async function with the given arguments.

**from_method**

Create a new instance of the KernelFunctionFromMethod class.

Python

**Parameters**

**Name**

**Description**

`**method**`

Required*

`**plugin_name**`

Required*

Default value: None

`**stream_method**`

Required*

Default value: None

**from_prompt**

Create a new instance of the KernelFunctionFromPrompt class.

Python

`from_method(method: Callable[[...], Any], plugin_name: str | ``None`` = ``None``, `

`stream_method: Callable[[...], Any] | ``None`` = ``None``) -> `

`KernelFunctionFromMethod`

ﾉ

**Expand table**

`from_prompt(function_name: str, plugin_name: str, description: str | ``None`` `

`= ``None``, prompt: str | ``None`` = ``None``, template_format: Literal[``'semantic-`

`kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-kernel'``,
prompt_template: `

`PromptTemplateBase | ``None`` = ``None``, prompt_template_config: `

`PromptTemplateConfig | ``None`` = ``None``, prompt_execution_settings: `

`PromptExecutionSettings | Sequence[PromptExecutionSettings] | `

`Mapping[str, PromptExecutionSettings] | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`



**Parameters**

**Name**

**Description**

`**function_name**`

Required*

`**plugin_name**`

Required*

`**description**`

Required*

Default value: None

`**prompt**`

Required*

Default value: None

`**template_format**`

Required*

Default value: semantic-kernel

`**prompt_template**`

Required*

Default value: None

`**prompt_template_config**`

Required*

Default value: None

`**prompt_execution_settings**`

Required*

Default value: None

**function_copy**

Copy the function, can also override the plugin_name.

Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

str

The new plugin name.

ﾉ

**Expand table**

`function_copy(plugin_name: str | ``None`` = ``None``) -> KernelFunction`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

**Returns**

**Type**

**Description**

KernelFunction

The copied function.

**invoke**

Invoke the function with the given arguments.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.functions.Kernel>

The kernel

`**arguments**`

Required*

KernelArguments

The Kernel arguments

Default value: None

`**metadata**`

Required*

<xref:Dict>[str,<xref: Any>]

Additional metadata.

Default value: {}

`**kwargs**`

Required*

Any

Additional keyword arguments that will be added to the KernelArguments.

**Returns**

ﾉ

**Expand table**

`async`` invoke(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``, `

`metadata: dict[str, Any] = {}, **kwargs: Any) -> FunctionResult | ``None`

ﾉ

**Expand table**



**Type**

**Description**

FunctionResult

The result of the function

**invoke_stream**

Invoke a stream async function with the given arguments.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.functions.Kernel>

The kernel

`**arguments**`

Required*

KernelArguments

The Kernel arguments

Default value: None

`**metadata**`

Required*

<xref:Dict>[str,<xref: Any>]

Additional metadata.

Default value: {}

`**kwargs**`

Required*

Any

Additional keyword arguments that will be added to the KernelArguments.

**description**

The description of the function.

**fully_qualified_name**

ﾉ

**Expand table**

`async`` invoke_stream(kernel: Kernel, arguments: KernelArguments | ``None`` = `

`None``, metadata: dict[str, Any] = {}, **kwargs: Any) -> `

`AsyncGenerator[FunctionResult | list[StreamingContentMixin | Any], Any]`

ﾉ

**Expand table**

**Attributes**



The fully qualified name of the function.

**is_prompt**

Whether the function is based on a prompt.

**name**

The name of the function.

**parameters**

The parameters for the function.

**plugin_name**

The name of the plugin that contains this function.

**return_parameter**

The return parameter for the function.

**stream_function**

The stream function for the function.

**function**

The function to call.

**prompt_execution_settings**

The AI prompt execution settings.

**prompt_template_config**

The prompt template configuration.

**metadata**



The metadata for the function.

Python

**invocation_duration_histogram**

Python

**streaming_duration_histogram**

Python

`metadata: KernelFunctionMetadata`

`invocation_duration_histogram: Histogram`

`streaming_duration_histogram: Histogram`



**KernelFunctionFromMethod Class**

Reference

Semantic Kernel Function from a method.

Initializes a new instance of the KernelFunctionFromMethod class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**method**`

Required*

<xref:Callable>[<xref:...>,<xref: Any>]

The method to be called

`**plugin_name**`

<xref:<xref:semantic_kernel.functions.str | None>>

The name of the plugin

Default value: None

`**stream_method**`

<xref:Callable>[<xref:...>,<xref: Any>]<xref: | None>

The stream method for the function

Default value: None

`**parameters**`

list[<xref:KernelParameterMetadata>]<xref: | None>

The parameters of the function

Default value: None

`**return_parameter**`

<xref:<xref:semantic_kernel.functions.KernelParameterMetadata |

None>>

The return parameter of the function

Default value: None

`**additional_metadata**`

dict[str,<xref: Any>]<xref: | None>

Additional metadata for the function

`KernelFunctionFromMethod(method: Callable[[...], Any], plugin_name: str | `

`None`` = ``None``, stream_method: Callable[[...], Any] | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] | ``None`` = ``None``, return_parameter: `

`KernelParameterMetadata | ``None`` = ``None``, additional_metadata: dict[str, Any] | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

gather_function_parameters

Gathers the function parameters from the arguments.

**gather_function_parameters**

Gathers the function parameters from the arguments.

Python

**Parameters**

**Name**

**Description**

`**context**`

Required*

**method**

Python

**stream_method**

Python

**Methods**

ﾉ

**Expand table**

`gather_function_parameters(context: FunctionInvocationContext) -> `

`dict[str, Any]`

ﾉ

**Expand table**

**Attributes**

`method: Callable[[...], Any]`



`stream_method: Callable[[...], Any] | ``None`



**KernelFunctionFromPrompt Class**

Reference

Semantic Kernel Function from a prompt.

Initializes a new instance of the KernelFunctionFromPrompt class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

str

The name of the function

`**plugin_name**`

str

The name of the plugin

Default value: None

`**description**`

str

The description for the function

Default value: None

`**prompt**`

<xref:Optional>[str]

The prompt

Default value: None

`**template_format**`

<xref:Optional>[str]

The template format, default is "semantic-kernel"

Default value: semantic-kernel

`**prompt_template**`

<xref:Optional>[<xref:KernelPromptTemplate>]

`KernelFunctionFromPrompt(function_name: str, plugin_name: str | ``None`` = ``None``, `

`description: str | ``None`` = ``None``, prompt: str | ``None`` = ``None``, template_format: `

`Literal[``'semantic-kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-
kernel'``, `

`prompt_template: PromptTemplateBase | ``None`` = ``None``, prompt_template_config: `

`PromptTemplateConfig | ``None`` = ``None``, prompt_execution_settings: `

`PromptExecutionSettings | Sequence[PromptExecutionSettings] | Mapping[str, `

`PromptExecutionSettings] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

The prompt template

Default value: None

`**prompt_template_config**`

<xref:Optional>[<xref:PromptTemplateConfig>]

The prompt template configuration

Default value: None

`**prompt_execution_settings**`

Optional

instance, list or dict of PromptExecutionSettings to be used by

the function, can also be supplied through

prompt_template_config, but the supplied one is used if both

are present. prompt_template_config

(Optional[PromptTemplateConfig]): the prompt template config.

Default value: None

from_directory

Creates a new instance of the KernelFunctionFromPrompt

class from a directory.

The directory needs to contain:

A prompt file named _skprompt.txt_

A config file named _config.json_

from_yaml

Creates a new instance of the KernelFunctionFromPrompt

class from a YAML string.

rewrite_execution_settings

Rewrite execution settings to a dictionary.

If the prompt_execution_settings is not a dictionary, it is

converted to a dictionary. If it is not supplied, but

prompt_template is, the prompt_template's execution settings

are used.

update_arguments_with_defaults

Update any missing values with their defaults.

**from_directory**

Creates a new instance of the KernelFunctionFromPrompt class from a directory.

The directory needs to contain:

**Methods**

ﾉ

**Expand table**



A prompt file named _skprompt.txt_

A config file named _config.json_

Python

**Parameters**

**Name**

**Description**

`**path**`

Required*

`**plugin_name**`

Required*

Default value: None

**Returns**

**Type**

**Description**

KernelFunctionFromPrompt

The kernel function from prompt

**from_yaml**

Creates a new instance of the KernelFunctionFromPrompt class from a YAML
string.

Python

**Parameters**

`from_directory(path: str, plugin_name: str | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_yaml(yaml_str: str, plugin_name: str | ``None`` = ``None``) -> `

`KernelFunctionFromPrompt`

ﾉ

**Expand table**



**Name**

**Description**

`**yaml_str**`

Required*

`**plugin_name**`

Required*

Default value: None

**rewrite_execution_settings**

Rewrite execution settings to a dictionary.

If the prompt_execution_settings is not a dictionary, it is converted to a
dictionary. If

it is not supplied, but prompt_template is, the prompt_template's execution
settings

are used.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**update_arguments_with_defaults**

Update any missing values with their defaults.

Python

**Parameters**

`rewrite_execution_settings(data: Any) -> dict[str, `

`PromptExecutionSettings]`

ﾉ

**Expand table**

`update_arguments_with_defaults(arguments: KernelArguments) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

**prompt_execution_settings**

Python

**prompt_template**

Python

**Attributes**

`prompt_execution_settings: dict[str, PromptExecutionSettings]`

`prompt_template: PromptTemplateBase`



**KernelFunctionMetadata Class**

Reference

The kernel function metadata.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**plugin_name**`

Required*

`**description**`

Required*

`**parameters**`

Required*

`**is_prompt**`

Required*

`KernelFunctionMetadata(*, name: Annotated[str, `

`_PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)], plugin_name: `

`Annotated[str | ``None``, _PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)] = `

`None``, description: str | ``None`` = ``None``, parameters: `

`list[KernelParameterMetadata] = ``None``, is_prompt: bool, is_asynchronous:
bool `

`| ``None`` = ``True``, return_parameter: KernelParameterMetadata | ``None`` = ``None``, `

`additional_properties: dict[str, Any] | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**is_asynchronous**`

Default value: True

`**return_parameter**`

Required*

`**additional_properties**`

Required*

custom_fully_qualified_name

Get the fully qualified name of the function with a custom

separator.

**custom_fully_qualified_name**

Get the fully qualified name of the function with a custom separator.

Python

**Parameters**

**Name**

**Description**

`**separator**`

Required*

str

The custom separator.

**Returns**

**Type**

**Description**

The fully qualified name of the function with a custom separator.

**Methods**

ﾉ

**Expand table**

`custom_fully_qualified_name(separator: str) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**fully_qualified_name**

Get the fully qualified name of the function.

A fully qualified name is the name of the combination of the plugin name and
the

function name, separated by a hyphen, if the plugin name is present.
Otherwise, it is

just the function name.

**Returns**

**Type**

**Description**

The fully qualified name of the function.

**additional_properties**

Python

**description**

Python

**is_asynchronous**

Python

**is_prompt**

Python

**Attributes**

ﾉ

**Expand table**

`additional_properties: dict[str, Any] | ``None`

`description: str | ``None`

`is_asynchronous: bool | ``None`



**name**

Python

**parameters**

Python

**plugin_name**

Python

**return_parameter**

Python

`is_prompt: bool`

`name: str`

`parameters: list[KernelParameterMetadata]`

`plugin_name: str | ``None`

`return_parameter: KernelParameterMetadata | ``None`



**KernelParameterMetadata Class**

Reference

The kernel parameter metadata.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**default_value**`

Required*

`**type**`

Default value: str

`**is_required**`

Required*

`**type_object**`

Required*

`KernelParameterMetadata(*, name: Annotated[str | ``None``, `

`_PydanticGeneralMetadata(pattern=``'^[0-9A-Za-z_]+$'``)], description: str | `

`None`` = ``None``, default_value: Any | ``None`` = ``None``, type: str | ``None`` = ``'str'``, `

`is_required: bool | ``None`` = ``False``, type_object: Any | ``None`` = ``None``, `

`schema_data: dict[str, Any] | ``None`` = ``None``, include_in_function_choices: bool `

`= ``True``)`

ﾉ

**Expand table**



**Name**

**Description**

`**schema_data**`

Required*

`**include_in_function_choices**`

Default value: True

form_schema

Create a schema for the parameter metadata.

infer_schema

Infer the schema for the parameter metadata.

**form_schema**

Create a schema for the parameter metadata.

Python

**Parameters**

**Name**

**Description**

`**data**`

Required*

**infer_schema**

Infer the schema for the parameter metadata.

Python

**Methods**

ﾉ

**Expand table**

`form_schema(data: Any) -> Any`

ﾉ

**Expand table**

`infer_schema(type_object: type | ``None`` = ``None``, parameter_type: str | ``None`` `

`= ``None``, default_value: Any | ``None`` = ``None``, description: str | ``None`` = ``None``, `

`structured_output: bool = ``False``) -> dict[str, Any] | ``None`



**Parameters**

**Name**

**Description**

`**type_object**`

Default value: None

`**parameter_type**`

Default value: None

`**default_value**`

Default value: None

`**description**`

Default value: None

`**structured_output**`

Default value: False

**default_value**

Python

**description**

Python

**include_in_function_choices**

Python

**is_required**

Python

ﾉ

**Expand table**

**Attributes**

`default_value: Any | ``None`

`description: str | ``None`

`include_in_function_choices: bool`

`is_required: bool | ``None`



**name**

Python

**schema_data**

Python

**type_**

Python

**type_object**

Python

`name: str | ``None`

`schema_data: dict[str, Any] | ``None`

`type_: str | ``None`

`type_object: Any | ``None`



**KernelPlugin Class**

Reference

Represents a Kernel Plugin with functions.

This class behaves mostly like a dictionary, with functions as values and
their names as

keys. When you add a function, through _.set_ or **_setitem_** , the function
is copied, the

metadata is deep-copied and the name of the plugin is set in the metadata and
added

to the dict of functions. This is done in the same way as a normal dict, so a
existing key

will be overwritten.

Class methods: from_object(plugin_name: str, plugin_instance: Any | dict[str, Any],

description: str | None = None): Create a plugin from a existing object, like a custom

class with annotated functions.

from_directory(plugin_name: str, parent_directory: str, description: str | None = None):

Create a plugin from a directory, parsing: .py files, .yaml files and
directories with

skprompt.txt and config.json files.

from_openapi( plugin_name: str, openapi_document_path: str,
execution_settings:

OpenAPIFunctionExecutionParameters | None = None, description: str | None = None):

Create a KernelPlugin.

**Constructor**

Python

**Parameters**

` Create a plugin from an OpenAPI document.`

`KernelPlugin(name: str, description: str | ``None`` = ``None``, functions: `

`KernelFunction | Callable[[...], Any] | KernelPlugin | `

`Sequence[KernelFunction | Callable[[...], Any] | KernelPlugin] | ``None`` | `

`dict[str, KernelFunction | Callable[[...], Any]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**name**`

Required*

The name of the plugin. The name can be upper/lower case letters and

underscores.

`**description**`

The description of the plugin.

Default value: None

`**functions**`

The functions in the plugin, will be rewritten to a dictionary of functions.

Default value: None

add

Add functions to the plugin.

add_dict

Add a dictionary of functions to the plugin.

add_list

Add a list of functions to the plugin.

from_directory

Create a plugin from a specified directory.

This method does not recurse into subdirectories

beyond one level deep from the specified plugin

directory. For YAML files, function names are

extracted from the content of the YAML files

themselves (the name property). For directories,

the function name is assumed to be the name of

the directory. Each KernelFunction object is

initialized with data parsed from the associated

files and added to a list of functions that are then

assigned to the created KernelPlugin object. A .py

file is parsed and a plugin created, the functions

within as then combined with any other functions

found. The python file needs to contain a class

with one or more kernel_function decorated

methods. If this class has a **_init_** method, it will be

called with the arguments provided in the

_class_init_arguments_ dictionary, the key needs to

be the same as the name of the class, with the

value being a dictionary of arguments to pass to

the class (using kwargs).

MyPlugins/

|<<-- pluginA.yaml |<<-- pluginB.yaml |

<<-- native_function.py |<<--

Directory1/

**Methods**

ﾉ

**Expand table**



|<<-- Directory2/ >>|<<-- skprompt.txt

>>|<<-- config.json

Calling _KernelPlugin.from_directory( "MyPlugins",_

_" /path/to")_ will create a KernelPlugin object

named "MyPlugins", containing KernelFunction

objects for _pluginA.yaml_ , _pluginB.yaml_ ,

_Directory1_ , and _Directory2_ , each initialized with

their respective configurations. And functions for

anything within native_function.py.

from_object

Creates a plugin that wraps the specified target

object and imports it into the kernel's plugin

collection.

from_openapi

Create a plugin from an OpenAPI document.

from_python_file

Create a plugin from a Python file.

from_text_search_with_get_search_results

Creates a plugin that wraps the text search

"get_search_results" function.

from_text_search_with_get_text_search_results

Creates a plugin that wraps the text search

"get_text_search_results" function.

from_text_search_with_search

Creates a plugin that wraps the text search

"search" function.

get

Get a function from the plugin.

get_functions_metadata

Get the metadata for the functions in the plugin.

set

Set a function in the plugin.

setdefault

Set a default value for a key.

update

Update the plugin with the functions from

another.

**add**

Add functions to the plugin.

` >>|<<-- skprompt.txt`

` >>|<<-- config.json`



Python

**add_dict**

Add a dictionary of functions to the plugin.

Python

**Parameters**

**Name**

**Description**

`**functions**`

Required*

**add_list**

Add a list of functions to the plugin.

Python

**Parameters**

**Name**

**Description**

`**functions**`

Required*

**from_directory**

`add(functions: Any) -> ``None`

`add_dict(functions: dict[str, KernelFunction | Callable[[...], Any]]) -> `

`None`

ﾉ

**Expand table**

`add_list(functions: list[KernelFunction | Callable[[...], Any] | `

`KernelPlugin]) -> ``None`

ﾉ

**Expand table**



Create a plugin from a specified directory.

This method does not recurse into subdirectories beyond one level deep from
the

specified plugin directory. For YAML files, function names are extracted from
the

content of the YAML files themselves (the name property). For directories, the

function name is assumed to be the name of the directory. Each KernelFunction

object is initialized with data parsed from the associated files and added to
a list of

functions that are then assigned to the created KernelPlugin object. A .py
file is

parsed and a plugin created, the functions within as then combined with any
other

functions found. The python file needs to contain a class with one or more

kernel_function decorated methods. If this class has a **_init_** method, it
will be called

with the arguments provided in the _class_init_arguments_ dictionary, the key
needs to

be the same as the name of the class, with the value being a dictionary of
arguments

to pass to the class (using kwargs).

MyPlugins/

|<<-- pluginA.yaml |<<-- pluginB.yaml |<<-- native_function.py |<<--

Directory1/

|<<-- Directory2/ >>|<<-- skprompt.txt >>|<<-- config.json

Calling _KernelPlugin.from_directory( "MyPlugins", "/path/to")_ will create a
KernelPlugin

object named "MyPlugins", containing KernelFunction objects for _pluginA.yaml_
,

_pluginB.yaml_ , _Directory1_ , and _Directory2_ , each initialized with their
respective

configurations. And functions for anything within native_function.py.

Python

**Parameters**

` >>|<<-- skprompt.txt`

` >>|<<-- config.json`

`from_directory(plugin_name: str, parent_directory: str, description: str `

`| ``None`` = ``None``, class_init_arguments: dict[str, dict[str, Any]] | ``None`` = `

`None``) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin, this is the name of the directory within

the parent directory

`**parent_directory**`

Required*

str

The parent directory path where the plugin directory resides

`**description**`

Required*

<xref:<xref:semantic_kernel.functions.str | None>>

The description of the plugin

Default value: None

`**class_init_arguments**`

Required*

dict[str,dict[str,<xref: Any>]]<xref: | None>

The class initialization arguments

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The created plugin of type KernelPlugin.

**Exceptions**

**Type**

**Description**

PluginInitializationError

If the plugin directory does not exist.

PluginInvalidNameError

If the plugin name is invalid.

**Examples**

Assuming a plugin directory structure as follows:

**from_object**

Creates a plugin that wraps the specified target object and imports it into
the

kernel's plugin collection.

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

str

The name of the plugin. Allows chars: upper, lower ASCII and

underscores.

`**plugin_instance**`

Required*

<xref:Any | dict>[str,<xref: Any>]

The plugin instance. This can be a custom class or a dictionary of classes

that contains methods with the kernel_function decorator for one or

several methods. See _TextMemoryPlugin_ as an example.

`**description**`

Required*

<xref:<xref:semantic_kernel.functions.str | None>>

The description of the plugin.

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The imported plugin of type KernelPlugin.

**from_openapi**

Create a plugin from an OpenAPI document.

Python

`from_object(plugin_name: str, plugin_instance: Any | dict[str, Any], `

`description: str | ``None`` = ``None``) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_openapi(plugin_name: str, openapi_document_path: str | ``None`` = ``None``, `

`openapi_parsed_spec: dict[str, Any] | ``None`` = ``None``, execution_settings: `

`OpenAPIFunctionExecutionParameters | ``None`` = ``None``, description: str | ``None`` `

`= ``None``) -> _T`



**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

The name of the plugin

`**openapi_document_path**`

Required*

The path to the OpenAPI document (optional)

Default value: None

`**openapi_parsed_spec**`

Required*

The parsed OpenAPI spec (optional)

Default value: None

`**execution_settings**`

Required*

The execution parameters

Default value: None

`**description**`

Required*

The description of the plugin

Default value: None

**Returns**

**Type**

**Description**

KernelPlugin

The created plugin

**Exceptions**

**Type**

**Description**

PluginInitializationError

if the plugin URL or plugin JSON/YAML is not provided

**from_python_file**

Create a plugin from a Python file.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

`from_python_file(plugin_name: str, py_file: str, description: str | ``None`` `

`= ``None``, class_init_arguments: dict[str, dict[str, Any]] | ``None`` = ``None``) -> `



**Parameters**

**Name**

**Description**

`**plugin_name**`

Required*

`**py_file**`

Required*

`**description**`

Required*

Default value: None

`**class_init_arguments**`

Required*

Default value: None

**from_text_search_with_get_search_results**

Creates a plugin that wraps the text search "get_search_results" function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`_T`

ﾉ

**Expand table**

`from_text_search_with_get_search_results(text_search: TextSearch, `

`plugin_name: str, plugin_description: str | ``None`` = ``None``, **kwargs: Any) -`

`> _T`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

**Type**

**Description**

a KernelPlugin.

**from_text_search_with_get_text_search_results**

Creates a plugin that wraps the text search "get_text_search_results"
function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

ﾉ

**Expand table**

`from_text_search_with_get_text_search_results(text_search: TextSearch, `

`plugin_name: str, plugin_description: str | ``None`` = ``None``, **kwargs: Any) -`

`> _T`

ﾉ

**Expand table**



**Type**

**Description**

a KernelPlugin.

**from_text_search_with_search**

Creates a plugin that wraps the text search "search" function.

Python

**Parameters**

**Name**

**Description**

`**text_search**`

Required*

The text search to use.

`**plugin_name**`

Required*

The name of the plugin.

`**plugin_description**`

Required*

The description of the search plugin.

Default value: None

`****kwargs**`

Required*

The keyword arguments to use to create the search function.

**Returns**

**Type**

**Description**

a KernelPlugin.

**get**

ﾉ

**Expand table**

`from_text_search_with_search(text_search: TextSearch, plugin_name: str, `

`plugin_description: str | ``None`` = ``None``, **kwargs: Any) -> _T`

ﾉ

**Expand table**

ﾉ

**Expand table**



Get a function from the plugin.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`**default**`

Required*

Default value: None

**get_functions_metadata**

Get the metadata for the functions in the plugin.

Python

**set**

Set a function in the plugin.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`get()`

ﾉ

**Expand table**

`get_functions_metadata()`

`set()`

ﾉ

**Expand table**



**Name**

**Description**

`**value**`

Required*

**setdefault**

Set a default value for a key.

Python

**Parameters**

**Name**

**Description**

`**key**`

Required*

`**value**`

Required*

Default value: None

**update**

Update the plugin with the functions from another.

Python

**name**

The name of the plugin. The name can be upper/lower case letters and
underscores.

Python

`setdefault()`

ﾉ

**Expand table**

`update()`

**Attributes**



**description**

The description of the plugin.

Python

**functions**

The functions in the plugin, indexed by their name.

Python

`name: StringConstraints(strip_whitespace=``None``, to_upper=``None``, `

`to_lower=``None``, strict=``None``, min_length=1, max_length=``None``,
pattern=^[0-`

`9A-Za-z_]+$)]`

`description: str | ``None`

`functions: dict[str, KernelFunction]`



**memory Package**

Reference

**Modules**

memory_query_result

memory_record

memory_store_base

null_memory

semantic_text_memory

semantic_text_memory_base

volatile_memory_store

**Classes**

SemanticTextMemory

Class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SemanticTextMemory.

VolatileMemoryStore

A volatile memory store that stores data in memory.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the VolatileMemoryStore class.

ﾉ

**Expand table**

ﾉ

**Expand table**



**memory_query_result Module**

Reference

**Classes**

MemoryQueryResult

The memory query result.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of MemoryQueryResult.

ﾉ

**Expand table**



**MemoryQueryResult Class**

Reference

The memory query result.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of MemoryQueryResult.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**is_reference**`

Required*

bool

Whether the record is a reference record.

`**external_source_name**`

Required*

<xref:Optional>[str]

The name of the external source.

`**id**`

Required*

str

A unique for the record.

`**description**`

Required*

<xref:Optional>[str]

The description of the record.

`**text**`

Required*

<xref:Optional>[str]

The text of the record.

`**additional_metadata**`

Required*

<xref:Optional>[str]

Custom metadata for the record.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_query_result.ndarray>

The embedding of the record.

`**relevance**`

float

`MemoryQueryResult(is_reference: bool, external_source_name: str | ``None``, id: `

`str, description: str | ``None``, text: str | ``None``, additional_metadata: str | `

`None``, embedding: ndarray | ``None``, relevance: float)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The relevance of the record to a known query.

from_memory_record

Create a new instance of MemoryQueryResult from a MemoryRecord.

**from_memory_record**

Create a new instance of MemoryQueryResult from a MemoryRecord.

Python

**Parameters**

**Name**

**Description**

`**record**`

Required*

<xref:semantic_kernel.memory.memory_query_result.MemoryRecord>

The MemoryRecord to create the MemoryQueryResult from.

`**relevance**`

Required*

float

The relevance of the record to a known query.

**Returns**

**Type**

**Description**

MemoryQueryResult

The created MemoryQueryResult.

**Methods**

ﾉ

**Expand table**

`static from_memory_record(record: MemoryRecord, relevance: float) -> `

`MemoryQueryResult`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Attributes**



**additional_metadata**

Python

**description**

Python

**embedding**

Python

**external_source_name**

Python

**id**

Python

**is_experimental**

Python

**is_reference**

`additional_metadata: str | ``None`

`description: str | ``None`

`embedding: ndarray | ``None`

`external_source_name: str | ``None`

`id: str`

`is_experimental = ``True`



Python

**relevance**

Python

**stage_status**

Python

**text**

Python

`is_reference: bool`

`relevance: float`

`stage_status = ``'experimental'`

`text: str | ``None`



**memory_record Module**

Reference

**Classes**

MemoryRecord

The in-built memory record.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of MemoryRecord.

ﾉ

**Expand table**



**MemoryRecord Class**

Reference

The in-built memory record.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of MemoryRecord.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**is_reference**`

Required*

bool

Whether the record is a reference record.

`**external_source_name**`

Required*

<xref:Optional>[str]

The name of the external source.

`**id**`

Required*

str

A unique for the record.

`**description**`

Required*

<xref:Optional>[str]

The description of the record.

`**text**`

Required*

<xref:Optional>[str]

The text of the record.

`**additional_metadata**`

Required*

<xref:Optional>[str]

Custom metadata for the record.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_record.ndarray>

The embedding of the record.

`MemoryRecord(is_reference: bool, external_source_name: str | ``None``, id: str, `

`description: str | ``None``, text: str | ``None``, additional_metadata: str | ``None``, `

`embedding: ndarray | ``None``, key: str | ``None`` = ``None``, timestamp: datetime | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**key**`

<xref:Optional>[str]

The key of the record.

Default value: None

`**timestamp**`

<xref:Optional>[datetime]

The timestamp of the record.

Default value: None

local_record

Create a local record.

reference_record

Create a reference record.

**local_record**

Create a local record.

Python

**Parameters**

**Name**

**Description**

`**id**`

Required*

str

A unique for the record.

`**text**`

Required*

str

The text of the record.

`**description**`

Required*

<xref:Optional>[str]

The description of the record.

`**additional_metadata**`

<xref:Optional>[str]

**Methods**

ﾉ

**Expand table**

`static local_record(id: str, text: str, description: str | ``None``, `

`additional_metadata: str | ``None``, embedding: ndarray, timestamp: datetime `

`| ``None`` = ``None``) -> MemoryRecord`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Custom metadata for the record.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_record.ndarray>

The embedding of the record.

`**timestamp**`

Required*

<xref:Optional>[datetime]

The timestamp of the record.

Default value: None

**Returns**

**Type**

**Description**

MemoryRecord

The local record.

**reference_record**

Create a reference record.

Python

**Parameters**

**Name**

**Description**

`**external_id**`

Required*

str

The external id of the record.

`**source_name**`

Required*

str

The name of the external source.

`**description**`

Required*

<xref:Optional>[str]

The description of the record.

`**additional_metadata**`

<xref:Optional>[str]

ﾉ

**Expand table**

`static reference_record(external_id: str, source_name: str, description: `

`str | ``None``, additional_metadata: str | ``None``, embedding: ndarray) -> `

`MemoryRecord`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Custom metadata for the record.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_record.ndarray>

The embedding of the record.

**Returns**

**Type**

**Description**

MemoryRecord

The reference record.

**additional_metadata**

Get the additional metadata of the memory record.

**description**

Get the description of the memory record.

**embedding**

Get the embedding of the memory record.

**id**

Get the unique identifier for the memory record.

**text**

Get the text of the memory record.

**timestamp**

Get the timestamp of the memory record.

ﾉ

**Expand table**

**Attributes**



**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**memory_store_base Module**

Reference

**Classes**

MemoryStoreBase

Base class for memory store.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**MemoryStoreBase Class**

Reference

Base class for memory store.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

close

Close the connection.

create_collection

Creates a new collection in the data store.

delete_collection

Deletes a collection from the data store.

does_collection_exist

Determines if a collection exists in the data store.

get

Gets a memory record from the data store. Does not guarantee that the

collection exists.

get_batch

Gets a batch of memory records from the data store. Does not guarantee

that the collection exists.

get_collections

Gets all collection names in the data store.

get_nearest_match

Gets the nearest match to an embedding of type float. Does not

guarantee that the collection exists.

get_nearest_matches

Gets the nearest matches to an embedding of type float. Does not

guarantee that the collection exists.

remove

Removes a memory record from the data store. Does not guarantee that

the collection exists.

remove_batch

Removes a batch of memory records from the data store. Does not

guarantee that the collection exists.

`MemoryStoreBase()`

**Methods**

ﾉ

**Expand table**



upsert

Upserts a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it

will be updated. If the record does not exist, it will be created.

upsert_batch

Upserts a group of memory records into the data store.

Does not guarantee that the collection exists. If the record already exists,
it

will be updated. If the record does not exist, it will be created.

**close**

Close the connection.

Python

**create_collection**

Creates a new collection in the data store.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

**delete_collection**

Deletes a collection from the data store.

Python

`async`` close()`

`abstract ``async`` create_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**

`abstract ``async`` delete_collection(collection_name: str) -> ``None`



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

**does_collection_exist**

Determines if a collection exists in the data store.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

**Returns**

**Type**

**Description**

bool

True if given collection exists, False if not.

**get**

Gets a memory record from the data store. Does not guarantee that the
collection

exists.

Python

ﾉ

**Expand table**

`abstract ``async`` does_collection_exist(collection_name: str) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**key**`

Required*

str

The unique id associated with the memory record to get.

`**with_embedding**`

Required*

bool

If true, the embedding will be returned in the memory record.

**Returns**

**Type**

**Description**

MemoryRecord

The memory record if found

**get_batch**

Gets a batch of memory records from the data store. Does not guarantee that
the

collection exists.

Python

**Parameters**

`abstract ``async`` get(collection_name: str, key: str, with_embedding: bool)
`

`-> MemoryRecord`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` get_batch(collection_name: str, keys: list[str], `

`with_embeddings: bool) -> list[MemoryRecord]`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**keys**`

Required*

<xref:List>[str]

The unique ids associated with the memory records to get.

`**with_embeddings**`

Required*

bool

If true, the embedding will be returned in the memory records.

**Returns**

**Type**

**Description**

List[MemoryRecord]

The memory records associated with the unique keys provided.

**get_collections**

Gets all collection names in the data store.

Python

**Returns**

**Type**

**Description**

List[str]

A group of collection names.

**get_nearest_match**

Gets the nearest match to an embedding of type float. Does not guarantee that
the

collection exists.

Python

ﾉ

**Expand table**

`abstract ``async`` get_collections() -> list[str]`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_store_base.ndarray>

The embedding to compare the collection's embeddings with.

`**min_relevance_score**`

Required*

float

The minimum relevance threshold for returned result.

`**with_embedding**`

Required*

bool

If true, the embeddings will be returned in the memory record.

**Returns**

**Type**

**Description**

Tuple[MemoryRecord,

float]

A tuple consisting of the MemoryRecord and the similarity score

as a float.

**get_nearest_matches**

Gets the nearest matches to an embedding of type float. Does not guarantee
that

the collection exists.

Python

`abstract ``async`` get_nearest_match(collection_name: str, embedding: `

`ndarray, min_relevance_score: float, with_embedding: bool) -> `

`tuple[MemoryRecord, float]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` get_nearest_matches(collection_name: str, embedding: `

`ndarray, limit: int, min_relevance_score: float, with_embeddings: bool) -`

`> list[tuple[MemoryRecord, float]]`



**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**embedding**`

Required*

<xref:semantic_kernel.memory.memory_store_base.ndarray>

The embedding to compare the collection's embeddings with.

`**limit**`

Required*

int

The maximum number of similarity results to return.

`**min_relevance_score**`

Required*

float

The minimum relevance threshold for returned results.

`**with_embeddings**`

Required*

bool

If true, the embeddings will be returned in the memory records.

**Returns**

**Type**

**Description**

List[Tuple[MemoryRecor

d, float]]

A list of tuples where item1 is a MemoryRecord and item2 is its

similarity score as a float.

**remove**

Removes a memory record from the data store. Does not guarantee that the

collection exists.

Python

**Parameters**

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` remove(collection_name: str, key: str) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**key**`

Required*

str

The unique id associated with the memory record to remove.

**remove_batch**

Removes a batch of memory records from the data store. Does not guarantee that

the collection exists.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**keys**`

Required*

<xref:List>[str]

The unique ids associated with the memory records to remove.

**upsert**

Upserts a memory record into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

Python

**Parameters**

`abstract ``async`` remove_batch(collection_name: str, keys: list[str]) -> `

`None`

ﾉ

**Expand table**

`abstract ``async`` upsert(collection_name: str, record: MemoryRecord) -> str`



**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**record**`

Required*

<xref:semantic_kernel.memory.memory_store_base.MemoryRecord>

The memory record to upsert.

**Returns**

**Type**

**Description**

str

The unique identifier for the memory record.

**upsert_batch**

Upserts a group of memory records into the data store.

Does not guarantee that the collection exists. If the record already exists,
it will be

updated. If the record does not exist, it will be created.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name associated with a collection of embeddings.

`**records**`

Required*

<xref:semantic_kernel.memory.memory_store_base.MemoryRecord>

The memory records to upsert.

**Returns**

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` upsert_batch(collection_name: str, records: `

`list[MemoryRecord]) -> list[str]`

ﾉ

**Expand table**



**Type**

**Description**

List[str]

The unique identifiers for the memory records.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**null_memory Module**

Reference

**Classes**

NullMemory

Class for null memory.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**NullMemory Class**

Reference

Class for null memory.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

get

Nullifies behavior of SemanticTextMemoryBase get.

get_collections

Nullifies behavior of SemanticTextMemoryBase get_collections.

save_information

Nullifies behavior of SemanticTextMemoryBase save_information.

save_reference

Nullifies behavior of SemanticTextMemoryBase save_reference.

search

Nullifies behavior of SemanticTextMemoryBase search.

**get**

Nullifies behavior of SemanticTextMemoryBase get.

Python

`NullMemory()`

**Methods**

ﾉ

**Expand table**

`async`` get(collection: str, query: str) -> MemoryQueryResult | ``None`



**Parameters**

**Name**

**Description**

`**collection**`

Required*

`**query**`

Required*

**get_collections**

Nullifies behavior of SemanticTextMemoryBase get_collections.

Python

**save_information**

Nullifies behavior of SemanticTextMemoryBase save_information.

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

`**text**`

Required*

`**id**`

Required*

ﾉ

**Expand table**

`async`` get_collections() -> list[str]`

`async`` save_information(collection: str, text: str, id: str, description: `

`str | ``None`` = ``None``, additional_metadata: str | ``None`` = ``None``) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**description**`

Required*

Default value: None

`**additional_metadata**`

Required*

Default value: None

**save_reference**

Nullifies behavior of SemanticTextMemoryBase save_reference.

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

`**text**`

Required*

`**external_id**`

Required*

`**external_source_name**`

Required*

`**description**`

Required*

Default value: None

`**additional_metadata**`

Required*

Default value: None

**search**

Nullifies behavior of SemanticTextMemoryBase search.

`async`` save_reference(collection: str, text: str, external_id: str, `

`external_source_name: str, description: str | ``None`` = ``None``, `

`additional_metadata: str | ``None`` = ``None``) -> ``None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

`**query**`

Required*

`**limit**`

Required*

Default value: 1

`**min_relevance_score**`

Required*

Default value: 0.7

**instance**

Python

**is_experimental**

Python

**stage_status**

Python

`async`` search(collection: str, query: str, limit: int = 1, `

`min_relevance_score: float = 0.7) -> list[MemoryQueryResult]`

ﾉ

**Expand table**

**Attributes**

`instance = NullMemory()`

`is_experimental = ``True`



`stage_status = ``'experimental'`



**semantic_text_memory Module**

Reference

**Classes**

SemanticTextMemory

Class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SemanticTextMemory.

ﾉ

**Expand table**



**SemanticTextMemory Class**

Reference

Class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SemanticTextMemory.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**storage**`

Required*

<xref:semantic_kernel.memory.semantic_text_memory.MemoryStoreBase>

The MemoryStoreBase to use for storage.

`**embeddings_generator**`

Required*

<xref:semantic_kernel.memory.semantic_text_memory.EmbeddingGeneratorBase>

The EmbeddingGeneratorBase to use for generating embeddings.

get

Get information from the memory (calls the memory store's get method).

get_collections

Get the list of collections in the memory (calls the memory store's
get_collections

method).

model_post_init

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when

calling it.

save_information

Save information to the memory (calls the memory store's upsert method).

`SemanticTextMemory(storage: MemoryStoreBase, embeddings_generator: `

`EmbeddingGeneratorBase)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



save_reference

Save a reference to the memory (calls the memory store's upsert method).

search

Search the memory (calls the memory store's get_nearest_matches method).

**get**

Get information from the memory (calls the memory store's get method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to get the information from.

`**key**`

Required*

str

The key of the information.

**Returns**

**Type**

**Description**

Optional[MemoryQueryResult]

The MemoryQueryResult if found, None otherwise.

**get_collections**

Get the list of collections in the memory (calls the memory store's
get_collections

method).

Python

**Returns**

`async`` get(collection: str, key: str) -> MemoryQueryResult | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_collections() -> list[str]`



**Type**

**Description**

List[str]

The list of all the memory collection names.

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private
attributes.

It takes context as an argument since that's what pydantic-core passes when
calling it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**Parameters**

**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**save_information**

Save information to the memory (calls the memory store's upsert method).

Python

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` save_information(collection: str, text: str, id: str, description:
str `

`| ``None`` = ``None``, additional_metadata: str | ``None`` = ``None``, embeddings_kwargs: `



**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to save the information to.

`**text**`

Required*

str

The text to save.

`**id**`

Required*

str

The id of the information.

`**description**`

Required*

<xref:Optional>[str]

The description of the information.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the information.

Default value: None

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the information.

Default value: {}

**save_reference**

Save a reference to the memory (calls the memory store's upsert method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

str

`dict[str, Any] | ``None`` = {}) -> ``None`

ﾉ

**Expand table**

`async`` save_reference(collection: str, text: str, external_id: str, `

`external_source_name: str, description: str | ``None`` = ``None``, `

`additional_metadata: str | ``None`` = ``None``, embeddings_kwargs: dict[str, Any] | `

`None`` = {}) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The collection to save the reference to.

`**text**`

Required*

str

The text to save.

`**external_id**`

Required*

str

The external id of the reference.

`**external_source_name**`

Required*

str

The external source name of the reference.

`**description**`

Required*

<xref:Optional>[str]

The description of the reference.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the reference.

Default value: None

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the reference.

Default value: {}

**search**

Search the memory (calls the memory store's get_nearest_matches method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to search in.

`**query**`

Required*

str

The query to search for.

`**limit**`

Required*

int

The maximum number of results to return. (default: {1})

`async`` search(collection: str, query: str, limit: int = 1, `

`min_relevance_score: float = 0.0, with_embeddings: bool = ``False``, `

`embeddings_kwargs: dict[str, Any] | ``None`` = {}) -> list[MemoryQueryResult]`

ﾉ

**Expand table**



**Name**

**Description**

Default value: 1

`**min_relevance_score**`

Required*

float

The minimum relevance score to return. (default: {0.0})

Default value: 0.0

`**with_embeddings**`

Required*

bool

Whether to return the embeddings of the results. (default: {False})

Default value: False

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the information.

Default value: {}

**Returns**

**Type**

**Description**

List[MemoryQueryResult]

The list of MemoryQueryResult found.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**semantic_text_memory_base Module**

Reference

**Classes**

SemanticTextMemoryBase

Base class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**SemanticTextMemoryBase Class**

Reference

Base class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

get

Get information from the memory (calls the memory store's get method).

get_collections

Get the list of collections in the memory (calls the memory store's

get_collections method).

save_information

Save information to the memory (calls the memory store's upsert method).

save_reference

Save a reference to the memory (calls the memory store's upsert method).

search

Search the memory (calls the memory store's get_nearest_matches method).

**get**

Get information from the memory (calls the memory store's get method).

Python

`SemanticTextMemoryBase()`

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to get the information from.

`**key**`

Required*

str

The key of the information.

**Returns**

**Type**

**Description**

Optional[MemoryQueryResult]

The MemoryQueryResult if found, None otherwise.

**get_collections**

Get the list of collections in the memory (calls the memory store's
get_collections

method).

Python

**Returns**

**Type**

**Description**

List[str]

The list of all the memory collection names.

**save_information**

`abstract ``async`` get(collection: str, key: str) -> MemoryQueryResult | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`abstract ``async`` get_collections() -> list[str]`

ﾉ

**Expand table**



Save information to the memory (calls the memory store's upsert method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to save the information to.

`**text**`

Required*

str

The text to save.

`**id**`

Required*

str

The id of the information.

`**description**`

Required*

<xref:Optional>[str]

The description of the information.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the information.

Default value: None

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the information.

Default value: None

**save_reference**

Save a reference to the memory (calls the memory store's upsert method).

Python

`abstract ``async`` save_information(collection: str, text: str, id: str, `

`description: str | ``None`` = ``None``, additional_metadata: str | ``None`` = ``None``, `

`embeddings_kwargs: dict[str, Any] | ``None`` = ``None``) -> ``None`

ﾉ

**Expand table**

`abstract ``async`` save_reference(collection: str, text: str, external_id: `

`str, external_source_name: str, description: str | ``None`` = ``None``, `

`additional_metadata: str | ``None`` = ``None``) -> ``None`



**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to save the reference to.

`**text**`

Required*

str

The text to save.

`**external_id**`

Required*

str

The external id of the reference.

`**external_source_name**`

Required*

str

The external source name of the reference.

`**description**`

Required*

<xref:Optional>[str]

The description of the reference.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the reference.

Default value: None

**search**

Search the memory (calls the memory store's get_nearest_matches method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to search in.

`**query**`

Required*

str

The query to search for.

ﾉ

**Expand table**

`abstract ``async`` search(collection: str, query: str, limit: int = 1, `

`min_relevance_score: float = 0.7) -> list[MemoryQueryResult]`

ﾉ

**Expand table**



**Name**

**Description**

`**limit**`

Required*

int

The maximum number of results to return. (default: {1})

Default value: 1

`**min_relevance_score**`

Required*

float

The minimum relevance score to return. (default: {0.0})

Default value: 0.7

`**with_embeddings**`

Required*

bool

Whether to return the embeddings of the results. (default: {False})

**Returns**

**Type**

**Description**

List[MemoryQueryResult]

The list of MemoryQueryResult found.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**volatile_memory_store Module**

Reference

**Classes**

VolatileMemoryStore

A volatile memory store that stores data in memory.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the VolatileMemoryStore class.

ﾉ

**Expand table**



**VolatileMemoryStore Class**

Reference

A volatile memory store that stores data in memory.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the VolatileMemoryStore class.

**Constructor**

Python

compute_similarity_scores

Computes the cosine similarity scores between a query embedding

and a group of embeddings.

create_collection

Creates a new collection if it does not exist.

delete_collection

Deletes a collection.

does_collection_exist

Checks if a collection exists.

get

Gets a record.

get_batch

Gets a batch of records.

get_collections

Gets the list of collections.

get_nearest_match

Gets the nearest match to an embedding using cosine similarity.

get_nearest_matches

Gets the nearest matches to an embedding using cosine similarity.

remove

Removes a record.

remove_batch

Removes a batch of records.

upsert

Upserts a record.

upsert_batch

Upserts a batch of records.

`VolatileMemoryStore()`

**Methods**

ﾉ

**Expand table**



**compute_similarity_scores**

Computes the cosine similarity scores between a query embedding and a group of

embeddings.

Python

**Parameters**

**Name**

**Description**

`**embedding**`

Required*

<xref:semantic_kernel.memory.volatile_memory_store.ndarray>

The query embedding.

`**embedding_array**`

Required*

<xref:semantic_kernel.memory.volatile_memory_store.ndarray>

The group of embeddings.

**Returns**

**Type**

**Description**

<xref:ndarray>

The cosine similarity scores.

**create_collection**

Creates a new collection if it does not exist.

Python

**Parameters**

`compute_similarity_scores(embedding: ndarray, embedding_array: ndarray) -`

`> ndarray`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` create_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to create.

**Returns**

**Type**

**Description**

None

**delete_collection**

Deletes a collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to delete.

**Returns**

**Type**

**Description**

None

**does_collection_exist**

Checks if a collection exists.

ﾉ

**Expand table**

`async`` delete_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to check.

**Returns**

**Type**

**Description**

bool

True if the collection exists; otherwise, False.

**get**

Gets a record.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the record from.

`**key**`

Required*

str

The unique database key of the record.

`async`` does_collection_exist(collection_name: str) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get(collection_name: str, key: str, with_embedding: bool = ``False``)
-`

`> MemoryRecord`

ﾉ

**Expand table**



**Name**

**Description**

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

MemoryRecord

The record.

**get_batch**

Gets a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the records from.

`**keys**`

Required*

<xref:List>[str]

The unique database keys of the records.

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

ﾉ

**Expand table**

`async`` get_batch(collection_name: str, keys: list[str], with_embeddings: `

`bool = ``False``) -> list[MemoryRecord]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

List[MemoryRecord]

The records.

**get_collections**

Gets the list of collections.

Python

**Returns**

**Type**

**Description**

List[str]

The list of collections.

**get_nearest_match**

Gets the nearest match to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest match from.

`**embedding**`

Required*

<xref:semantic_kernel.memory.volatile_memory_store.ndarray>

The embedding to find the nearest match to.

`**min_relevance_score**`

float

`async`` get_collections() -> list[str]`

ﾉ

**Expand table**

`async`` get_nearest_match(collection_name: str, embedding: ndarray, `

`min_relevance_score: float = 0.0, with_embedding: bool = ``False``) -> `

`tuple[MemoryRecord, float]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The minimum relevance score of the match. (default: {0.0})

Default value: 0.0

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

Tuple[MemoryRecord, float]

The record and the relevance score.

**get_nearest_matches**

Gets the nearest matches to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.memory.volatile_memory_store.ndarray>

The embedding to find the nearest matches to.

`**limit**`

Required*

int

The maximum number of matches to return.

`**min_relevance_score**`

Required*

float

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

ﾉ

**Expand table**

`async`` get_nearest_matches(collection_name: str, embedding: ndarray, `

`limit: int, min_relevance_score: float = 0.0, with_embeddings: bool = `

`False``) -> list[tuple[MemoryRecord, float]]`

ﾉ

**Expand table**



**Name**

**Description**

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[Tuple[MemoryRecord, float]]

The records and their relevance scores.

**remove**

Removes a record.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the record from.

`**key**`

Required*

str

The unique database key of the record to remove.

**Returns**

**Type**

**Description**

None

ﾉ

**Expand table**

`async`` remove(collection_name: str, key: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**remove_batch**

Removes a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the records from.

`**keys**`

Required*

<xref:List>[str]

The unique database keys of the records to remove.

**Returns**

**Type**

**Description**

None

**upsert**

Upserts a record.

Python

**Parameters**

`async`` remove_batch(collection_name: str, keys: list[str]) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` upsert(collection_name: str, record: MemoryRecord) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to upsert the record into.

`**record**`

Required*

<xref:semantic_kernel.memory.volatile_memory_store.MemoryRecord>

The record to upsert.

**Returns**

**Type**

**Description**

str

The unique database key of the record.

**upsert_batch**

Upserts a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to upsert the records into.

`**records**`

Required*

<xref:List>[<xref:MemoryRecord>]

The records to upsert.

**Returns**

ﾉ

**Expand table**

`async`` upsert_batch(collection_name: str, records: list[MemoryRecord]) -> `

`list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

List[str]

The unique database keys of the records.

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**SemanticTextMemory Class**

Reference

Class for semantic text memory.

Note: This class is marked as 'experimental' and may change in the future.

Initialize a new instance of SemanticTextMemory.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**storage**`

Required*

<xref:semantic_kernel.memory.MemoryStoreBase>

The MemoryStoreBase to use for storage.

`**embeddings_generator**`

Required*

<xref:semantic_kernel.memory.EmbeddingGeneratorBase>

The EmbeddingGeneratorBase to use for generating embeddings.

get

Get information from the memory (calls the memory store's get method).

get_collections

Get the list of collections in the memory (calls the memory store's

get_collections method).

model_post_init

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when

calling it.

`SemanticTextMemory(storage: MemoryStoreBase, embeddings_generator: `

`EmbeddingGeneratorBase)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



save_information

Save information to the memory (calls the memory store's upsert method).

save_reference

Save a reference to the memory (calls the memory store's upsert method).

search

Search the memory (calls the memory store's get_nearest_matches method).

**get**

Get information from the memory (calls the memory store's get method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to get the information from.

`**key**`

Required*

str

The key of the information.

**Returns**

**Type**

**Description**

Optional[MemoryQueryResult]

The MemoryQueryResult if found, None otherwise.

**get_collections**

Get the list of collections in the memory (calls the memory store's
get_collections

method).

Python

`async`` get(collection: str, key: str) -> MemoryQueryResult | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get_collections() -> list[str]`



**Returns**

**Type**

**Description**

List[str]

The list of all the memory collection names.

**model_post_init**

This function is meant to behave like a BaseModel method to initialise private

attributes.

It takes context as an argument since that's what pydantic-core passes when
calling

it.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**Parameters**

**Name**

**Description**

`**self**`

Required*

The BaseModel instance.

`**context**`

Required*

The context.

**save_information**

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Save information to the memory (calls the memory store's upsert method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to save the information to.

`**text**`

Required*

str

The text to save.

`**id**`

Required*

str

The id of the information.

`**description**`

Required*

<xref:Optional>[str]

The description of the information.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the information.

Default value: None

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the information.

Default value: {}

**save_reference**

Save a reference to the memory (calls the memory store's upsert method).

Python

`async`` save_information(collection: str, text: str, id: str, description: `

`str | ``None`` = ``None``, additional_metadata: str | ``None`` = ``None``, `

`embeddings_kwargs: dict[str, Any] | ``None`` = {}) -> ``None`

ﾉ

**Expand table**

`async`` save_reference(collection: str, text: str, external_id: str, `

`external_source_name: str, description: str | ``None`` = ``None``, `

`additional_metadata: str | ``None`` = ``None``, embeddings_kwargs: dict[str, Any] `

`| ``None`` = {}) -> ``None`



**Parameters**

**Name**

**Description**

`**collection**`

Required*

str

The collection to save the reference to.

`**text**`

Required*

str

The text to save.

`**external_id**`

Required*

str

The external id of the reference.

`**external_source_name**`

Required*

str

The external source name of the reference.

`**description**`

Required*

<xref:Optional>[str]

The description of the reference.

Default value: None

`**additional_metadata**`

Required*

<xref:Optional>[str]

Additional metadata of the reference.

Default value: None

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the reference.

Default value: {}

**search**

Search the memory (calls the memory store's get_nearest_matches method).

Python

**Parameters**

**Name**

**Description**

`**collection**`

str

ﾉ

**Expand table**

`async`` search(collection: str, query: str, limit: int = 1, `

`min_relevance_score: float = 0.0, with_embeddings: bool = ``False``, `

`embeddings_kwargs: dict[str, Any] | ``None`` = {}) -> list[MemoryQueryResult]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The collection to search in.

`**query**`

Required*

str

The query to search for.

`**limit**`

Required*

int

The maximum number of results to return. (default: {1})

Default value: 1

`**min_relevance_score**`

Required*

float

The minimum relevance score to return. (default: {0.0})

Default value: 0.0

`**with_embeddings**`

Required*

bool

Whether to return the embeddings of the results. (default: {False})

Default value: False

`**embeddings_kwargs**`

Required*

<xref:Optional>[<xref:Dict>[str,<xref: Any>]]

The embeddings kwargs of the information.

Default value: {}

**Returns**

**Type**

**Description**

List[MemoryQueryResult]

The list of MemoryQueryResult found.

**is_experimental**

Python

**stage_status**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`







**VolatileMemoryStore Class**

Reference

A volatile memory store that stores data in memory.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of the VolatileMemoryStore class.

**Constructor**

Python

compute_similarity_scores

Computes the cosine similarity scores between a query embedding

and a group of embeddings.

create_collection

Creates a new collection if it does not exist.

delete_collection

Deletes a collection.

does_collection_exist

Checks if a collection exists.

get

Gets a record.

get_batch

Gets a batch of records.

get_collections

Gets the list of collections.

get_nearest_match

Gets the nearest match to an embedding using cosine similarity.

get_nearest_matches

Gets the nearest matches to an embedding using cosine similarity.

remove

Removes a record.

remove_batch

Removes a batch of records.

upsert

Upserts a record.

upsert_batch

Upserts a batch of records.

`VolatileMemoryStore()`

**Methods**

ﾉ

**Expand table**



**compute_similarity_scores**

Computes the cosine similarity scores between a query embedding and a group of

embeddings.

Python

**Parameters**

**Name**

**Description**

`**embedding**`

Required*

<xref:semantic_kernel.memory.ndarray>

The query embedding.

`**embedding_array**`

Required*

<xref:semantic_kernel.memory.ndarray>

The group of embeddings.

**Returns**

**Type**

**Description**

<xref:ndarray>

The cosine similarity scores.

**create_collection**

Creates a new collection if it does not exist.

Python

**Parameters**

`compute_similarity_scores(embedding: ndarray, embedding_array: ndarray) -`

`> ndarray`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` create_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to create.

**Returns**

**Type**

**Description**

None

**delete_collection**

Deletes a collection.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to delete.

**Returns**

**Type**

**Description**

None

**does_collection_exist**

Checks if a collection exists.

ﾉ

**Expand table**

`async`` delete_collection(collection_name: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to check.

**Returns**

**Type**

**Description**

bool

True if the collection exists; otherwise, False.

**get**

Gets a record.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the record from.

`**key**`

Required*

str

The unique database key of the record.

`async`` does_collection_exist(collection_name: str) -> bool`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` get(collection_name: str, key: str, with_embedding: bool = ``False``)
-`

`> MemoryRecord`

ﾉ

**Expand table**



**Name**

**Description**

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

MemoryRecord

The record.

**get_batch**

Gets a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the records from.

`**keys**`

Required*

<xref:List>[str]

The unique database keys of the records.

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

ﾉ

**Expand table**

`async`` get_batch(collection_name: str, keys: list[str], with_embeddings: `

`bool = ``False``) -> list[MemoryRecord]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

List[MemoryRecord]

The records.

**get_collections**

Gets the list of collections.

Python

**Returns**

**Type**

**Description**

List[str]

The list of collections.

**get_nearest_match**

Gets the nearest match to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest match from.

`**embedding**`

Required*

<xref:semantic_kernel.memory.ndarray>

The embedding to find the nearest match to.

`**min_relevance_score**`

float

`async`` get_collections() -> list[str]`

ﾉ

**Expand table**

`async`` get_nearest_match(collection_name: str, embedding: ndarray, `

`min_relevance_score: float = 0.0, with_embedding: bool = ``False``) -> `

`tuple[MemoryRecord, float]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The minimum relevance score of the match. (default: {0.0})

Default value: 0.0

`**with_embedding**`

Required*

bool

Whether to include the embedding in the result. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

Tuple[MemoryRecord, float]

The record and the relevance score.

**get_nearest_matches**

Gets the nearest matches to an embedding using cosine similarity.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to get the nearest matches from.

`**embedding**`

Required*

<xref:semantic_kernel.memory.ndarray>

The embedding to find the nearest matches to.

`**limit**`

Required*

int

The maximum number of matches to return.

`**min_relevance_score**`

Required*

float

The minimum relevance score of the matches. (default: {0.0})

Default value: 0.0

ﾉ

**Expand table**

`async`` get_nearest_matches(collection_name: str, embedding: ndarray, `

`limit: int, min_relevance_score: float = 0.0, with_embeddings: bool = `

`False``) -> list[tuple[MemoryRecord, float]]`

ﾉ

**Expand table**



**Name**

**Description**

`**with_embeddings**`

Required*

bool

Whether to include the embeddings in the results. (default: {False})

Default value: False

**Returns**

**Type**

**Description**

List[Tuple[MemoryRecord, float]]

The records and their relevance scores.

**remove**

Removes a record.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the record from.

`**key**`

Required*

str

The unique database key of the record to remove.

**Returns**

**Type**

**Description**

None

ﾉ

**Expand table**

`async`` remove(collection_name: str, key: str) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**remove_batch**

Removes a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to remove the records from.

`**keys**`

Required*

<xref:List>[str]

The unique database keys of the records to remove.

**Returns**

**Type**

**Description**

None

**upsert**

Upserts a record.

Python

**Parameters**

`async`` remove_batch(collection_name: str, keys: list[str]) -> ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` upsert(collection_name: str, record: MemoryRecord) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to upsert the record into.

`**record**`

Required*

<xref:semantic_kernel.memory.MemoryRecord>

The record to upsert.

**Returns**

**Type**

**Description**

str

The unique database key of the record.

**upsert_batch**

Upserts a batch of records.

Python

**Parameters**

**Name**

**Description**

`**collection_name**`

Required*

str

The name of the collection to upsert the records into.

`**records**`

Required*

<xref:List>[<xref:MemoryRecord>]

The records to upsert.

**Returns**

ﾉ

**Expand table**

`async`` upsert_batch(collection_name: str, records: list[MemoryRecord]) -> `

`list[str]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

List[str]

The unique database keys of the records.

**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**planners Package**

Reference

**Packages**

function_calling_stepwise_planner

sequential_planner

**Modules**

plan

planner_extensions

planner_options

**Classes**

FunctionCallingStepwisePlanner

A Function Calling Stepwise Planner.

Initialize a new instance of the

FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based

on top of an OpenAI Chat Completion service (whether

it be AzureOpenAI or OpenAI), so that we can use tools.

If the options are configured to use callbacks to get the

initial plan and the step prompt, the planner will use

those provided callbacks to get that information.

Otherwise, it will read from the default yaml plan file

and the step prompt file.

FunctionCallingStepwisePlannerOptions

The Function Calling Stepwise Planner Options.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner.

Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

Plan

A plan for the kernel.

Initializes a new instance of the Plan class.

PlannerOptions

The default planner options that planners inherit from.

Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

SequentialPlanner

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.



**function_calling_stepwise_planner**

**Package**

Reference

**Modules**

function_calling_stepwise_planner

function_calling_stepwise_planner_options

function_calling_stepwise_planner_result

**Classes**

FunctionCallingStepwisePlanner

A Function Calling Stepwise Planner.

Initialize a new instance of the

FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based

on top of an OpenAI Chat Completion service (whether

it be AzureOpenAI or OpenAI), so that we can use tools.

If the options are configured to use callbacks to get the

initial plan and the step prompt, the planner will use

those provided callbacks to get that information.

Otherwise, it will read from the default yaml plan file

and the step prompt file.

FunctionCallingStepwisePlannerOptions

The Function Calling Stepwise Planner Options.

Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

ﾉ

**Expand table**

ﾉ

**Expand table**



FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner.

Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.



**function_calling_stepwise_planner**

**Module**

Reference

**Classes**

FunctionCallingStepwisePlanner

A Function Calling Stepwise Planner.

Initialize a new instance of the FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based on top

of an OpenAI Chat Completion service (whether it be

AzureOpenAI or OpenAI), so that we can use tools.

If the options are configured to use callbacks to get the initial

plan and the step prompt, the planner will use those provided

callbacks to get that information. Otherwise, it will read from

the default yaml plan file and the step prompt file.

ﾉ

**Expand table**



**FunctionCallingStepwisePlanner Class**

Reference

A Function Calling Stepwise Planner.

Initialize a new instance of the FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based on top of an OpenAI Chat

Completion service (whether it be AzureOpenAI or OpenAI), so that we can use
tools.

If the options are configured to use callbacks to get the initial plan and the
step prompt,

the planner will use those provided callbacks to get that information.
Otherwise, it will

read from the default yaml plan file and the step prompt file.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

Required*

str

The service id

`**options**`

<xref:Optional>[<xref:FunctionCallingStepwisePlannerOptions>],<xref: optional>

The options for the function calling stepwise planner. Defaults to None.

Default value: None

invoke

Execute the function calling stepwise planner.

`FunctionCallingStepwisePlanner(service_id: str, options: `

`FunctionCallingStepwisePlannerOptions | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**invoke**

Execute the function calling stepwise planner.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**question**`

Required*

The input question

`**arguments**`

Required*

(optional) The kernel arguments

Default value: None

`**kwargs**`

Required*

(optional) Additional keyword arguments

**Returns**

**Type**

**Description**

FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner

**Exceptions**

**Type**

**Description**

PlannerInvalidConfigurationError

If the input question is empty

`async`` invoke(kernel: Kernel, question: str, arguments: KernelArguments | `

`None`` = ``None``, **kwargs: Any) -> FunctionCallingStepwisePlannerResult`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**generate_plan_yaml**

Python

**options**

Python

**service_id**

Python

**step_prompt**

Python

**Attributes**

`generate_plan_yaml: str`

`options: FunctionCallingStepwisePlannerOptions`

`service_id: str`

`step_prompt: str`



**function_calling_stepwise_planner_optio**

**ns Module**

Reference

**Classes**

FunctionCallingStepwisePlannerOptions

The Function Calling Stepwise Planner Options.

Create a new model by parsing and validating input

data from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

ﾉ

**Expand table**



**FunctionCallingStepwisePlannerOptions**

**Class**

Reference

The Function Calling Stepwise Planner Options.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**excluded_plugins**`

Required*

`**excluded_functions**`

Required*

`**get_available_functions**`

Required*

`**max_tokens**`

`FunctionCallingStepwisePlannerOptions(*, excluded_plugins: set[str] =
``None``, `

`excluded_functions: set[str] = ``None``, get_available_functions: `

`Callable[[PlannerOptions, str | ``None``], list[KernelFunctionMetadata]] | ``None`` `

`= ``None``, max_tokens: int | ``None`` = ``None``, max_tokens_ratio: float | ``None`` = 0.1, `

`max_completion_tokens: int | ``None`` = ``None``, max_prompt_tokens: int | ``None`` = `

`None``, get_initial_plan: Callable[[], str] | ``None`` = ``None``, get_step_prompt: `

`Callable[[], str] | ``None`` = ``None``, max_iterations: int | ``None`` = 15, `

`min_iteration_time_ms: int | ``None`` = 100, execution_settings: `

`OpenAIPromptExecutionSettings | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**max_tokens_ratio**`

Default value: 0.1

`**max_completion_tokens**`

Required*

`**max_prompt_tokens**`

Required*

`**get_initial_plan**`

Required*

`**get_step_prompt**`

Required*

`**max_iterations**`

Default value: 15

`**min_iteration_time_ms**`

Default value: 100

`**execution_settings**`

Required*

calculate_token_limits

Calculate the token limits based on the max_tokens and

max_tokens_ratio.

**calculate_token_limits**

Calculate the token limits based on the max_tokens and max_tokens_ratio.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`calculate_token_limits(data: Any) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

`**data**`

Required*

**excluded_functions**

Python

**excluded_plugins**

Python

**execution_settings**

Python

**get_available_functions**

Python

**get_initial_plan**

Python

**Attributes**

`excluded_functions: set[str]`

`excluded_plugins: set[str]`

`execution_settings: OpenAIPromptExecutionSettings | ``None`

`get_available_functions: Callable[[``'PlannerOptions'``, str | ``None``], `

`list[KernelFunctionMetadata]] | ``None`

`get_initial_plan: Callable[[], str] | ``None`



**get_step_prompt**

Python

**max_completion_tokens**

Python

**max_iterations**

Python

**max_prompt_tokens**

Python

**max_tokens**

Python

**max_tokens_ratio**

Python

**min_iteration_time_ms**

`get_step_prompt: Callable[[], str] | ``None`

`max_completion_tokens: int | ``None`

`max_iterations: int | ``None`

`max_prompt_tokens: int | ``None`

`max_tokens: int | ``None`

`max_tokens_ratio: float | ``None`



Python

`min_iteration_time_ms: int | ``None`



**function_calling_stepwise_planner_result**

**Module**

Reference

**Classes**

FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner.

Create a new model by parsing and validating input data

from keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if

the input data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field

name.

UserInteraction

The Kernel Function used to interact with the user.

ﾉ

**Expand table**



**FunctionCallingStepwisePlannerResult**

**Class**

Reference

The result of the function calling stepwise planner.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**final_answer**`

Required*

`**chat_history**`

Required*

`**iterations**`

Required*

**chat_history**

Python

`FunctionCallingStepwisePlannerResult(*, final_answer: str = ``''``, `

`chat_history: ChatHistory | ``None`` = ``None``, iterations: int = 0)`

ﾉ

**Expand table**

**Attributes**



**final_answer**

Python

**iterations**

Python

`chat_history: ChatHistory | ``None`

`final_answer: str`

`iterations: int`



**UserInteraction Class**

Reference

The Kernel Function used to interact with the user.

**Constructor**

Python

send_final_answer

Send the final answer to the user.

**send_final_answer**

Send the final answer to the user.

Python

**Parameters**

**Name**

**Description**

`**answer**`

Required*

`UserInteraction()`

**Methods**

ﾉ

**Expand table**

`send_final_answer(answer: Annotated[str, ``'The final answer'``]) -> str`

ﾉ

**Expand table**



**FunctionCallingStepwisePlanner Class**

Reference

A Function Calling Stepwise Planner.

Initialize a new instance of the FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based on top of an OpenAI Chat

Completion service (whether it be AzureOpenAI or OpenAI), so that we can use
tools.

If the options are configured to use callbacks to get the initial plan and the
step prompt,

the planner will use those provided callbacks to get that information.
Otherwise, it will

read from the default yaml plan file and the step prompt file.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

Required*

str

The service id

`**options**`

<xref:Optional>[<xref:FunctionCallingStepwisePlannerOptions>],<xref: optional>

The options for the function calling stepwise planner. Defaults to None.

Default value: None

invoke

Execute the function calling stepwise planner.

`FunctionCallingStepwisePlanner(service_id: str, options: `

`FunctionCallingStepwisePlannerOptions | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**invoke**

Execute the function calling stepwise planner.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**question**`

Required*

The input question

`**arguments**`

Required*

(optional) The kernel arguments

Default value: None

`**kwargs**`

Required*

(optional) Additional keyword arguments

**Returns**

**Type**

**Description**

FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner

**Exceptions**

**Type**

**Description**

PlannerInvalidConfigurationError

If the input question is empty

`async`` invoke(kernel: Kernel, question: str, arguments: KernelArguments | `

`None`` = ``None``, **kwargs: Any) -> FunctionCallingStepwisePlannerResult`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**generate_plan_yaml**

Python

**options**

Python

**service_id**

Python

**step_prompt**

Python

**Attributes**

`generate_plan_yaml: str`

`options: FunctionCallingStepwisePlannerOptions`

`service_id: str`

`step_prompt: str`



**FunctionCallingStepwisePlannerOptions**

**Class**

Reference

The Function Calling Stepwise Planner Options.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**excluded_plugins**`

Required*

`**excluded_functions**`

Required*

`**get_available_functions**`

Required*

`**max_tokens**`

`FunctionCallingStepwisePlannerOptions(*, excluded_plugins: set[str] =
``None``, `

`excluded_functions: set[str] = ``None``, get_available_functions: `

`Callable[[PlannerOptions, str | ``None``], list[KernelFunctionMetadata]] | ``None`` `

`= ``None``, max_tokens: int | ``None`` = ``None``, max_tokens_ratio: float | ``None`` = 0.1, `

`max_completion_tokens: int | ``None`` = ``None``, max_prompt_tokens: int | ``None`` = `

`None``, get_initial_plan: Callable[[], str] | ``None`` = ``None``, get_step_prompt: `

`Callable[[], str] | ``None`` = ``None``, max_iterations: int | ``None`` = 15, `

`min_iteration_time_ms: int | ``None`` = 100, execution_settings: `

`OpenAIPromptExecutionSettings | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**max_tokens_ratio**`

Default value: 0.1

`**max_completion_tokens**`

Required*

`**max_prompt_tokens**`

Required*

`**get_initial_plan**`

Required*

`**get_step_prompt**`

Required*

`**max_iterations**`

Default value: 15

`**min_iteration_time_ms**`

Default value: 100

`**execution_settings**`

Required*

calculate_token_limits

Calculate the token limits based on the max_tokens and

max_tokens_ratio.

**calculate_token_limits**

Calculate the token limits based on the max_tokens and max_tokens_ratio.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`calculate_token_limits(data: Any) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

`**data**`

Required*

**excluded_functions**

Python

**excluded_plugins**

Python

**execution_settings**

Python

**get_available_functions**

Python

**get_initial_plan**

Python

**Attributes**

`excluded_functions: set[str]`

`excluded_plugins: set[str]`

`execution_settings: OpenAIPromptExecutionSettings | ``None`

`get_available_functions: Callable[[``'PlannerOptions'``, str | ``None``], `

`list[KernelFunctionMetadata]] | ``None`

`get_initial_plan: Callable[[], str] | ``None`



**get_step_prompt**

Python

**max_completion_tokens**

Python

**max_iterations**

Python

**max_prompt_tokens**

Python

**max_tokens**

Python

**max_tokens_ratio**

Python

**min_iteration_time_ms**

`get_step_prompt: Callable[[], str] | ``None`

`max_completion_tokens: int | ``None`

`max_iterations: int | ``None`

`max_prompt_tokens: int | ``None`

`max_tokens: int | ``None`

`max_tokens_ratio: float | ``None`



Python

`min_iteration_time_ms: int | ``None`



**FunctionCallingStepwisePlannerResult**

**Class**

Reference

The result of the function calling stepwise planner.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**final_answer**`

Required*

`**chat_history**`

Required*

`**iterations**`

Required*

**chat_history**

Python

`FunctionCallingStepwisePlannerResult(*, final_answer: str = ``''``, `

`chat_history: ChatHistory | ``None`` = ``None``, iterations: int = 0)`

ﾉ

**Expand table**

**Attributes**



**final_answer**

Python

**iterations**

Python

`chat_history: ChatHistory | ``None`

`final_answer: str`

`iterations: int`



**sequential_planner Package**

Reference

**Modules**

sequential_planner

sequential_planner_config

sequential_planner_extensions

sequential_planner_parser

**Classes**

SequentialPlanner

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.

ﾉ

**Expand table**

ﾉ

**Expand table**



**sequential_planner Module**

Reference

**Classes**

SequentialPlanner

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.

**read_file**

Reads the content of a file.

Python

**Parameters**

**Name**

**Description**

`**file_path**`

Required*

ﾉ

**Expand table**

**Functions**

`read_file(file_path: str) -> str`

ﾉ

**Expand table**



**SequentialPlanner Class**

Reference

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.planners.sequential_planner.sequential_planner.Kernel>

The kernel instance to use for planning

`**service_id**`

Required*

str

The service id to use to get the AI service

`**config**`

<xref:

<xref:semantic_kernel.planners.sequential_planner.sequential_planner.SequentialPlannerConfig,

optional>>

The configuration to use for planning. Defaults to None.

Default value: None

`**prompt**`

<xref:<xref:semantic_kernel.planners.sequential_planner.sequential_planner.str,
optional>>

The prompt to use for planning. Defaults to None.

Default value: None

create_plan

Create a plan for the specified goal.

**create_plan**

`SequentialPlanner(kernel: Kernel, service_id: str, config: `

`SequentialPlannerConfig = ``None``, prompt: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Create a plan for the specified goal.

Python

**Parameters**

**Name**

**Description**

`**goal**`

Required*

**RESTRICTED_PLUGIN_NAME**

Python

**config**

Python

`async`` create_plan(goal: str) -> Plan`

ﾉ

**Expand table**

**Attributes**

`RESTRICTED_PLUGIN_NAME = ``'SequentialPlanner_Excluded'`

`config: SequentialPlannerConfig`



**sequential_planner_config Module**

Reference

**Classes**

SequentialPlannerConfig

Configuration for the SequentialPlanner.

Initializes a new instance of the SequentialPlannerConfig class.

ﾉ

**Expand table**



**SequentialPlannerConfig Class**

Reference

Configuration for the SequentialPlanner.

Initializes a new instance of the SequentialPlannerConfig class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**relevancy_threshold**`

Default value: None

`**max_relevant_functions**`

Default value: 100

`**excluded_plugins**`

Default value: None

`**excluded_functions**`

Default value: None

`**included_functions**`

Default value: None

`**max_tokens**`

Default value: 1024

`**allow_missing_functions**`

Default value: False

`**get_available_functions**`

Default value: None

`**get_plugin_function**`

Default value: None

`SequentialPlannerConfig(relevancy_threshold: float | ``None`` = ``None``, `

`max_relevant_functions: int = 100, excluded_plugins: list[str] | ``None`` = `

`None``, excluded_functions: list[str] | ``None`` = ``None``, included_functions: `

`list[str] | ``None`` = ``None``, max_tokens: int = 1024, allow_missing_functions: `

`bool = ``False``, get_available_functions: Callable | ``None`` = ``None``, `

`get_plugin_function: Callable | ``None`` = ``None``)`

ﾉ

**Expand table**



**sequential_planner_extensions Module**

Reference

**Classes**

SequentialPlannerFunctionExtension

Function extension for the sequential planner.

SequentialPlannerKernelExtension

Kernel extension for the sequential planner.

ﾉ

**Expand table**



**SequentialPlannerFunctionExtension**

**Class**

Reference

Function extension for the sequential planner.

**Constructor**

Python

to_embedding_string

Convert the function to an embedding string.

to_manual_string

Convert the function to a manual string.

**to_embedding_string**

Convert the function to an embedding string.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

`SequentialPlannerFunctionExtension()`

**Methods**

ﾉ

**Expand table**

`static to_embedding_string(function: KernelFunctionMetadata)`

ﾉ

**Expand table**



**to_manual_string**

Convert the function to a manual string.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

`static to_manual_string(function: KernelFunctionMetadata)`

ﾉ

**Expand table**



**SequentialPlannerKernelExtension Class**

Reference

Kernel extension for the sequential planner.

**Constructor**

Python

get_available_functions

Get the available functions based on the semantic query.

get_functions_manual

Get the functions manual.

get_relevant_functions

Get relevant functions from the memories.

**get_available_functions**

Get the available functions based on the semantic query.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`SequentialPlannerKernelExtension()`

**Methods**

ﾉ

**Expand table**

`async`` static get_available_functions(kernel: Kernel, arguments: `

`KernelArguments, config: SequentialPlannerConfig, semantic_query: str | `

`None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

`**config**`

Required*

`**semantic_query**`

Required*

Default value: None

**get_functions_manual**

Get the functions manual.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

`**semantic_query**`

Required*

Default value: None

`**config**`

Required*

Default value: None

**get_relevant_functions**

Get relevant functions from the memories.

Python

`async`` static get_functions_manual(kernel: Kernel, arguments: `

`KernelArguments, semantic_query: str | ``None`` = ``None``, config: `

`SequentialPlannerConfig = ``None``) -> str`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**available_functions**`

Required*

`**memories**`

Required*

Default value: None

**PLANNER_MEMORY_COLLECTION_NAME**

Python

**PLAN_KERNEL_FUNCTIONS_ARE_REMEMBERED**

Python

`async`` static get_relevant_functions(kernel: Kernel, available_functions: `

`list[KernelFunctionMetadata], memories: list[MemoryQueryResult] | ``None`` = `

`None``) -> list[KernelFunctionMetadata]`

ﾉ

**Expand table**

**Attributes**

`PLANNER_MEMORY_COLLECTION_NAME = ``' Planning.KernelFunctionManual'`

`PLAN_KERNEL_FUNCTIONS_ARE_REMEMBERED = `

`'Planning.KernelFunctionsAreRemembered'`



**sequential_planner_parser Module**

Reference

**Classes**

SequentialPlanParser

Parser for Sequential planners.

ﾉ

**Expand table**



**SequentialPlanParser Class**

Reference

Parser for Sequential planners.

**Constructor**

Python

get_plugin_function_names

Get the plugin and function names from the plugin function name.

to_plan_from_xml

Convert an xml string to a plan.

**get_plugin_function_names**

Get the plugin and function names from the plugin function name.

Python

**Parameters**

**Name**

**Description**

`**plugin_function_name**`

Required*

**to_plan_from_xml**

`SequentialPlanParser()`

**Methods**

ﾉ

**Expand table**

`static get_plugin_function_names(plugin_function_name: str) -> tuple[str, `

`str]`

ﾉ

**Expand table**



Convert an xml string to a plan.

Python

**Parameters**

**Name**

**Description**

`**xml_string**`

Required*

`**goal**`

Required*

`**kernel**`

Required*

`**get_plugin_function**`

Required*

Default value: None

`**allow_missing_functions**`

Required*

Default value: False

`static to_plan_from_xml(xml_string: str, goal: str, kernel: Kernel, `

`get_plugin_function: Callable[[str, str], KernelFunction | ``None``] | ``None`` = `

`None``, allow_missing_functions: bool = ``False``)`

ﾉ

**Expand table**



**SequentialPlanner Class**

Reference

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.planners.sequential_planner.Kernel>

The kernel instance to use for planning

`**service_id**`

Required*

str

The service id to use to get the AI service

`**config**`

<xref:<xref:semantic_kernel.planners.sequential_planner.SequentialPlannerConfig,

optional>>

The configuration to use for planning. Defaults to None.

Default value: None

`**prompt**`

<xref:<xref:semantic_kernel.planners.sequential_planner.str, optional>>

The prompt to use for planning. Defaults to None.

Default value: None

create_plan

Create a plan for the specified goal.

`SequentialPlanner(kernel: Kernel, service_id: str, config: `

`SequentialPlannerConfig = ``None``, prompt: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**create_plan**

Create a plan for the specified goal.

Python

**Parameters**

**Name**

**Description**

`**goal**`

Required*

**RESTRICTED_PLUGIN_NAME**

Python

**config**

Python

`async`` create_plan(goal: str) -> Plan`

ﾉ

**Expand table**

**Attributes**

`RESTRICTED_PLUGIN_NAME = ``'SequentialPlanner_Excluded'`

`config: SequentialPlannerConfig`



**plan Module**

Reference

**Classes**

Plan

A plan for the kernel.

Initializes a new instance of the Plan class.

ﾉ

**Expand table**



**Plan Class**

Reference

A plan for the kernel.

Initializes a new instance of the Plan class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Default value: None

`**plugin_name**`

Default value: None

`**description**`

Default value: None

`**next_step_index**`

Default value: None

`**state**`

Default value: None

`**parameters**`

Default value: None

`**outputs**`

Default value: None

`**steps**`

Default value: None

`**function**`

Default value: None

`Plan(name: str | ``None`` = ``None``, plugin_name: str | ``None`` = ``None``, description: `

`str | ``None`` = ``None``, next_step_index: int | ``None`` = ``None``, state: `

`KernelArguments | ``None`` = ``None``, parameters: KernelArguments | ``None`` = ``None``, `

`outputs: list[str] | ``None`` = ``None``, steps: list[Plan] | ``None`` = ``None``, function: `

`KernelFunction | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**



add_steps

Add steps to the plan.

add_variables_to_state

Add variables to the state.

expand_from_arguments

Expand variables in the input from the step using the

arguments.

from_function

Create a plan from a function.

from_goal

Create a plan from a goal.

get_next_step_arguments

Get the arguments for the next step.

invoke

Invoke the plan asynchronously.

invoke_next_step

Invoke the next step in the plan.

run_next_step

Run the next step in the plan.

set_ai_configuration

Set the AI configuration for the plan.

set_available_functions

Set the available functions for the plan.

set_function

Set the function for the plan.

update_arguments_with_outputs

Update the arguments with the outputs from the current step.

**add_steps**

Add steps to the plan.

Python

**Parameters**

**Name**

**Description**

`**steps**`

Required*

**add_variables_to_state**

ﾉ

**Expand table**

`add_steps(steps: list[Plan] | list[KernelFunction]) -> ``None`

ﾉ

**Expand table**



Add variables to the state.

Python

**Parameters**

**Name**

**Description**

`**state**`

Required*

`**variables**`

Required*

**expand_from_arguments**

Expand variables in the input from the step using the arguments.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**input_from_step**`

Required*

**from_function**

Create a plan from a function.

`add_variables_to_state(state: KernelArguments, variables: `

`KernelArguments) -> ``None`

ﾉ

**Expand table**

`expand_from_arguments(arguments: KernelArguments, input_from_step: Any) -`

`> str`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

**from_goal**

Create a plan from a goal.

Python

**Parameters**

**Name**

**Description**

`**goal**`

Required*

**get_next_step_arguments**

Get the arguments for the next step.

Python

**Parameters**

`from_function(function: KernelFunction) -> Plan`

ﾉ

**Expand table**

`from_goal(goal: str) -> Plan`

ﾉ

**Expand table**

`get_next_step_arguments(arguments: KernelArguments, step: Plan) -> `

`KernelArguments`



**Name**

**Description**

`**arguments**`

Required*

`**step**`

Required*

**invoke**

Invoke the plan asynchronously.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.planners.plan.Kernel>

The kernel to use for invocation.

`**arguments**`

Required*

<xref:<xref:semantic_kernel.planners.plan.KernelArguments, optional>>

The context to use. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

FunctionResult

The result of the function.

**invoke_next_step**

Invoke the next step in the plan.

ﾉ

**Expand table**

`async`` invoke(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`FunctionResult`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

**run_next_step**

Run the next step in the plan.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

**set_ai_configuration**

Set the AI configuration for the plan.

Python

`async`` invoke_next_step(kernel: Kernel, arguments: KernelArguments) -> `

`FunctionResult | ``None`

ﾉ

**Expand table**

`async`` run_next_step(kernel: Kernel, arguments: KernelArguments) -> `

`FunctionResult | ``None`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**settings**`

Required*

**set_available_functions**

Set the available functions for the plan.

Python

**Parameters**

**Name**

**Description**

`**plan**`

Required*

`**kernel**`

Required*

`**arguments**`

Required*

**set_function**

Set the function for the plan.

Python

`set_ai_configuration(settings: PromptExecutionSettings) -> ``None`

ﾉ

**Expand table**

`set_available_functions(plan: Plan, kernel: Kernel, arguments: `

`KernelArguments) -> Plan`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**function**`

Required*

**update_arguments_with_outputs**

Update the arguments with the outputs from the current step.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

**description**

Get the description for the plan.

**function**

Get the function for the plan.

`set_function(function: KernelFunction) -> ``None`

ﾉ

**Expand table**

`update_arguments_with_outputs(arguments: KernelArguments) -> `

`KernelArguments`

ﾉ

**Expand table**

**Attributes**



**has_next_step**

Check if the plan has a next step.

**is_native**

Check if the plan is native code.

**is_prompt**

Check if the plan is a prompt.

**metadata**

Get the metadata for the plan.

**name**

Get the name for the plan.

**next_step_index**

Get the next step index.

**parameters**

Get the parameters for the plan.

**plugin_name**

Get the plugin name for the plan.

**prompt_execution_settings**

Get the AI configuration for the plan.

**state**

Get the state for the plan.



**steps**

Get the steps for the plan.

**DEFAULT_RESULT_KEY**

Python

`DEFAULT_RESULT_KEY: ClassVar[str] = ``'PLAN.RESULT'`



**planner_extensions Module**

Reference

**Classes**

PlannerFunctionExtension

Function extension for the planner.

PlannerKernelExtension

Kernel extension for the planner.

ﾉ

**Expand table**



**PlannerFunctionExtension Class**

Reference

Function extension for the planner.

**Constructor**

Python

to_embedding_string

Convert the function to a string that can be used as an embedding.

to_manual_string

Convert the function to a string that can be used in the manual.

**to_embedding_string**

Convert the function to a string that can be used as an embedding.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

**to_manual_string**

`PlannerFunctionExtension()`

**Methods**

ﾉ

**Expand table**

`static to_embedding_string(function: KernelFunctionMetadata)`

ﾉ

**Expand table**



Convert the function to a string that can be used in the manual.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

`static to_manual_string(function: KernelFunctionMetadata)`

ﾉ

**Expand table**



**PlannerKernelExtension Class**

Reference

Kernel extension for the planner.

**Constructor**

Python

get_available_functions

Get the available functions for the kernel.

get_functions_manual

Get the string of the function.

**get_available_functions**

Get the available functions for the kernel.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

`PlannerKernelExtension()`

**Methods**

ﾉ

**Expand table**

`async`` static get_available_functions(kernel: Kernel, arguments: `

`KernelArguments, options: PlannerOptions)`

ﾉ

**Expand table**



**Name**

**Description**

`**options**`

Required*

**get_functions_manual**

Get the string of the function.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

`**options**`

Required*

Default value: None

**PLANNER_MEMORY_COLLECTION_NAME**

Python

**PLAN_KERNEL_FUNCTIONS_ARE_REMEMBERED**

Python

`async`` static get_functions_manual(kernel: Kernel, arguments: `

`KernelArguments, options: PlannerOptions = ``None``) -> str`

ﾉ

**Expand table**

**Attributes**

`PLANNER_MEMORY_COLLECTION_NAME = ``' Planning.KernelFunctionManual'`



`PLAN_KERNEL_FUNCTIONS_ARE_REMEMBERED = `

`'Planning.KernelFunctionsAreRemembered'`



**planner_options Module**

Reference

**Classes**

PlannerOptions

The default planner options that planners inherit from.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PlannerOptions Class**

Reference

The default planner options that planners inherit from.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**excluded_plugins**`

Required*

`**excluded_functions**`

Required*

`**get_available_functions**`

Required*

**excluded_functions**

Python

`PlannerOptions(*, excluded_plugins: set[str] = ``None``, excluded_functions:
`

`set[str] = ``None``, get_available_functions: Callable[[PlannerOptions, str | `

`None``], list[KernelFunctionMetadata]] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**excluded_plugins**

Python

**get_available_functions**

Python

`excluded_functions: set[str]`

`excluded_plugins: set[str]`

`get_available_functions: Callable[[PlannerOptions, str | ``None``], `

`list[KernelFunctionMetadata]] | ``None`



**FunctionCallingStepwisePlanner Class**

Reference

A Function Calling Stepwise Planner.

Initialize a new instance of the FunctionCallingStepwisePlanner.

The FunctionCallingStepwisePlanner is a planner based on top of an OpenAI Chat

Completion service (whether it be AzureOpenAI or OpenAI), so that we can use
tools.

If the options are configured to use callbacks to get the initial plan and the
step prompt,

the planner will use those provided callbacks to get that information.
Otherwise, it will

read from the default yaml plan file and the step prompt file.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**service_id**`

Required*

str

The service id

`**options**`

<xref:Optional>[<xref:FunctionCallingStepwisePlannerOptions>],<xref: optional>

The options for the function calling stepwise planner. Defaults to None.

Default value: None

invoke

Execute the function calling stepwise planner.

`FunctionCallingStepwisePlanner(service_id: str, options: `

`FunctionCallingStepwisePlannerOptions | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**invoke**

Execute the function calling stepwise planner.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**question**`

Required*

The input question

`**arguments**`

Required*

(optional) The kernel arguments

Default value: None

`**kwargs**`

Required*

(optional) Additional keyword arguments

**Returns**

**Type**

**Description**

FunctionCallingStepwisePlannerResult

The result of the function calling stepwise planner

**Exceptions**

**Type**

**Description**

PlannerInvalidConfigurationError

If the input question is empty

`async`` invoke(kernel: Kernel, question: str, arguments: KernelArguments | `

`None`` = ``None``, **kwargs: Any) -> FunctionCallingStepwisePlannerResult`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**generate_plan_yaml**

Python

**options**

Python

**service_id**

Python

**step_prompt**

Python

**Attributes**

`generate_plan_yaml: str`

`options: FunctionCallingStepwisePlannerOptions`

`service_id: str`

`step_prompt: str`



**FunctionCallingStepwisePlannerOptions**

**Class**

Reference

The Function Calling Stepwise Planner Options.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**excluded_plugins**`

Required*

`**excluded_functions**`

Required*

`**get_available_functions**`

Required*

`**max_tokens**`

`FunctionCallingStepwisePlannerOptions(*, excluded_plugins: set[str] =
``None``, `

`excluded_functions: set[str] = ``None``, get_available_functions: `

`Callable[[PlannerOptions, str | ``None``], list[KernelFunctionMetadata]] | ``None`` `

`= ``None``, max_tokens: int | ``None`` = ``None``, max_tokens_ratio: float | ``None`` = 0.1, `

`max_completion_tokens: int | ``None`` = ``None``, max_prompt_tokens: int | ``None`` = `

`None``, get_initial_plan: Callable[[], str] | ``None`` = ``None``, get_step_prompt: `

`Callable[[], str] | ``None`` = ``None``, max_iterations: int | ``None`` = 15, `

`min_iteration_time_ms: int | ``None`` = 100, execution_settings: `

`OpenAIPromptExecutionSettings | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**max_tokens_ratio**`

Default value: 0.1

`**max_completion_tokens**`

Required*

`**max_prompt_tokens**`

Required*

`**get_initial_plan**`

Required*

`**get_step_prompt**`

Required*

`**max_iterations**`

Default value: 15

`**min_iteration_time_ms**`

Default value: 100

`**execution_settings**`

Required*

calculate_token_limits

Calculate the token limits based on the max_tokens and

max_tokens_ratio.

**calculate_token_limits**

Calculate the token limits based on the max_tokens and max_tokens_ratio.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`calculate_token_limits(data: Any) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

`**data**`

Required*

**execution_settings**

Python

**get_initial_plan**

Python

**get_step_prompt**

Python

**max_completion_tokens**

Python

**max_iterations**

Python

**Attributes**

`execution_settings: OpenAIPromptExecutionSettings | ``None`

`get_initial_plan: Callable[[], str] | ``None`

`get_step_prompt: Callable[[], str] | ``None`

`max_completion_tokens: int | ``None`

`max_iterations: int | ``None`



**max_prompt_tokens**

Python

**max_tokens**

Python

**max_tokens_ratio**

Python

**min_iteration_time_ms**

Python

`max_prompt_tokens: int | ``None`

`max_tokens: int | ``None`

`max_tokens_ratio: float | ``None`

`min_iteration_time_ms: int | ``None`



**FunctionCallingStepwisePlannerResult**

**Class**

Reference

The result of the function calling stepwise planner.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**final_answer**`

Required*

`**chat_history**`

Required*

`**iterations**`

Required*

**chat_history**

Python

`FunctionCallingStepwisePlannerResult(*, final_answer: str = ``''``, `

`chat_history: ChatHistory | ``None`` = ``None``, iterations: int = 0)`

ﾉ

**Expand table**

**Attributes**



**final_answer**

Python

**iterations**

Python

`chat_history: ChatHistory | ``None`

`final_answer: str`

`iterations: int`



**Plan Class**

Reference

A plan for the kernel.

Initializes a new instance of the Plan class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Default value: None

`**plugin_name**`

Default value: None

`**description**`

Default value: None

`**next_step_index**`

Default value: None

`**state**`

Default value: None

`**parameters**`

Default value: None

`**outputs**`

Default value: None

`**steps**`

Default value: None

`**function**`

Default value: None

`Plan(name: str | ``None`` = ``None``, plugin_name: str | ``None`` = ``None``, description: `

`str | ``None`` = ``None``, next_step_index: int | ``None`` = ``None``, state: `

`KernelArguments | ``None`` = ``None``, parameters: KernelArguments | ``None`` = ``None``, `

`outputs: list[str] | ``None`` = ``None``, steps: list[Plan] | ``None`` = ``None``, function: `

`KernelFunction | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**



add_steps

Add steps to the plan.

add_variables_to_state

Add variables to the state.

expand_from_arguments

Expand variables in the input from the step using the

arguments.

from_function

Create a plan from a function.

from_goal

Create a plan from a goal.

get_next_step_arguments

Get the arguments for the next step.

invoke

Invoke the plan asynchronously.

invoke_next_step

Invoke the next step in the plan.

run_next_step

Run the next step in the plan.

set_ai_configuration

Set the AI configuration for the plan.

set_available_functions

Set the available functions for the plan.

set_function

Set the function for the plan.

update_arguments_with_outputs

Update the arguments with the outputs from the current step.

**add_steps**

Add steps to the plan.

Python

**Parameters**

**Name**

**Description**

`**steps**`

Required*

**add_variables_to_state**

ﾉ

**Expand table**

`add_steps(steps: list[Plan] | list[KernelFunction]) -> ``None`

ﾉ

**Expand table**



Add variables to the state.

Python

**Parameters**

**Name**

**Description**

`**state**`

Required*

`**variables**`

Required*

**expand_from_arguments**

Expand variables in the input from the step using the arguments.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**input_from_step**`

Required*

**from_function**

Create a plan from a function.

`add_variables_to_state(state: KernelArguments, variables: `

`KernelArguments) -> ``None`

ﾉ

**Expand table**

`expand_from_arguments(arguments: KernelArguments, input_from_step: Any) -`

`> str`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

**from_goal**

Create a plan from a goal.

Python

**Parameters**

**Name**

**Description**

`**goal**`

Required*

**get_next_step_arguments**

Get the arguments for the next step.

Python

**Parameters**

`from_function(function: KernelFunction) -> Plan`

ﾉ

**Expand table**

`from_goal(goal: str) -> Plan`

ﾉ

**Expand table**

`get_next_step_arguments(arguments: KernelArguments, step: Plan) -> `

`KernelArguments`



**Name**

**Description**

`**arguments**`

Required*

`**step**`

Required*

**invoke**

Invoke the plan asynchronously.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.planners.Kernel>

The kernel to use for invocation.

`**arguments**`

Required*

<xref:<xref:semantic_kernel.planners.KernelArguments, optional>>

The context to use. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

FunctionResult

The result of the function.

**invoke_next_step**

Invoke the next step in the plan.

ﾉ

**Expand table**

`async`` invoke(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`FunctionResult`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

**run_next_step**

Run the next step in the plan.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

**set_ai_configuration**

Set the AI configuration for the plan.

Python

`async`` invoke_next_step(kernel: Kernel, arguments: KernelArguments) -> `

`FunctionResult | ``None`

ﾉ

**Expand table**

`async`` run_next_step(kernel: Kernel, arguments: KernelArguments) -> `

`FunctionResult | ``None`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**settings**`

Required*

**set_available_functions**

Set the available functions for the plan.

Python

**Parameters**

**Name**

**Description**

`**plan**`

Required*

`**kernel**`

Required*

`**arguments**`

Required*

**set_function**

Set the function for the plan.

Python

`set_ai_configuration(settings: PromptExecutionSettings) -> ``None`

ﾉ

**Expand table**

`set_available_functions(plan: Plan, kernel: Kernel, arguments: `

`KernelArguments) -> Plan`

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**function**`

Required*

**update_arguments_with_outputs**

Update the arguments with the outputs from the current step.

Python

**Parameters**

**Name**

**Description**

`**arguments**`

Required*

**description**

Get the description for the plan.

**function**

Get the function for the plan.

`set_function(function: KernelFunction) -> ``None`

ﾉ

**Expand table**

`update_arguments_with_outputs(arguments: KernelArguments) -> `

`KernelArguments`

ﾉ

**Expand table**

**Attributes**



**has_next_step**

Check if the plan has a next step.

**is_native**

Check if the plan is native code.

**is_prompt**

Check if the plan is a prompt.

**metadata**

Get the metadata for the plan.

**name**

Get the name for the plan.

**next_step_index**

Get the next step index.

**parameters**

Get the parameters for the plan.

**plugin_name**

Get the plugin name for the plan.

**prompt_execution_settings**

Get the AI configuration for the plan.

**state**

Get the state for the plan.



**steps**

Get the steps for the plan.

**DEFAULT_RESULT_KEY**

Python

`DEFAULT_RESULT_KEY: ClassVar[str] = ``'PLAN.RESULT'`



**PlannerOptions Class**

Reference

The default planner options that planners inherit from.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**excluded_plugins**`

Required*

`**excluded_functions**`

Required*

`**get_available_functions**`

Required*

**excluded_functions**

Python

`PlannerOptions(*, excluded_plugins: set[str] = ``None``, excluded_functions:
`

`set[str] = ``None``, get_available_functions: Callable[[PlannerOptions, str | `

`None``], list[KernelFunctionMetadata]] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**excluded_plugins**

Python

**get_available_functions**

Python

`excluded_functions: set[str]`

`excluded_plugins: set[str]`

`get_available_functions: Callable[[PlannerOptions, str | ``None``], `

`list[KernelFunctionMetadata]] | ``None`



**SequentialPlanner Class**

Reference

Sequential planner class.

Initializes a new instance of the SequentialPlanner class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

<xref:semantic_kernel.planners.Kernel>

The kernel instance to use for planning

`**service_id**`

Required*

str

The service id to use to get the AI service

`**config**`

<xref:<xref:semantic_kernel.planners.SequentialPlannerConfig, optional>>

The configuration to use for planning. Defaults to None.

Default value: None

`**prompt**`

<xref:<xref:semantic_kernel.planners.str, optional>>

The prompt to use for planning. Defaults to None.

Default value: None

create_plan

Create a plan for the specified goal.

**create_plan**

`SequentialPlanner(kernel: Kernel, service_id: str, config: `

`SequentialPlannerConfig = ``None``, prompt: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Create a plan for the specified goal.

Python

**Parameters**

**Name**

**Description**

`**goal**`

Required*

**RESTRICTED_PLUGIN_NAME**

Python

**config**

Python

`async`` create_plan(goal: str) -> Plan`

ﾉ

**Expand table**

**Attributes**

`RESTRICTED_PLUGIN_NAME = ``'SequentialPlanner_Excluded'`

`config: SequentialPlannerConfig`



**processes Package**

Reference

**Packages**

kernel_process

local_runtime

**Modules**

const

process_builder

process_edge_builder

process_end_step

process_event

process_function_target_builder

process_message

process_message_factory

process_step_builder

process_step_edge_builder

process_types

step_utils

**Classes**

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



ProcessBuilder

A builder for a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.



**actors Package**

Reference

**Modules**

actor_state_key

ﾉ

**Expand table**



**actor_state_key Module**

Reference

**Enums**

ActorStateKeys

Keys used to store actor state in Dapr.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**ActorStateKeys Enum**

Reference

Keys used to store actor state in Dapr.

Note: This class is marked as 'experimental' and may change in the future.

EventQueueState

ExternalEventQueueState

MessageQueueState

ProcessInfoState

StepActivatedState

StepIncomingMessagesState

StepInfoState

StepParentProcessId

StepStateJson

StepStateType

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**kernel_process Package**

Reference

**Modules**

kernel_process

kernel_process_edge

kernel_process_event

kernel_process_function_target

kernel_process_message_channel

kernel_process_state

kernel_process_step

kernel_process_step_context

kernel_process_step_info

kernel_process_step_state

**Classes**

KernelProcess

A kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize the kernel process.

KernelProcessEvent

A kernel process event.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**



Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelProcessStep

A KernelProcessStep Base class for process steps.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelProcessStepContext

The context of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize the step context.

KernelProcessStepState

The state of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

KernelProcessEventVisibility

Visibility of a kernel process event.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**



**kernel_process Module**

Reference

**Classes**

KernelProcess

A kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the kernel process.

ﾉ

**Expand table**



**KernelProcess Class**

Reference

A kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the kernel process.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**state**`

Required*

The state of the process.

`**steps**`

Required*

The steps of the process.

`**edges**`

The edges of the process. Defaults to None.

Default value: None

`**factories**`

The factories of the process. This allows for the creation of steps that
require

complex dependencies that cannot be JSON serialized or deserialized.

Default value: None

**factories**

Python

`KernelProcess(state: KernelProcessState, steps: list[KernelProcessStepInfo],
`

`edges: dict[str, list[KernelProcessEdge]] | ``None`` = ``None``, factories: `

`dict[str, Callable] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**inner_step_type**

Python

**is_experimental**

Python

**output_edges**

Python

**stage_status**

Python

**state**

Python

**steps**

Python

`factories: dict[str, Callable]`

`inner_step_type: type`

`is_experimental = ``True`

`output_edges: dict[str, list[KernelProcessEdge]]`

`stage_status = ``'experimental'`

`state: KernelProcessStepState`

`steps: list[KernelProcessStepInfo]`







**kernel_process_edge Module**

Reference

**Classes**

KernelProcessEdge

Represents an edge between steps.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessEdge Class**

Reference

Represents an edge between steps.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**source_step_id**`

Required*

`**output_target**`

Required*

**is_experimental**

Python

`KernelProcessEdge(*, source_step_id: str, output_target: `

`KernelProcessFunctionTarget)`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`



**output_target**

Python

**source_step_id**

Python

**stage_status**

Python

`output_target: KernelProcessFunctionTarget`

`source_step_id: str`

`stage_status = ``'experimental'`



**kernel_process_event Module**

Reference

**Classes**

KernelProcessEvent

A kernel process event.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Enums**

KernelProcessEventVisibility

Visibility of a kernel process event.

Note: This class is marked as 'experimental' and may change in the

future.

ﾉ

**Expand table**

ﾉ

**Expand table**



**KernelProcessEvent Class**

Reference

A kernel process event.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**id**`

Required*

`**data**`

Required*

`**visibility**`

Default value: KernelProcessEventVisibility.Internal

**data**

Python

`KernelProcessEvent(*, id: str, data: Any | ``None`` = ``None``, visibility: `

`KernelProcessEventVisibility = KernelProcessEventVisibility.Internal)`

ﾉ

**Expand table**

**Attributes**



**id**

Python

**is_experimental**

Python

**stage_status**

Python

**visibility**

Python

`data: Any | ``None`

`id: str`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`visibility: KernelProcessEventVisibility`



**KernelProcessEventVisibility Enum**

Reference

Visibility of a kernel process event.

Note: This class is marked as 'experimental' and may change in the future.

Internal

Public

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**kernel_process_function_target Module**

Reference

**Classes**

KernelProcessFunctionTarget

The target of a function call in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessFunctionTarget Class**

Reference

The target of a function call in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**step_id**`

Required*

`**function_name**`

Required*

`**parameter_name**`

Required*

`**target_event_id**`

Required*

**function_name**

`KernelProcessFunctionTarget(*, step_id: str, function_name: str, `

`parameter_name: str | ``None`` = ``None``, target_event_id: str | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



Python

**is_experimental**

Python

**parameter_name**

Python

**stage_status**

Python

**step_id**

Python

**target_event_id**

Python

`function_name: str`

`is_experimental = ``True`

`parameter_name: str | ``None`

`stage_status = ``'experimental'`

`step_id: str`

`target_event_id: str | ``None`



**kernel_process_message_channel**

**Module**

Reference

**Classes**

KernelProcessMessageChannel

Abstract base class for emitting events from a step.

Note: This class is marked as 'experimental' and may change in

the future.

ﾉ

**Expand table**



**KernelProcessMessageChannel Class**

Reference

Abstract base class for emitting events from a step.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

emit_event

Emits the specified event from the step.

**emit_event**

Emits the specified event from the step.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

`KernelProcessMessageChannel()`

**Methods**

ﾉ

**Expand table**

`abstract ``async`` emit_event(process_event: KernelProcessEvent) -> ``None`

ﾉ

**Expand table**

**Attributes**



**is_experimental**

Python

**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**kernel_process_state Module**

Reference

**Classes**

KernelProcessState

The state of a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessState Class**

Reference

The state of a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: KernelProcessState

`**name**`

Required*

`**id**`

Required*

`**state**`

Required*

**is_experimental**

`KernelProcessState(*, type: Literal[``'KernelProcessState'``] = `

`'KernelProcessState'``, name: str, id: str | ``None`` = ``None``, state: TState | ``None`` `

`= ``None``)`

ﾉ

**Expand table**

**Attributes**



Python

**stage_status**

Python

**type**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`

`type: Literal[``'KernelProcessState'``]`



**kernel_process_step Module**

Reference

**Classes**

KernelProcessStep

A KernelProcessStep Base class for process steps.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessStep Class**

Reference

A KernelProcessStep Base class for process steps.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**state**`

Required*

activate

Activates the step and sets the state.

**activate**

Activates the step and sets the state.

Python

`KernelProcessStep(*, state: TState | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**state**`

Required*

**is_experimental**

Python

**stage_status**

Python

**state**

Python

`async`` activate(state: KernelProcessStepState[TState])`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`

`state: TState | ``None`



**kernel_process_step_context Module**

Reference

**Classes**

KernelProcessStepContext

The context of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Initialize the step context.

ﾉ

**Expand table**



**KernelProcessStepContext Class**

Reference

The context of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the step context.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**channel**`

Required*

emit_event

Emit an event from the current step.

It is possible to either specify a _KernelProcessEvent_ object or the ID of
the event along with the

_data_ and optional _visibility_ keyword arguments.

**emit_event**

Emit an event from the current step.

It is possible to either specify a _KernelProcessEvent_ object or the ID of
the event along with the

_data_ and optional _visibility_ keyword arguments.

Python

`KernelProcessStepContext(channel: KernelProcessMessageChannel)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`async`` emit_event(process_event: KernelProcessEvent | str | Enum | ``None``, **kwargs) -`

`> ``None`



**Parameters**

**Name**

**Description**

`**process_event**`

Required*

<xref:

<xref:semantic_kernel.processes.kernel_process.kernel_process_step_context.KernelProcessEvent

| str>>

The event to emit.

`****kwargs**`

Required*

Additional keyword arguments to pass to the event.

**is_experimental**

Python

**stage_status**

Python

**step_message_channel**

Python

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`

`step_message_channel: KernelProcessMessageChannel`



**kernel_process_step_info Module**

Reference

**Classes**

KernelProcessStepInfo

Information about a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessStepInfo Class**

Reference

Information about a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**inner_step_type**`

Required*

`**state**`

Required*

`**output_edges**`

Required*

**edges**

The edges of the step.

`KernelProcessStepInfo(*, inner_step_type: type, state: `

`KernelProcessStepState, output_edges: dict[str, list[KernelProcessEdge]])`

ﾉ

**Expand table**

**Attributes**



**inner_step_type**

Python

**is_experimental**

Python

**output_edges**

Python

**stage_status**

Python

**state**

Python

`inner_step_type: type`

`is_experimental = ``True`

`output_edges: dict[str, list[KernelProcessEdge]]`

`stage_status = ``'experimental'`

`state: KernelProcessStepState`



**kernel_process_step_state Module**

Reference

**Classes**

KernelProcessStepState

The state of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the

future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelProcessStepState Class**

Reference

The state of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: KernelProcessStepState

`**name**`

Required*

`**id**`

Required*

`**state**`

Required*

**id**

`KernelProcessStepState(*, type: Literal[``'KernelProcessStepState'``] = `

`'KernelProcessStepState'``, name: str, id: str | ``None`` = ``None``, state: TState | `

`None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



Python

**is_experimental**

Python

**name**

Python

**stage_status**

Python

**state**

Python

**type**

Python

`id: str | ``None`

`is_experimental = ``True`

`name: str`

`stage_status = ``'experimental'`

`state: TState | ``None`

`type: Literal[``'KernelProcessStepState'``]`



**KernelProcess Class**

Reference

A kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the kernel process.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**state**`

Required*

The state of the process.

`**steps**`

Required*

The steps of the process.

`**edges**`

The edges of the process. Defaults to None.

Default value: None

`**factories**`

The factories of the process. This allows for the creation of steps that
require

complex dependencies that cannot be JSON serialized or deserialized.

Default value: None

**factories**

Python

`KernelProcess(state: KernelProcessState, steps: list[KernelProcessStepInfo],
`

`edges: dict[str, list[KernelProcessEdge]] | ``None`` = ``None``, factories: `

`dict[str, Callable] | ``None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



**is_experimental**

Python

**stage_status**

Python

**steps**

Python

`factories: dict[str, Callable]`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`steps: list[KernelProcessStepInfo]`



**KernelProcessEvent Class**

Reference

A kernel process event.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**id**`

Required*

`**data**`

Required*

`**visibility**`

Default value: KernelProcessEventVisibility.Internal

**data**

Python

`KernelProcessEvent(*, id: str, data: Any | ``None`` = ``None``, visibility: `

`KernelProcessEventVisibility = KernelProcessEventVisibility.Internal)`

ﾉ

**Expand table**

**Attributes**



**id**

Python

**is_experimental**

Python

**stage_status**

Python

**visibility**

Python

`data: Any | ``None`

`id: str`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`visibility: KernelProcessEventVisibility`



**KernelProcessEventVisibility Enum**

Reference

Visibility of a kernel process event.

Note: This class is marked as 'experimental' and may change in the future.

Internal

Public

is_experimental

stage_status

**Fields**

ﾉ

**Expand table**



**KernelProcessStep Class**

Reference

A KernelProcessStep Base class for process steps.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**state**`

Required*

activate

Activates the step and sets the state.

**activate**

Activates the step and sets the state.

Python

`KernelProcessStep(*, state: TState | ``None`` = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**state**`

Required*

**is_experimental**

Python

**stage_status**

Python

**state**

Python

`async`` activate(state: KernelProcessStepState[TState])`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`

`state: TState | ``None`



**KernelProcessStepContext Class**

Reference

The context of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the step context.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**channel**`

Required*

emit_event

Emit an event from the current step.

It is possible to either specify a _KernelProcessEvent_ object or the ID of
the event

along with the _data_ and optional _visibility_ keyword arguments.

**emit_event**

Emit an event from the current step.

It is possible to either specify a _KernelProcessEvent_ object or the ID of
the event

along with the _data_ and optional _visibility_ keyword arguments.

`KernelProcessStepContext(channel: KernelProcessMessageChannel)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

<xref:<xref:semantic_kernel.processes.kernel_process.KernelProcessEvent |

str>>

The event to emit.

`****kwargs**`

Required*

Additional keyword arguments to pass to the event.

**is_experimental**

Python

**stage_status**

Python

**step_message_channel**

Python

`async`` emit_event(process_event: KernelProcessEvent | str | Enum | ``None``, `

`**kwargs) -> ``None`

ﾉ

**Expand table**

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`

`step_message_channel: KernelProcessMessageChannel`



**KernelProcessStepState Class**

Reference

The state of a step in a kernel process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**type**`

Default value: KernelProcessStepState

`**name**`

Required*

`**id**`

Required*

`**state**`

Required*

**id**

`KernelProcessStepState(*, type: Literal[``'KernelProcessStepState'``] = `

`'KernelProcessStepState'``, name: str, id: str | ``None`` = ``None``, state: TState | `

`None`` = ``None``)`

ﾉ

**Expand table**

**Attributes**



Python

**is_experimental**

Python

**name**

Python

**stage_status**

Python

**state**

Python

**type**

Python

`id: str | ``None`

`is_experimental = ``True`

`name: str`

`stage_status = ``'experimental'`

`state: TState | ``None`

`type: Literal[``'KernelProcessStepState'``]`



**local_runtime Package**

Reference

**Modules**

local_event

local_kernel_process

local_kernel_process_context

local_message

local_message_factory

local_process

local_step

ﾉ

**Expand table**



**local_event Module**

Reference

**Classes**

LocalEvent

An event that is local to a namespace.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**LocalEvent Class**

Reference

An event that is local to a namespace.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**namespace**`

Required*

`**inner_event**`

Required*

from_kernel_process_event

Create a local event from a kernel process event.

**from_kernel_process_event**

`LocalEvent(*, namespace: str | ``None``, inner_event: KernelProcessEvent)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Create a local event from a kernel process event.

Python

**Parameters**

**Name**

**Description**

`**kernel_process_event**`

Required*

`**namespace**`

Required*

**data**

The data of the event.

**id**

The ID of the event.

**visibility**

The visibility of the event.

**inner_event**

Python

**is_experimental**

`from_kernel_process_event(kernel_process_event: KernelProcessEvent, `

`namespace: str) -> LocalEvent`

ﾉ

**Expand table**

**Attributes**

`inner_event: KernelProcessEvent`



Python

**namespace**

Python

**stage_status**

Python

`is_experimental = ``True`

`namespace: str | ``None`

`stage_status = ``'experimental'`



**local_kernel_process Module**

Reference

**start**

Start the kernel process.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**process**`

Required*

`**kernel**`

Required*

`**initial_event**`

Required*

**Functions**

`async`` start(process: KernelProcess, kernel: Kernel, initial_event: `

`KernelProcessEvent | str | Enum, **kwargs) -> LocalKernelProcessContext`

ﾉ

**Expand table**



**local_kernel_process_context Module**

Reference

**Classes**

LocalKernelProcessContext

A local kernel process context.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes the local kernel process context.

ﾉ

**Expand table**



**LocalKernelProcessContext Class**

Reference

A local kernel process context.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the local kernel process context.

**Constructor**

Python

dispose

Disposes of the resources used by the process.

get_state

Gets the current state of the process.

send_event

Sends an event to the process.

start_with_event

Starts the local process with an initial event.

stop

Stops the local process.

**dispose**

Disposes of the resources used by the process.

Python

**get_state**

Gets the current state of the process.

`LocalKernelProcessContext()`

**Methods**

ﾉ

**Expand table**

`async`` dispose() -> ``None`



Python

**send_event**

Sends an event to the process.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

**start_with_event**

Starts the local process with an initial event.

Python

**Parameters**

**Name**

**Description**

`**initial_event**`

Required*

**stop**

Stops the local process.

`async`` get_state() -> KernelProcess`

`async`` send_event(process_event: KernelProcessEvent) -> ``None`

ﾉ

**Expand table**

`async`` start_with_event(initial_event: KernelProcessEvent) -> ``None`

ﾉ

**Expand table**



Python

**is_experimental**

Python

**local_process**

Python

**stage_status**

Python

`async`` stop() -> ``None`

**Attributes**

`is_experimental = ``True`

`local_process: LocalProcess`

`stage_status = ``'experimental'`



**local_message Module**

Reference

**Classes**

LocalMessage

A message that is local to a namespace.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**LocalMessage Class**

Reference

A message that is local to a namespace.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**source_id**`

Required*

`**destination_id**`

Required*

`**function_name**`

Required*

`**values**`

Required*

`**target_event_id**`

Required*

`**target_event_data**`

`LocalMessage(*, source_id: str, destination_id: str, function_name: str, `

`values: dict[str, Any | ``None``], target_event_id: str | ``None`` = ``None``, `

`target_event_data: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**destination_id**

Python

**function_name**

Python

**is_experimental**

Python

**source_id**

Python

**stage_status**

Python

**target_event_data**

**Attributes**

`destination_id: str`

`function_name: str`

`is_experimental = ``True`

`source_id: str`

`stage_status = ``'experimental'`



Python

**target_event_id**

Python

**values**

Python

`target_event_data: Any | ``None`

`target_event_id: str | ``None`

`values: dict[str, Any | ``None``]`



**local_message_factory Module**

Reference

**Classes**

LocalMessageFactory

Factory class to create LocalMessage instances.

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**LocalMessageFactory Class**

Reference

Factory class to create LocalMessage instances.

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

create_from_edge

Creates a new LocalMessage instance from a KernelProcessEdge and a data

object.

**create_from_edge**

Creates a new LocalMessage instance from a KernelProcessEdge and a data
object.

Python

**Parameters**

**Name**

**Description**

`**edge**`

Required*

`**data**`

Required*

Default value: None

`LocalMessageFactory()`

**Methods**

ﾉ

**Expand table**

`static create_from_edge(edge: KernelProcessEdge, data: Any | ``None`` = ``None``) `

`-> LocalMessage`

ﾉ

**Expand table**



**is_experimental**

Python

**stage_status**

Python

**Attributes**

`is_experimental = ``True`

`stage_status = ``'experimental'`



**local_process Module**

Reference

**Classes**

LocalProcess

A local process that contains a collection of steps.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the local process.

ﾉ

**Expand table**



**LocalProcess Class**

Reference

A local process that contains a collection of steps.

Note: This class is marked as 'experimental' and may change in the future.

Initializes the local process.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**step_info**`

Required*

`**outgoing_event_queue**`

Required*

`**initialize_task**`

Required*

`**event_namespace**`

Required*

`**step_state**`

Required*

`LocalProcess(*, kernel: Kernel, step_info: KernelProcessStepInfo, `

`outgoing_event_queue: Queue[LocalEvent] = ``None``, initialize_task: bool | ``None`` `

`= ``False``, event_namespace: str, step_state: KernelProcessStepState,
inputs: `

`dict[str, dict[str, Any | ``None``]] = ``None``, initial_inputs: dict[str, dict[str, `

`Any | ``None``]] = ``None``, functions: dict[str, KernelFunction] = ``None``, `

`output_edges: dict[str, list[KernelProcessEdge]] = ``None``,
parent_process_id: `

`str | ``None`` = ``None``, init_lock: Lock = ``None``, factories: dict[str, Callable])`

ﾉ

**Expand table**



**Name**

**Description**

`**inputs**`

Required*

`**initial_inputs**`

Required*

`**functions**`

Required*

`**output_edges**`

Required*

`**parent_process_id**`

Required*

`**init_lock**`

Required*

`**factories**`

Required*

dispose

Clean up resources.

enqueue_external_messages

Processes external events that have been sent to the process.

enqueue_step_messages

Processes events emitted by the given step and enqueues them.

ensure_initialized

Ensures the process is initialized lazily (synchronously).

get_process_info

Gets the process information.

handle_message

Handles a LocalMessage that has been sent to the process.

initialize_process

Initializes the input and output edges for the process and initializes

the steps.

initialize_step

Initializes the step.

internal_execute

Internal execution logic for the process.

run_once

Starts the process with an initial event and waits for it to finish.

send_message

Sends a message to the process.

**Methods**

ﾉ

**Expand table**



start

Starts the process with an initial event.

stop

Stops a running process.

to_kernel_process

Builds a KernelProcess from the current LocalProcess.

to_kernel_process_step_info

Extracts the current state of the step and returns it as a

KernelProcessStepInfo.

**dispose**

Clean up resources.

Python

**enqueue_external_messages**

Processes external events that have been sent to the process.

Python

**Parameters**

**Name**

**Description**

`**message_channel**`

Required*

**enqueue_step_messages**

Processes events emitted by the given step and enqueues them.

Python

`dispose()`

`enqueue_external_messages(message_channel: Queue[LocalMessage])`

ﾉ

**Expand table**

`async`` enqueue_step_messages(step: LocalStep, message_channel: `

`Queue[LocalMessage])`



**Parameters**

**Name**

**Description**

`**step**`

Required*

`**message_channel**`

Required*

**ensure_initialized**

Ensures the process is initialized lazily (synchronously).

Python

**get_process_info**

Gets the process information.

Python

**handle_message**

Handles a LocalMessage that has been sent to the process.

Python

**Parameters**

ﾉ

**Expand table**

`ensure_initialized()`

`async`` get_process_info() -> KernelProcess`

`async`` handle_message(message: LocalMessage)`

ﾉ

**Expand table**



**Name**

**Description**

`**message**`

Required*

**initialize_process**

Initializes the input and output edges for the process and initializes the
steps.

Python

**initialize_step**

Initializes the step.

Python

**internal_execute**

Internal execution logic for the process.

Python

**Parameters**

**Name**

**Description**

`**max_supersteps**`

Default value: 100

`**keep_alive**`

Default value: True

**run_once**

`initialize_process()`

`async`` initialize_step()`

`async`` internal_execute(max_supersteps: int = 100, keep_alive: bool = `

`True``)`

ﾉ

**Expand table**



Starts the process with an initial event and waits for it to finish.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

**send_message**

Sends a message to the process.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

**start**

Starts the process with an initial event.

Python

`async`` run_once(process_event: KernelProcessEvent)`

ﾉ

**Expand table**

`async`` send_message(process_event: KernelProcessEvent)`

ﾉ

**Expand table**

`async`` start(keep_alive: bool = ``True``)`



**Parameters**

**Name**

**Description**

`**keep_alive**`

Default value: True

**stop**

Stops a running process.

Python

**to_kernel_process**

Builds a KernelProcess from the current LocalProcess.

Python

**to_kernel_process_step_info**

Extracts the current state of the step and returns it as a
KernelProcessStepInfo.

Python

**external_event_queue**

Python

ﾉ

**Expand table**

`async`` stop()`

`async`` to_kernel_process() -> KernelProcess`

`async`` to_kernel_process_step_info() -> KernelProcessStepInfo`

**Attributes**

`external_event_queue: Queue`



**factories**

Python

**initialize_task**

Python

**is_experimental**

Python

**kernel**

Python

**process**

Python

**process_task**

Python

**stage_status**

`factories: dict[str, Callable]`

`initialize_task: bool | ``None`

`is_experimental = ``True`

`kernel: Kernel`

`process: KernelProcess`

`process_task: Task | ``None`



Python

**step_infos**

Python

**steps**

Python

`stage_status = ``'experimental'`

`step_infos: list[KernelProcessStepInfo]`

`steps: list[LocalStep]`



**local_step Module**

Reference

**Classes**

LocalStep

A local step that is part of a local process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**LocalStep Class**

Reference

A local step that is part of a local process.

Note: This class is marked as 'experimental' and may change in the future.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**step_info**`

Required*

`**outgoing_event_queue**`

Required*

`**initialize_task**`

Required*

`**event_namespace**`

`LocalStep(*, kernel: Kernel, step_info: KernelProcessStepInfo, `

`outgoing_event_queue: Queue[LocalEvent] = ``None``, initialize_task: bool | ``None`` `

`= ``False``, event_namespace: str, step_state: KernelProcessStepState,
inputs: `

`dict[str, dict[str, Any | ``None``]] = ``None``, initial_inputs: dict[str, dict[str, `

`Any | ``None``]] = ``None``, functions: dict[str, KernelFunction] = ``None``, `

`output_edges: dict[str, list[KernelProcessEdge]] = ``None``,
parent_process_id: `

`str | ``None`` = ``None``, init_lock: Lock = ``None``, factories: dict[str, Callable])`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**step_state**`

Required*

`**inputs**`

Required*

`**initial_inputs**`

Required*

`**functions**`

Required*

`**output_edges**`

Required*

`**parent_process_id**`

Required*

`**init_lock**`

Required*

`**factories**`

Required*

emit_event

Emits an event from the step.

emit_local_event

Emits an event from the step.

get_all_events

Retrieves all events that have been emitted by this step in

the previous superstep.

get_edge_for_event

Retrieves all edges that are associated with the provided

event Id.

handle_message

Handles a LocalMessage that has been sent to the step.

initialize_step

Initializes the step.

invoke_function

Invokes the function.

parse_initial_configuration

Parses the initial configuration of the step.

**Methods**

ﾉ

**Expand table**



scoped_event

Generates a scoped event for the step.

scoped_event_from_kernel_process

Generates a scoped event for the step from a

KernelProcessEvent.

to_kernel_process_step_info

Extracts the current state of the step and returns it as a

KernelProcessStepInfo.

**emit_event**

Emits an event from the step.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

**emit_local_event**

Emits an event from the step.

Python

**Parameters**

**Name**

**Description**

`**local_event**`

Required*

`async`` emit_event(process_event: KernelProcessEvent)`

ﾉ

**Expand table**

`async`` emit_local_event(local_event: LocalEvent)`

ﾉ

**Expand table**



**get_all_events**

Retrieves all events that have been emitted by this step in the previous
superstep.

Python

**get_edge_for_event**

Retrieves all edges that are associated with the provided event Id.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**handle_message**

Handles a LocalMessage that has been sent to the step.

Python

**Parameters**

**Name**

**Description**

`**message**`

Required*

`get_all_events() -> list[LocalEvent]`

`get_edge_for_event(event_id: str) -> list[KernelProcessEdge]`

ﾉ

**Expand table**

`async`` handle_message(message: LocalMessage)`

ﾉ

**Expand table**



**initialize_step**

Initializes the step.

Python

**invoke_function**

Invokes the function.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

`**kernel**`

Required*

`**arguments**`

Required*

**parse_initial_configuration**

Parses the initial configuration of the step.

Python

**Parameters**

`async`` initialize_step()`

`async`` invoke_function(function: KernelFunction, kernel: Kernel, `

`arguments: dict[str, Any])`

ﾉ

**Expand table**

`parse_initial_configuration(data: Any) -> Any`



**Name**

**Description**

`**data**`

Required*

**scoped_event**

Generates a scoped event for the step.

Python

**Parameters**

**Name**

**Description**

`**local_event**`

Required*

**scoped_event_from_kernel_process**

Generates a scoped event for the step from a KernelProcessEvent.

Python

**Parameters**

**Name**

**Description**

`**process_event**`

Required*

ﾉ

**Expand table**

`scoped_event(local_event: LocalEvent) -> LocalEvent`

ﾉ

**Expand table**

`scoped_event_from_kernel_process(process_event: KernelProcessEvent) -> `

`LocalEvent`

ﾉ

**Expand table**



**to_kernel_process_step_info**

Extracts the current state of the step and returns it as a
KernelProcessStepInfo.

Python

**id**

Gets the ID of the step.

**name**

Gets the name of the step.

**event_namespace**

Python

**factories**

Python

**functions**

Python

**init_lock**

`async`` to_kernel_process_step_info() -> KernelProcessStepInfo`

**Attributes**

`event_namespace: str`

`factories: dict[str, Callable]`

`functions: dict[str, KernelFunction]`



Python

**initial_inputs**

Python

**initialize_task**

Python

**inputs**

Python

**is_experimental**

Python

**kernel**

Python

**outgoing_event_queue**

Python

`init_lock: Lock`

`initial_inputs: dict[str, dict[str, Any | ``None``]]`

`initialize_task: bool | ``None`

`inputs: dict[str, dict[str, Any | ``None``]]`

`is_experimental = ``True`

`kernel: Kernel`



**output_edges**

Python

**parent_process_id**

Python

**stage_status**

Python

**step_info**

Python

**step_state**

Python

`outgoing_event_queue: Queue[LocalEvent]`

`output_edges: dict[str, list[KernelProcessEdge]]`

`parent_process_id: str | ``None`

`stage_status = ``'experimental'`

`step_info: KernelProcessStepInfo`

`step_state: KernelProcessStepState`



**const Module**

Reference



**process_builder Module**

Reference

**Classes**

ProcessBuilder

A builder for a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.

ﾉ

**Expand table**



**ProcessBuilder Class**

Reference

A builder for a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.

**Constructor**

Python

add_step

Register a step type with optional constructor arguments.

add_step_from_process

Adds a step from the given process.

build

Builds the KernelProcess.

build_step

Builds the process step.

link_to

Links to the given event ID.

on_input_event

Creates a new ProcessEdgeBuilder for the input event.

resolve_function_target

Resolves the function target.

where_input_event_is

Filters the input event.

**add_step**

Register a step type with optional constructor arguments.

Python

`ProcessBuilder()`

**Methods**

ﾉ

**Expand table**

`add_step(step_type: type[TStep], name: str | ``None`` = ``None``, initial_state: `

`TState | ``None`` = ``None``, factory_function: Callable | ``None`` = ``None``, **kwargs) `



**Parameters**

**Name**

**Description**

`**step_type**`

Required*

The step type.

`**name**`

Required*

The name of the step. Defaults to None.

Default value: None

`**initial_state**`

Required*

The initial state of the step. Defaults to None.

Default value: None

`**factory_function**`

Required*

The factory function. Allows for a callable that is used to create the step

instance that may have complex dependencies that cannot be JSON

serialized or deserialized. Defaults to None.

Default value: None

`**kwargs**`

Required*

Additional keyword arguments.

**Returns**

**Type**

**Description**

The process step builder.

**add_step_from_process**

Adds a step from the given process.

Python

**Parameters**

`-> ProcessStepBuilder`

ﾉ

**Expand table**

ﾉ

**Expand table**

`add_step_from_process(kernel_process: ProcessBuilder) -> ProcessBuilder`



**Name**

**Description**

`**kernel_process**`

Required*

**build**

Builds the KernelProcess.

Python

**build_step**

Builds the process step.

Python

**link_to**

Links to the given event ID.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

`**edge_builder**`

Required*

ﾉ

**Expand table**

`build() -> KernelProcess`

`build_step() -> KernelProcessStepInfo`

`link_to(event_id: str, edge_builder: ProcessStepEdgeBuilder) -> ``None`

ﾉ

**Expand table**



**on_input_event**

Creates a new ProcessEdgeBuilder for the input event.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**resolve_function_target**

Resolves the function target.

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

`**parameter_name**`

Required*

**where_input_event_is**

Filters the input event.

`on_input_event(event_id: str | Enum) -> ProcessEdgeBuilder`

ﾉ

**Expand table**

`resolve_function_target(function_name: str | ``None``, parameter_name: str | `

`None``) -> KernelProcessFunctionTarget`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**edges**

Python

**entry_steps**

Python

**event_namespace**

Python

**external_event_target_map**

Python

`where_input_event_is(event_id: str | Enum) -> `

`ProcessFunctionTargetBuilder`

ﾉ

**Expand table**

**Attributes**

`edges: dict[str, list[Any]]`

`entry_steps: list[ProcessStepBuilder]`

`event_namespace: str`



**factories**

Python

**function_type**

Python

**functions_dict**

Python

**has_parent_process**

Python

**id**

Python

**initial_state**

Python

`external_event_target_map: dict[str, ProcessFunctionTargetBuilder]`

`factories: dict[str, Callable]`

`function_type: type[TStep] | ``None`

`functions_dict: dict[str, ``'KernelFunctionMetadata'``]`

`has_parent_process: bool`

`id: str | ``None`

`initial_state: TState | ``None`



**is_experimental**

Python

**name**

Python

**stage_status**

Python

**steps**

Python

`is_experimental = ``True`

`name: str`

`stage_status = ``'experimental'`

`steps: list[ProcessStepBuilder]`



**process_edge_builder Module**

Reference

**Classes**

ProcessEdgeBuilder

A builder for a process edge.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of ProcessEdgeBuilder.

ﾉ

**Expand table**



**ProcessEdgeBuilder Class**

Reference

A builder for a process edge.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of ProcessEdgeBuilder.

**Constructor**

Python

send_event_to

Sends the event to the target.

**send_event_to**

Sends the event to the target.

Python

**Parameters**

**Name**

**Description**

`**target**`

Required*

`ProcessEdgeBuilder()`

**Methods**

ﾉ

**Expand table**

`send_event_to(target: ProcessFunctionTargetBuilder | ProcessStepBuilder, `

`**kwargs) -> ProcessEdgeBuilder`

ﾉ

**Expand table**



**event_id**

Python

**is_experimental**

Python

**source**

Python

**stage_status**

Python

**target**

Python

**Attributes**

`event_id: str`

`is_experimental = ``True`

`source: ProcessBuilder`

`stage_status = ``'experimental'`

`target: ProcessFunctionTargetBuilder | ``None`



**process_end_step Module**

Reference

**Classes**

EndStep

An end step in a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the end step.

ﾉ

**Expand table**



**EndStep Class**

Reference

An end step in a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the end step.

**Constructor**

Python

build_step

Build the step.

get_function_metadata_map

Gets the function metadata map.

get_instance

Get the instance of the end step.

**build_step**

Build the step.

Python

**get_function_metadata_map**

Gets the function metadata map.

Python

`EndStep()`

**Methods**

ﾉ

**Expand table**

`build_step() -> KernelProcessStepInfo`



**Parameters**

**Name**

**Description**

`**plugin_type**`

Required*

`**name**`

Required*

Default value: None

`**kernel**`

Required*

Default value: None

**get_instance**

Get the instance of the end step.

Python

**END_STEP_ID**

Python

**END_STEP_NAME**

Python

`get_function_metadata_map(plugin_type, name: str | ``None`` = ``None``, kernel: `

`Kernel | ``None`` = ``None``) -> dict[str, KernelFunctionMetadata]`

ﾉ

**Expand table**

`static get_instance() -> EndStep`

**Attributes**

`END_STEP_ID: ClassVar[str] = ``'Microsoft.SemanticKernel.Process.EndStep'`

`END_STEP_NAME: ClassVar[str] = ``'Microsoft.SemanticKernel.Process.EndStep'`



**END_STEP_VALUE**

Python

**is_experimental**

Python

**stage_status**

Python

`END_STEP_VALUE: ClassVar[str] = `

`'Microsoft.SemanticKernel.Process.EndStep'`

`is_experimental = ``True`

`stage_status = ``'experimental'`



**process_event Module**

Reference

**Classes**

ProcessEvent

A wrapper around KernelProcessEvent that helps to manage the namespace of the

event.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ProcessEvent Class**

Reference

A wrapper around KernelProcessEvent that helps to manage the namespace of the

event.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**namespace**`

Required*

`**inner_event**`

Required*

**data**

The data of the event.

**id**

`ProcessEvent(*, namespace: str | ``None`` = ``None``, inner_event: `

`KernelProcessEvent)`

ﾉ

**Expand table**

**Attributes**



The Id of the event.

**visibility**

The visibility of the event.

**inner_event**

Python

**namespace**

Python

`inner_event: KernelProcessEvent`

`namespace: str | ``None`



**process_function_target_builder Module**

Reference

**Classes**

ProcessFunctionTargetBuilder

A builder for a process function target.

Note: This class is marked as 'experimental' and may change in

the future.

Initializes a new instance of ProcessFunctionTargetBuilder.

ﾉ

**Expand table**



**ProcessFunctionTargetBuilder Class**

Reference

A builder for a process function target.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of ProcessFunctionTargetBuilder.

**Constructor**

Python

build

Builds the KernelProcessFunctionTarget.

**build**

Builds the KernelProcessFunctionTarget.

Python

**function_name**

Python

`ProcessFunctionTargetBuilder()`

**Methods**

ﾉ

**Expand table**

`build() -> KernelProcessFunctionTarget`

**Attributes**

`function_name: str | ``None`



**is_experimental**

Python

**parameter_name**

Python

**stage_status**

Python

**step**

Python

**target_event_id**

Python

`is_experimental = ``True`

`parameter_name: str | ``None`

`stage_status = ``'experimental'`

`step: ProcessStepBuilder`

`target_event_id: str | ``None`



**process_message Module**

Reference

**Classes**

ProcessMessage

Represents a message used in a process runtime.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ProcessMessage Class**

Reference

Represents a message used in a process runtime.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**source_id**`

Required*

`**destination_id**`

Required*

`**function_name**`

Required*

`**values**`

Required*

`**target_event_id**`

Required*

`**target_event_data**`

Required*

`ProcessMessage(*, source_id: str, destination_id: str, function_name: str, `

`values: dict[str, Any], target_event_id: str | ``None`` = ``None``, `

`target_event_data: Any | ``None`` = ``None``)`

ﾉ

**Expand table**



**destination_id**

Python

**function_name**

Python

**source_id**

Python

**target_event_data**

Python

**target_event_id**

Python

**values**

Python

**Attributes**

`destination_id: str`

`function_name: str`

`source_id: str`

`target_event_data: Any | ``None`

`target_event_id: str | ``None`

`values: dict[str, Any]`



**process_message_factory Module**

Reference

**Classes**

ProcessMessageFactory

Factory class for creating ProcessMessage instances.

ﾉ

**Expand table**



**ProcessMessageFactory Class**

Reference

Factory class for creating ProcessMessage instances.

**Constructor**

Python

create_from_edge

Creates a new ProcessMessage from a KernelProcessEdge.

**create_from_edge**

Creates a new ProcessMessage from a KernelProcessEdge.

Python

**Parameters**

**Name**

**Description**

`**edge**`

Required*

`**data**`

Required*

`ProcessMessageFactory()`

**Methods**

ﾉ

**Expand table**

`static create_from_edge(edge: KernelProcessEdge, data: Any) -> `

`ProcessMessage`

ﾉ

**Expand table**



**process_step_builder Module**

Reference

**Classes**

ProcessStepBuilder

A builder for a process step.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.

ﾉ

**Expand table**



**ProcessStepBuilder Class**

Reference

A builder for a process step.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.

**Constructor**

Python

build_step

Builds the process step.

get_function_metadata_map

Returns a mapping of function names to their metadata.

get_scoped_event_id

Returns the scoped event ID.

get_subtype_of_stateful_step

Check if the provided type is a subclass of a generic

KernelProcessStep and return its generic type if so.

link_to

Links an event ID to a ProcessStepEdgeBuilder.

on_event

Creates a new ProcessStepEdgeBuilder for the event.

on_function_result

Creates a new ProcessStepEdgeBuilder for the function result.

on_input_event

Creates a new ProcessStepEdgeBuilder for the input event.

resolve_function_target

Resolves the function target for the given function name and

parameter name.

**build_step**

Builds the process step.

`ProcessStepBuilder()`

**Methods**

ﾉ

**Expand table**



Python

**get_function_metadata_map**

Returns a mapping of function names to their metadata.

Python

**Parameters**

**Name**

**Description**

`**plugin_type**`

Required*

`**name**`

Required*

Default value: None

`**kernel**`

Required*

Default value: None

**get_scoped_event_id**

Returns the scoped event ID.

Python

**Parameters**

`build_step() -> KernelProcessStepInfo`

`get_function_metadata_map(plugin_type, name: str | ``None`` = ``None``, kernel: `

`Kernel | ``None`` = ``None``) -> dict[str, KernelFunctionMetadata]`

ﾉ

**Expand table**

`get_scoped_event_id(event_id: str) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**event_id**`

Required*

**get_subtype_of_stateful_step**

Check if the provided type is a subclass of a generic KernelProcessStep and
return its

generic type if so.

Python

**Parameters**

**Name**

**Description**

`**type_to_check**`

Required*

**link_to**

Links an event ID to a ProcessStepEdgeBuilder.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

`**edge_builder**`

Required*

`get_subtype_of_stateful_step(type_to_check)`

ﾉ

**Expand table**

`link_to(event_id: str, edge_builder: ProcessStepEdgeBuilder) -> ``None`

ﾉ

**Expand table**



**on_event**

Creates a new ProcessStepEdgeBuilder for the event.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**on_function_result**

Creates a new ProcessStepEdgeBuilder for the function result.

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

The function name as a string or Enum.

**Returns**

**Type**

**Description**

ProcessStepEdgeBuilder

The ProcessStepEdgeBuilder instance.

`on_event(event_id: str | Enum) -> ProcessStepEdgeBuilder`

ﾉ

**Expand table**

`on_function_result(function_name: str | Enum) -> ProcessStepEdgeBuilder`

ﾉ

**Expand table**

ﾉ

**Expand table**



**on_input_event**

Creates a new ProcessStepEdgeBuilder for the input event.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**resolve_function_target**

Resolves the function target for the given function name and parameter name.

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

`**parameter_name**`

Required*

**edges**

`on_input_event(event_id: str | Enum) -> ProcessStepEdgeBuilder`

ﾉ

**Expand table**

`resolve_function_target(function_name: str | ``None``, parameter_name: str | `

`None``) -> KernelProcessFunctionTarget`

ﾉ

**Expand table**

**Attributes**



Python

**event_namespace**

Python

**function_type**

Python

**functions_dict**

Python

**id**

Python

**initial_state**

Python

**is_experimental**

Python

`edges: dict[str, list[Any]]`

`event_namespace: str`

`function_type: type[TStep] | ``None`

`functions_dict: dict[str, KernelFunctionMetadata]`

`id: str | ``None`

`initial_state: TState | ``None`



**name**

Python

**stage_status**

Python

`is_experimental = ``True`

`name: str`

`stage_status = ``'experimental'`



**process_step_edge_builder Module**

Reference

**Classes**

ProcessStepEdgeBuilder

A builder for a process step edge.

Note: This class is marked as 'experimental' and may change in the

future.

Initializes a new instance of ProcessStepEdgeBuilder.

ﾉ

**Expand table**



**ProcessStepEdgeBuilder Class**

Reference

A builder for a process step edge.

Note: This class is marked as 'experimental' and may change in the future.

Initializes a new instance of ProcessStepEdgeBuilder.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**source**`

Required*

`**event_id**`

Required*

build

Builds the KernelProcessEdge.

send_event_to

Sends the event to the target.

stop_process

Stops the process.

**build**

Builds the KernelProcessEdge.

`ProcessStepEdgeBuilder(source: ProcessStepBuilder, event_id: str)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**send_event_to**

Sends the event to the target.

Python

**Parameters**

**Name**

**Description**

`**target**`

Required*

The target to send the event to.

`****kwargs**`

Required*

Additional keyword arguments.

**Returns**

**Type**

**Description**

ProcessStepEdgeBuilder

The ProcessStepEdgeBuilder instance.

**stop_process**

Stops the process.

Python

`build() -> KernelProcessEdge`

`send_event_to(target: ProcessFunctionTargetBuilder | ProcessStepBuilder, `

`**kwargs) -> ProcessStepEdgeBuilder`

ﾉ

**Expand table**

ﾉ

**Expand table**

`stop_process()`



**event_id**

Python

**is_experimental**

Python

**source**

Python

**stage_status**

Python

**target**

Python

**Attributes**

`event_id: str`

`is_experimental = ``True`

`source: ProcessStepBuilder`

`stage_status = ``'experimental'`

`target: ProcessFunctionTargetBuilder | ``None`` = ``None`



**process_types Module**

Reference

**get_generic_state_type**

Given a subclass of KernelProcessStep, retrieve the concrete type of 'state'.

Python

**Parameters**

**Name**

**Description**

`**cls**`

Required*

**Functions**

`get_generic_state_type(cls) -> Any`

ﾉ

**Expand table**



**step_utils Module**

Reference

**find_input_channels**

Finds and creates input channels.

Python

**Parameters**

**Name**

**Description**

`**channel**`

Required*

`**functions**`

Required*

**get_fully_qualified_name**

Gets the fully qualified name of a class.

Python

**Parameters**

**Functions**

`find_input_channels(channel: KernelProcessMessageChannel, functions: `

`dict[str, KernelFunction]) -> dict[str, dict[str, Any | ``None``]]`

ﾉ

**Expand table**

`get_fully_qualified_name(cls) -> str`

ﾉ

**Expand table**



**Name**

**Description**

`**cls**`

Required*



**ProcessBuilder Class**

Reference

A builder for a process.

Note: This class is marked as 'experimental' and may change in the future.

Initialize the ProcessStepBuilder with a step class type and name.

**Constructor**

Python

add_step

Register a step type with optional constructor arguments.

add_step_from_process

Adds a step from the given process.

build

Builds the KernelProcess.

build_step

Builds the process step.

link_to

Links to the given event ID.

on_input_event

Creates a new ProcessEdgeBuilder for the input event.

resolve_function_target

Resolves the function target.

where_input_event_is

Filters the input event.

**add_step**

Register a step type with optional constructor arguments.

Python

`ProcessBuilder()`

**Methods**

ﾉ

**Expand table**

`add_step(step_type: type[TStep], name: str | ``None`` = ``None``, initial_state: `

`TState | ``None`` = ``None``, factory_function: Callable | ``None`` = ``None``, **kwargs) `



**Parameters**

**Name**

**Description**

`**step_type**`

Required*

The step type.

`**name**`

Required*

The name of the step. Defaults to None.

Default value: None

`**initial_state**`

Required*

The initial state of the step. Defaults to None.

Default value: None

`**factory_function**`

Required*

The factory function. Allows for a callable that is used to create the step

instance that may have complex dependencies that cannot be JSON

serialized or deserialized. Defaults to None.

Default value: None

`**kwargs**`

Required*

Additional keyword arguments.

**Returns**

**Type**

**Description**

The process step builder.

**add_step_from_process**

Adds a step from the given process.

Python

**Parameters**

`-> ProcessStepBuilder`

ﾉ

**Expand table**

ﾉ

**Expand table**

`add_step_from_process(kernel_process: ProcessBuilder) -> ProcessBuilder`



**Name**

**Description**

`**kernel_process**`

Required*

**build**

Builds the KernelProcess.

Python

**build_step**

Builds the process step.

Python

**link_to**

Links to the given event ID.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

`**edge_builder**`

Required*

ﾉ

**Expand table**

`build() -> KernelProcess`

`build_step() -> KernelProcessStepInfo`

`link_to(event_id: str, edge_builder: ProcessStepEdgeBuilder) -> ``None`

ﾉ

**Expand table**



**on_input_event**

Creates a new ProcessEdgeBuilder for the input event.

Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**resolve_function_target**

Resolves the function target.

Python

**Parameters**

**Name**

**Description**

`**function_name**`

Required*

`**parameter_name**`

Required*

**where_input_event_is**

Filters the input event.

`on_input_event(event_id: str | Enum) -> ProcessEdgeBuilder`

ﾉ

**Expand table**

`resolve_function_target(function_name: str | ``None``, parameter_name: str | `

`None``) -> KernelProcessFunctionTarget`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**event_id**`

Required*

**entry_steps**

Python

**external_event_target_map**

Python

**factories**

Python

**has_parent_process**

Python

`where_input_event_is(event_id: str | Enum) -> `

`ProcessFunctionTargetBuilder`

ﾉ

**Expand table**

**Attributes**

`entry_steps: list[ProcessStepBuilder]`

`external_event_target_map: dict[str, ProcessFunctionTargetBuilder]`

`factories: dict[str, Callable]`



**is_experimental**

Python

**stage_status**

Python

**steps**

Python

`has_parent_process: bool`

`is_experimental = ``True`

`stage_status = ``'experimental'`

`steps: list[ProcessStepBuilder]`



**prompt_template Package**

Reference

**Packages**

utils

**Modules**

const

handlebars_prompt_template

input_variable

jinja2_prompt_template

kernel_prompt_template

prompt_template_base

prompt_template_config

**Classes**

HandlebarsPromptTemplate

Create a Handlebars prompt template.

Handlebars are parsed as a whole and therefore do not have

variables that can be extracted, also with handlebars there is no

distinction in syntax between a variable and a value, a value that is

encountered is tried to resolve with the arguments and the

functions, if not found, the literal value is returned.

Create a new model by parsing and validating input data from

keyword arguments.

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

InputVariable

Input variable for a prompt template.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

Jinja2PromptTemplate

Creates and renders Jinja2 prompt templates to text.

Jinja2 templates support advanced features such as variable

substitution, control structures, and inheritance, making it possible

to dynamically generate text based on input arguments and

predefined functions. This class leverages Jinja2's flexibility to

render prompts that can include conditional logic, loops, and

functions, based on the provided template configuration and

arguments.

Note that the fully qualified function name (in the form of "plugin-

function") is not allowed in Jinja2 because of the hyphen. Therefore,

the function name is replaced with an underscore, which are

allowed in Python function names.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelPromptTemplate

Create a Kernel prompt template.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

PromptTemplateConfig

Configuration for a prompt template.

Create a new model by parsing and validating input data from

keyword arguments.



Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.



**utils Package**

Reference

**Modules**

handlebars_system_helpers

jinja2_system_helpers

template_function_helpers

**create_template_helper_from_function**

Create a helper function for both the Handlebars and Jinja2 templating engines
from a kernel

function.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

<xref:semantic_kernel.prompt_template.utils.KernelFunction>

The kernel function to create a helper for.

`**kernel**`

Required*

<xref:semantic_kernel.prompt_template.utils.Kernel>

The kernel to use for invoking the function.

`**base_arguments**`

Required*

<xref:semantic_kernel.prompt_template.utils.KernelArguments>

The base arguments to use when invoking the function.

`**template_format**`

Required*

<xref:semantic_kernel.prompt_template.utils.TEMPLATE_FORMAT_TYPES>

The template format to create the helper for.

`**allow_dangerously_set_content**`

<xref:<xref:semantic_kernel.prompt_template.utils.bool, optional>>

ﾉ

**Expand table**

**Functions**

`create_template_helper_from_function(function: KernelFunction, kernel:
Kernel, `

`base_arguments: KernelArguments, template_format: Literal[``'semantic-
kernel'``, `

`'handlebars'``, ``'jinja2'``], allow_dangerously_set_content: bool =
``False``, `

`enable_async: bool = ``False``) -> Callable[[...], Any]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Return the content of the function result without encoding it or not.

Default value: False

`**enable_async**`

Required*

<xref:<xref:semantic_kernel.prompt_template.utils.bool, optional>>

Enable async helper function. Defaults to False. Currently only works for

Jinja2 templates.

Default value: False

**Returns**

**Type**

**Description**

The function with args that are callable by the different templates.

**Exceptions**

**Type**

**Description**

ValueError

If the template format is not supported.

ﾉ

**Expand table**

ﾉ

**Expand table**



**handlebars_system_helpers Module**

Reference



**jinja2_system_helpers Module**

Reference



**template_function_helpers Module**

Reference

**create_template_helper_from_function**

Create a helper function for both the Handlebars and Jinja2 templating engines
from a kernel function.

Python

**Parameters**

**Name**

**Description**

`**function**`

Required*

<xref:semantic_kernel.prompt_template.utils.template_function_helpers.KernelFunction>

The kernel function to create a helper for.

`**kernel**`

Required*

<xref:semantic_kernel.prompt_template.utils.template_function_helpers.Kernel>

The kernel to use for invoking the function.

`**base_arguments**`

Required*

<xref:semantic_kernel.prompt_template.utils.template_function_helpers.KernelArguments>

The base arguments to use when invoking the function.

`**template_format**`

Required*

<xref:semantic_kernel.prompt_template.utils.template_function_helpers.TEMPLATE_FORMAT_TYPES>

The template format to create the helper for.

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.utils.template_function_helpers.bool,
optional>>

Return the content of the function result without encoding it or not.

Default value: False

`**enable_async**`

Required*

<xref:<xref:semantic_kernel.prompt_template.utils.template_function_helpers.bool,
optional>>

Enable async helper function. Defaults to False. Currently only works for
Jinja2 templates.

Default value: False

**Returns**

**Type**

**Description**

The function with args that are callable by the different templates.

**Exceptions**

**Functions**

`create_template_helper_from_function(function: KernelFunction, kernel:
Kernel, base_arguments: `

`KernelArguments, template_format: Literal[``'semantic-kernel'``,
``'handlebars'``, ``'jinja2'``], `

`allow_dangerously_set_content: bool = ``False``, enable_async: bool =
``False``) -> Callable[[...], Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

ValueError

If the template format is not supported.



**const Module**

Reference



**handlebars_prompt_template Module**

Reference

**Classes**

HandlebarsPromptTemplate

Create a Handlebars prompt template.

Handlebars are parsed as a whole and therefore do not have

variables that can be extracted, also with handlebars there is no

distinction in syntax between a variable and a value, a value that is

encountered is tried to resolve with the arguments and the

functions, if not found, the literal value is returned.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**HandlebarsPromptTemplate Class**

Reference

Create a Handlebars prompt template.

Handlebars are parsed as a whole and therefore do not have variables that can
be extracted, also with

handlebars there is no distinction in syntax between a variable and a value, a
value that is encountered is tried to

resolve with the arguments and the functions, if not found, the literal value
is returned.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

<xref:semantic_kernel.prompt_template.handlebars_prompt_template.PromptTemplateConfig>

The prompt template configuration This is checked if the template format is
'handlebars'

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.handlebars_prompt_template.bool =
False>>

Allow content without encoding throughout, this overrides the same settings in
the prompt

template config and input variables. This reverts the behavior to unencoded
input.

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

`HandlebarsPromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**

ﾉ

**Expand table**

**Methods**



model_post_init

We need to both initialize private attributes and call the user-defined
model_post_init method.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the functions

replacing their reference with the function result.

validate_template_format

Validate the template format.

**model_post_init**

We need to both initialize private attributes and call the user-defined
model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the functions replacing their

reference with the function result.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**arguments**`

Required*

The kernel arguments

Default value: None

**Returns**

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> str`

ﾉ

**Expand table**



**Type**

**Description**

The prompt template ready to be used for an AI request

**validate_template_format**

Validate the template format.

Python

**Parameters**

**Name**

**Description**

`**v**`

Required*

**allow_dangerously_set_content**

Python

**prompt_template_config**

Python

ﾉ

**Expand table**

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`

`prompt_template_config: PromptTemplateConfig`



**input_variable Module**

Reference

**Classes**

InputVariable

Input variable for a prompt template.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**InputVariable Class**

Reference

Input variable for a prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the input variable.

`**description**`

Required*

The description of the input variable.

`**default**`

Required*

The default value of the input variable.

`**is_required**`

Required*

Whether the input variable is required.

`**json_schema**`

Required*

The JSON schema for the input variable.

`**allow_dangerously_set_content**`

Required*

Allow content without encoding, this controls if this

variable is encoded before use, default is False.

`InputVariable(*, name: str, description: str | ``None`` = ``''``, default: Any | `

`None`` = ``''``, is_required: bool | ``None`` = ``True``, json_schema: str | ``None`` = ``''``, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**default**`

Required*

`**is_required**`

Default value: True

`**json_schema**`

Required*

`**allow_dangerously_set_content**`

Required*

**allow_dangerously_set_content**

Python

**default**

Python

**description**

Python

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`

`default: Any | ``None`

`description: str | ``None`



**is_required**

Python

**json_schema**

Python

**name**

Python

`is_required: bool | ``None`

`json_schema: str | ``None`

`name: str`



**jinja2_prompt_template Module**

Reference

**Classes**

Jinja2PromptTemplate

Creates and renders Jinja2 prompt templates to text.

Jinja2 templates support advanced features such as variable substitution,

control structures, and inheritance, making it possible to dynamically

generate text based on input arguments and predefined functions. This

class leverages Jinja2's flexibility to render prompts that can include

conditional logic, loops, and functions, based on the provided template

configuration and arguments.

Note that the fully qualified function name (in the form of "plugin-

function") is not allowed in Jinja2 because of the hyphen. Therefore, the

function name is replaced with an underscore, which are allowed in

Python function names.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**Jinja2PromptTemplate Class**

Reference

Creates and renders Jinja2 prompt templates to text.

Jinja2 templates support advanced features such as variable substitution,
control structures, and

inheritance, making it possible to dynamically generate text based on input
arguments and predefined

functions. This class leverages Jinja2's flexibility to render prompts that
can include conditional logic, loops,

and functions, based on the provided template configuration and arguments.

Note that the fully qualified function name (in the form of "plugin-function")
is not allowed in Jinja2

because of the hyphen. Therefore, the function name is replaced with an
underscore, which are allowed in

Python function names.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated to form a valid

model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

<xref:semantic_kernel.prompt_template.jinja2_prompt_template.PromptTemplateConfig>

The configuration object for the prompt template. This should specify the
template

format as 'jinja2' and include any necessary configuration details required
for rendering

the template.

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.jinja2_prompt_template.bool =
False>>

Allow content without encoding throughout, this overrides the same settings in
the

prompt template config and input variables. This reverts the behavior to
unencoded

input.

**Keyword-Only Parameters**

`Jinja2PromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

model_post_init

We need to both initialize private attributes and call the user-defined
model_post_init

method.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the functions

replacing their reference with the function result.

validate_template_format

Validate the template format.

**model_post_init**

We need to both initialize private attributes and call the user-defined
model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the functions replacing

their reference with the function result.

Python

**Methods**

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> str`



**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**arguments**`

Required*

The kernel arguments

Default value: None

**Returns**

**Type**

**Description**

The prompt template ready to be used for an AI request

**validate_template_format**

Validate the template format.

Python

**Parameters**

**Name**

**Description**

`**v**`

Required*

**allow_dangerously_set_content**

Python

**prompt_template_config**

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`



`prompt_template_config: PromptTemplateConfig`



**kernel_prompt_template Module**

Reference

**Classes**

KernelPromptTemplate

Create a Kernel prompt template.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelPromptTemplate Class**

Reference

Create a Kernel prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

extract_blocks

Given the prompt template, extract all the blocks (text, variables,

function calls).

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

`KernelPromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



quick_render

Quick render a Kernel prompt template, only supports text and

variable blocks.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and

execute the functions replacing their reference with the function

result.

render_blocks

Given a list of blocks render each block and compose the final result.

validate_template_format

Validate the template format.

**extract_blocks**

Given the prompt template, extract all the blocks (text, variables, function
calls).

Python

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**quick_render**

Quick render a Kernel prompt template, only supports text and variable blocks.

`extract_blocks() -> list[Block]`

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**template**`

Required*

The template to render

`**arguments**`

Required*

The arguments to use for rendering

**Returns**

**Type**

**Description**

str

The prompt template ready to be used for an AI request

**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the

functions replacing their reference with the function result.

Python

**Parameters**

`static quick_render(template: str, arguments: dict[str, Any]) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`str`

ﾉ

**Expand table**



**Name**

**Description**

`**kernel**`

Required*

<xref:<xref:semantic_kernel.prompt_template.kernel_prompt_template."Kernel">>

The kernel to use for functions.

`**arguments**`

Required*

<xref:

<xref:semantic_kernel.prompt_template.kernel_prompt_template."KernelArguments

| None">>

The arguments to use for rendering. (Default value = None)

Default value: None

**Returns**

**Type**

**Description**

str

The prompt template ready to be used for an AI request

**render_blocks**

Given a list of blocks render each block and compose the final result.

Python

**Parameters**

**Name**

**Description**

`**blocks**`

Required*

list[<xref:Block>]

Template blocks generated by ExtractBlocks

`**kernel**`

Required*

<xref:<xref:semantic_kernel.prompt_template.kernel_prompt_template."Kernel">>

The kernel to use for functions

`**arguments**`

Required*

<xref:

<xref:semantic_kernel.prompt_template.kernel_prompt_template."KernelArguments

| None">>

The arguments to use for rendering (Default value = None)

ﾉ

**Expand table**

`async`` render_blocks(blocks: list[Block], kernel: Kernel, arguments: `

`KernelArguments | ``None`` = ``None``) -> str`

ﾉ

**Expand table**



**Name**

**Description**

Default value: None

**Returns**

**Type**

**Description**

str

The prompt template ready to be used for an AI request

**validate_template_format**

Validate the template format.

Python

**Parameters**

**Name**

**Description**

`**v**`

Required*

**allow_dangerously_set_content**

Python

**prompt_template_config**

Python

ﾉ

**Expand table**

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`



`prompt_template_config: PromptTemplateConfig`



**prompt_template_base Module**

Reference

**Classes**

PromptTemplateBase

Base class for prompt templates.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PromptTemplateBase Class**

Reference

Base class for prompt templates.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

render

Render the prompt template.

**render**

Render the prompt template.

`PromptTemplateBase(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

Default value: None

**allow_dangerously_set_content**

Python

**prompt_template_config**

Python

`abstract ``async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = `

`None``) -> str`

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`

`prompt_template_config: PromptTemplateConfig`



**prompt_template_config Module**

Reference

**Classes**

PromptTemplateConfig

Configuration for a prompt template.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PromptTemplateConfig Class**

Reference

Configuration for a prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be validated

to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the prompt template.

`**description**`

Required*

The description of the prompt template.

`**template**`

Required*

The template for the prompt.

`**template_format**`

Required*

The format of the template, should be 'semantic-kernel', 'jinja2' or

'handlebars'.

`**input_variables**`

Required*

The input variables for the prompt.

`**allow_dangerously_set_content**`

Required*

<xref:

<xref:semantic_kernel.prompt_template.prompt_template_config.bool

`PromptTemplateConfig(*, name: str = ``''``, description: str | ``None`` = ``''``, `

`template: str | ``None`` = ``None``, template_format: Literal[``'semantic-kernel'``, `

`'handlebars'``, ``'jinja2'``] = ``'semantic-kernel'``, input_variables: `

`MutableSequence[InputVariable] = ``None``, allow_dangerously_set_content:
bool = `

`False``, execution_settings: MutableMapping[str, PromptExecutionSettings] = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

= False>>

Allow content without encoding throughout, this overrides the same

settings in the prompt template config and input variables. This

reverts the behavior to unencoded input.

`**execution_settings**`

Required*

The execution settings for the prompt.

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**template**`

Required*

`**template_format**`

Default value: semantic-kernel

`**input_variables**`

Required*

`**allow_dangerously_set_content**`

Required*

`**execution_settings**`

Required*

add_execution_settings

Add execution settings to the prompt template.

check_input_variables

Verify that input variable default values are string only.

from_json

Create a PromptTemplateConfig instance from a JSON string.

get_kernel_parameter_metadata

Get the kernel parameter metadata for the input variables.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



restore

Restore a PromptTemplateConfig instance from the specified

parameters.

rewrite_execution_settings

Rewrite execution settings to a dictionary.

**add_execution_settings**

Add execution settings to the prompt template.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

`**overwrite**`

Required*

Default value: True

**check_input_variables**

Verify that input variable default values are string only.

Python

**from_json**

Create a PromptTemplateConfig instance from a JSON string.

Python

`add_execution_settings(settings: PromptExecutionSettings, overwrite: bool = `

`True``) -> ``None`

ﾉ

**Expand table**

`check_input_variables()`

`from_json(json_str: str) -> _T`



**Parameters**

**Name**

**Description**

`**json_str**`

Required*

**get_kernel_parameter_metadata**

Get the kernel parameter metadata for the input variables.

Python

**restore**

Restore a PromptTemplateConfig instance from the specified parameters.

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the prompt template.

`**description**`

Required*

The description of the prompt template.

`**template**`

Required*

The template for the prompt.

ﾉ

**Expand table**

`get_kernel_parameter_metadata() -> Sequence[KernelParameterMetadata]`

`restore(name: str, description: str, template: str, template_format: `

`Literal[``'semantic-kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-
kernel'``, `

`input_variables: MutableSequence[InputVariable] = [], execution_settings: `

`MutableMapping[str, PromptExecutionSettings] = {}, `

`allow_dangerously_set_content: bool = ``False``) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

`**template_format**`

Required*

The format of the template, should be 'semantic-kernel',

'jinja2' or 'handlebars'.

Default value: semantic-kernel

`**input_variables**`

Required*

The input variables for the prompt.

Default value: []

`**execution_settings**`

Required*

The execution settings for the prompt.

Default value: {}

`**allow_dangerously_set_content**`

Required*

Allow content without encoding.

Default value: False

**Returns**

**Type**

**Description**

A new PromptTemplateConfig instance.

**rewrite_execution_settings**

Rewrite execution settings to a dictionary.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

ﾉ

**Expand table**

`rewrite_execution_settings(settings: PromptExecutionSettings | `

`Sequence[PromptExecutionSettings] | MutableMapping[str, `

`PromptExecutionSettings] | ``None``) -> MutableMapping[str, `

`PromptExecutionSettings]`

ﾉ

**Expand table**

**Attributes**



**allow_dangerously_set_content**

Python

**description**

Python

**execution_settings**

Python

**input_variables**

Python

**name**

Python

**template**

Python

**template_format**

Python

`allow_dangerously_set_content: bool`

`description: str | ``None`

`execution_settings: MutableMapping[str, PromptExecutionSettings]`

`input_variables: MutableSequence[InputVariable]`

`name: str`

`template: str | ``None`



`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``]`



**HandlebarsPromptTemplate Class**

Reference

Create a Handlebars prompt template.

Handlebars are parsed as a whole and therefore do not have variables that can
be

extracted, also with handlebars there is no distinction in syntax between a
variable and a

value, a value that is encountered is tried to resolve with the arguments and
the

functions, if not found, the literal value is returned.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

PromptTemplateConfig

The prompt template configuration This is checked if the

template format is 'handlebars'

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.bool =

False>>

Allow content without encoding throughout, this overrides

the same settings in the prompt template config and input

variables. This reverts the behavior to unencoded input.

**Keyword-Only Parameters**

`HandlebarsPromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**



**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and

execute the functions replacing their reference with the function

result.

validate_template_format

Validate the template format.

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the

functions replacing their reference with the function result.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**arguments**`

Required*

The kernel arguments

Default value: None

**Returns**

**Type**

**Description**

The prompt template ready to be used for an AI request

**validate_template_format**

Validate the template format.

Python

**Parameters**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`



**Name**

**Description**

`**v**`

Required*

ﾉ

**Expand table**



**InputVariable Class**

Reference

Input variable for a prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the input variable.

`**description**`

Required*

The description of the input variable.

`**default**`

Required*

The default value of the input variable.

`**is_required**`

Required*

Whether the input variable is required.

`**json_schema**`

Required*

The JSON schema for the input variable.

`**allow_dangerously_set_content**`

Required*

Allow content without encoding, this controls if this

variable is encoded before use, default is False.

`InputVariable(*, name: str, description: str | ``None`` = ``''``, default: Any | `

`None`` = ``''``, is_required: bool | ``None`` = ``True``, json_schema: str | ``None`` = ``''``, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**default**`

Required*

`**is_required**`

Default value: True

`**json_schema**`

Required*

`**allow_dangerously_set_content**`

Required*

**allow_dangerously_set_content**

Python

**default**

Python

**description**

Python

ﾉ

**Expand table**

**Attributes**

`allow_dangerously_set_content: bool`

`default: Any | ``None`

`description: str | ``None`



**is_required**

Python

**json_schema**

Python

**name**

Python

`is_required: bool | ``None`

`json_schema: str | ``None`

`name: str`



**Jinja2PromptTemplate Class**

Reference

Creates and renders Jinja2 prompt templates to text.

Jinja2 templates support advanced features such as variable substitution,
control

structures, and inheritance, making it possible to dynamically generate text
based on

input arguments and predefined functions. This class leverages Jinja2's
flexibility to

render prompts that can include conditional logic, loops, and functions, based
on the

provided template configuration and arguments.

Note that the fully qualified function name (in the form of "plugin-function")
is not

allowed in Jinja2 because of the hyphen. Therefore, the function name is
replaced with

an underscore, which are allowed in Python function names.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

PromptTemplateConfig

The configuration object for the prompt template. This

should specify the template format as 'jinja2' and include

any necessary configuration details required for rendering

the template.

`Jinja2PromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**



**Name**

**Description**

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.bool =

False>>

Allow content without encoding throughout, this overrides

the same settings in the prompt template config and input

variables. This reverts the behavior to unencoded input.

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and

execute the functions replacing their reference with the function

result.

validate_template_format

Validate the template format.

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`model_post_init(context: Any, /) -> ``None`



**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the

functions replacing their reference with the function result.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel instance

`**arguments**`

Required*

The kernel arguments

Default value: None

**Returns**

**Type**

**Description**

The prompt template ready to be used for an AI request

**validate_template_format**

ﾉ

**Expand table**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`str`

ﾉ

**Expand table**

ﾉ

**Expand table**



Validate the template format.

Python

**Parameters**

**Name**

**Description**

`**v**`

Required*

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`

ﾉ

**Expand table**



**KernelPromptTemplate Class**

Reference

Create a Kernel prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**prompt_template_config**`

Required*

`**allow_dangerously_set_content**`

Required*

extract_blocks

Given the prompt template, extract all the blocks (text, variables,

function calls).

model_post_init

We need to both initialize private attributes and call the user-defined

model_post_init method.

`KernelPromptTemplate(*, prompt_template_config: PromptTemplateConfig, `

`allow_dangerously_set_content: bool = ``False``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



quick_render

Quick render a Kernel prompt template, only supports text and

variable blocks.

render

Render the prompt template.

Using the prompt template, replace the variables with their values and

execute the functions replacing their reference with the function

result.

render_blocks

Given a list of blocks render each block and compose the final result.

validate_template_format

Validate the template format.

**extract_blocks**

Given the prompt template, extract all the blocks (text, variables, function
calls).

Python

**model_post_init**

We need to both initialize private attributes and call the user-defined

model_post_init method.

Python

**Positional-Only Parameters**

**Name**

**Description**

`**context**`

Required*

**quick_render**

Quick render a Kernel prompt template, only supports text and variable blocks.

`extract_blocks() -> list[Block]`

`model_post_init(context: Any, /) -> ``None`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**template**`

Required*

The template to render

`**arguments**`

Required*

The arguments to use for rendering

**Returns**

**Type**

**Description**

str

The prompt template ready to be used for an AI request

**render**

Render the prompt template.

Using the prompt template, replace the variables with their values and execute
the

functions replacing their reference with the function result.

Python

**Parameters**

`static quick_render(template: str, arguments: dict[str, Any]) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> `

`str`

ﾉ

**Expand table**



**Name**

**Description**

`**kernel**`

Required*

<xref:<xref:semantic_kernel.prompt_template."Kernel">>

The kernel to use for functions.

`**arguments**`

Required*

<xref:<xref:semantic_kernel.prompt_template."KernelArguments | None">>

The arguments to use for rendering. (Default value = None)

Default value: None

**Returns**

**Type**

**Description**

str

The prompt template ready to be used for an AI request

**render_blocks**

Given a list of blocks render each block and compose the final result.

Python

**Parameters**

**Name**

**Description**

`**blocks**`

Required*

list[<xref:Block>]

Template blocks generated by ExtractBlocks

`**kernel**`

Required*

<xref:<xref:semantic_kernel.prompt_template."Kernel">>

The kernel to use for functions

`**arguments**`

Required*

<xref:<xref:semantic_kernel.prompt_template."KernelArguments | None">>

The arguments to use for rendering (Default value = None)

Default value: None

**Returns**

ﾉ

**Expand table**

`async`` render_blocks(blocks: list[Block], kernel: Kernel, arguments: `

`KernelArguments | ``None`` = ``None``) -> str`

ﾉ

**Expand table**



**Type**

**Description**

str

The prompt template ready to be used for an AI request

**validate_template_format**

Validate the template format.

Python

**Parameters**

**Name**

**Description**

`**v**`

Required*

ﾉ

**Expand table**

`validate_template_format(v: PromptTemplateConfig) -> PromptTemplateConfig`

ﾉ

**Expand table**



**PromptTemplateConfig Class**

Reference

Configuration for a prompt template.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the prompt template.

`**description**`

Required*

The description of the prompt template.

`**template**`

Required*

The template for the prompt.

`**template_format**`

Required*

The format of the template, should be 'semantic-kernel',

'jinja2' or 'handlebars'.

`**input_variables**`

Required*

The input variables for the prompt.

`PromptTemplateConfig(*, name: str = ``''``, description: str | ``None`` = ``''``, `

`template: str | ``None`` = ``None``, template_format: Literal[``'semantic-kernel'``, `

`'handlebars'``, ``'jinja2'``] = ``'semantic-kernel'``, input_variables: `

`MutableSequence[InputVariable] = ``None``, allow_dangerously_set_content:
bool = `

`False``, execution_settings: MutableMapping[str, PromptExecutionSettings] = `

`None``)`

ﾉ

**Expand table**



**Name**

**Description**

`**allow_dangerously_set_content**`

Required*

<xref:<xref:semantic_kernel.prompt_template.bool =

False>>

Allow content without encoding throughout, this overrides

the same settings in the prompt template config and input

variables. This reverts the behavior to unencoded input.

`**execution_settings**`

Required*

The execution settings for the prompt.

**Keyword-Only Parameters**

**Name**

**Description**

`**name**`

Required*

`**description**`

Required*

`**template**`

Required*

`**template_format**`

Default value: semantic-kernel

`**input_variables**`

Required*

`**allow_dangerously_set_content**`

Required*

`**execution_settings**`

Required*

add_execution_settings

Add execution settings to the prompt template.

check_input_variables

Verify that input variable default values are string only.

from_json

Create a PromptTemplateConfig instance from a JSON string.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



get_kernel_parameter_metadata

Get the kernel parameter metadata for the input variables.

restore

Restore a PromptTemplateConfig instance from the specified

parameters.

rewrite_execution_settings

Rewrite execution settings to a dictionary.

**add_execution_settings**

Add execution settings to the prompt template.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

`**overwrite**`

Required*

Default value: True

**check_input_variables**

Verify that input variable default values are string only.

Python

**from_json**

Create a PromptTemplateConfig instance from a JSON string.

Python

`add_execution_settings(settings: PromptExecutionSettings, overwrite: bool `

`= ``True``) -> ``None`

ﾉ

**Expand table**

`check_input_variables()`



**Parameters**

**Name**

**Description**

`**json_str**`

Required*

**get_kernel_parameter_metadata**

Get the kernel parameter metadata for the input variables.

Python

**restore**

Restore a PromptTemplateConfig instance from the specified parameters.

Python

**Parameters**

**Name**

**Description**

`**name**`

Required*

The name of the prompt template.

`**description**`

The description of the prompt template.

`from_json(json_str: str) -> _T`

ﾉ

**Expand table**

`get_kernel_parameter_metadata() -> Sequence[KernelParameterMetadata]`

`restore(name: str, description: str, template: str, template_format: `

`Literal[``'semantic-kernel'``, ``'handlebars'``, ``'jinja2'``] = ``'semantic-
kernel'``, `

`input_variables: MutableSequence[InputVariable] = [], execution_settings: `

`MutableMapping[str, PromptExecutionSettings] = {}, `

`allow_dangerously_set_content: bool = ``False``) -> _T`

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**template**`

Required*

The template for the prompt.

`**template_format**`

Required*

The format of the template, should be 'semantic-

kernel', 'jinja2' or 'handlebars'.

Default value: semantic-kernel

`**input_variables**`

Required*

The input variables for the prompt.

Default value: []

`**execution_settings**`

Required*

The execution settings for the prompt.

Default value: {}

`**allow_dangerously_set_content**`

Required*

Allow content without encoding.

Default value: False

**Returns**

**Type**

**Description**

A new PromptTemplateConfig instance.

**rewrite_execution_settings**

Rewrite execution settings to a dictionary.

Python

**Parameters**

**Name**

**Description**

`**settings**`

ﾉ

**Expand table**

`rewrite_execution_settings(settings: PromptExecutionSettings | `

`Sequence[PromptExecutionSettings] | MutableMapping[str, `

`PromptExecutionSettings] | ``None``) -> MutableMapping[str, `

`PromptExecutionSettings]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**allow_dangerously_set_content**

Python

**description**

Python

**execution_settings**

Python

**input_variables**

Python

**name**

Python

**template**

**Attributes**

`allow_dangerously_set_content: bool`

`description: str | ``None`

`execution_settings: MutableMapping[str, PromptExecutionSettings]`

`input_variables: MutableSequence[InputVariable]`

`name: str`



Python

**template_format**

Python

`template: str | ``None`

`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``]`



**reliability Package**

Reference

**Modules**

kernel_reliability_extension

pass_through_without_retry

retry_mechanism_base

ﾉ

**Expand table**



**kernel_reliability_extension Module**

Reference

**Classes**

KernelReliabilityExtension

Kernel reliability extension.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelReliabilityExtension Class**

Reference

Kernel reliability extension.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**retry_mechanism**`

Required*

**retry_mechanism**

Data descriptor used to emit a runtime deprecation warning before accessing a

deprecated field.

Python

**msg**

`KernelReliabilityExtension(*, retry_mechanism: RetryMechanismBase =
``None``)`

ﾉ

**Expand table**

**Attributes**

`retry_mechanism: RetryMechanismBase`



The deprecation message to be emitted.

**wrapped_property**

The property instance if the deprecated field is a computed field, or _None_.

**field_name**

The name of the field being deprecated.



**pass_through_without_retry Module**

Reference

**Classes**

PassThroughWithoutRetry

A retry mechanism that does not retry.

Create a new model by parsing and validating input data from

keyword arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input

data cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**PassThroughWithoutRetry Class**

Reference

A retry mechanism that does not retry.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

execute_with_retry

Executes the given action with retry logic.

**execute_with_retry**

Executes the given action with retry logic.

Python

**Parameters**

**Name**

**Description**

`**action**`

<xref:Callable>[[],<xref: Awaitable>[<xref:T>]]

`PassThroughWithoutRetry()`

**Methods**

ﾉ

**Expand table**

`async`` execute_with_retry(action: Callable[[], Awaitable[T]]) -> `

`Awaitable[T]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The action to retry on exception.

**Returns**

**Type**

**Description**

Awaitable[<xref:T>]

An awaitable that will return the result of the action.

ﾉ

**Expand table**



**retry_mechanism_base Module**

Reference

**Classes**

RetryMechanismBase

Base class for retry mechanisms.

ﾉ

**Expand table**



**RetryMechanismBase Class**

Reference

Base class for retry mechanisms.

**Constructor**

Python

execute_with_retry

Executes the given action with retry logic.

**execute_with_retry**

Executes the given action with retry logic.

Python

**Parameters**

**Name**

**Description**

`**action**`

Required*

<xref:Callable>[[],<xref: Awaitable>[<xref:T>]]

The action to retry on exception.

**Returns**

`RetryMechanismBase()`

**Methods**

ﾉ

**Expand table**

`abstract ``async`` execute_with_retry(action: Callable[[], Awaitable[T]]) ->
`

`Awaitable[T]`

ﾉ

**Expand table**



**Type**

**Description**

Awaitable[<xref:T>]

An awaitable that will return the result of the action.

ﾉ

**Expand table**



**schema Package**

Reference

**Modules**

kernel_json_schema_builder

ﾉ

**Expand table**



**kernel_json_schema_builder Module**

Reference

**Classes**

KernelJsonSchemaBuilder

Kernel JSON schema builder.

ﾉ

**Expand table**



**KernelJsonSchemaBuilder Class**

Reference

Kernel JSON schema builder.

**Constructor**

Python

build

Builds the JSON schema for a given parameter type and description.

build_enum_schema

Builds the JSON schema for an enum type.

build_from_type_name

Builds the JSON schema for a given parameter type name and

description.

build_model_schema

Builds the JSON schema for a given model and description.

get_json_schema

Gets JSON schema for a given parameter type.

handle_complex_type

Handles building the JSON schema for complex types.

**build**

Builds the JSON schema for a given parameter type and description.

Python

**Parameters**

`KernelJsonSchemaBuilder()`

**Methods**

ﾉ

**Expand table**

`build(parameter_type: type | str | Any, description: str | ``None`` = ``None``, `

`structured_output: bool = ``False``) -> dict[str, Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**parameter_type**`

Required*

The parameter type.

`**description**`

Required*

The description of the parameter. Defaults to None.

Default value: None

`**structured_output**`

Required*

Whether the outputs are structured. Defaults to False.

Default value: False

**Returns**

**Type**

**Description**

dict[str, Any]

The JSON schema for the parameter type.

**build_enum_schema**

Builds the JSON schema for an enum type.

Python

**Parameters**

**Name**

**Description**

`**enum_type**`

Required*

type

The enum type.

`**description**`

Required*

<xref:<xref:semantic_kernel.schema.kernel_json_schema_builder.str,

optional>>

The description of the enum. Defaults to None.

Default value: None

**Returns**

ﾉ

**Expand table**

`build_enum_schema(enum_type: type, description: str | ``None`` = ``None``) -> `

`dict[str, Any]`

ﾉ

**Expand table**



**Type**

**Description**

dict[str, Any]

The JSON schema for the enum type.

**build_from_type_name**

Builds the JSON schema for a given parameter type name and description.

Python

**Parameters**

**Name**

**Description**

`**parameter_type**`

Required*

str

The parameter type name.

`**description**`

Required*

<xref:<xref:semantic_kernel.schema.kernel_json_schema_builder.str,

optional>>

The description of the parameter. Defaults to None.

Default value: None

**Returns**

**Type**

**Description**

dict[str, Any]

The JSON schema for the parameter type.

**build_model_schema**

Builds the JSON schema for a given model and description.

Python

ﾉ

**Expand table**

`build_from_type_name(parameter_type: str, description: str | ``None`` = ``None``) `

`-> dict[str, Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**model**`

Required*

The model type.

`**description**`

Required*

The description of the model. Defaults to None.

Default value: None

`**structured_output**`

Required*

Whether the outputs are structured. Defaults to False.

Default value: False

**Returns**

**Type**

**Description**

dict[str, Any]

The JSON schema for the model.

**get_json_schema**

Gets JSON schema for a given parameter type.

Python

**Parameters**

**Name**

**Description**

`**parameter_type**`

Required*

type

The parameter type.

`build_model_schema(model: type | KernelBaseModel, description: str | ``None`` `

`= ``None``, structured_output: bool = ``False``) -> dict[str, Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**

`get_json_schema(parameter_type: type) -> dict[str, Any]`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

dict[str, Any]

The JSON schema for the parameter type.

**handle_complex_type**

Handles building the JSON schema for complex types.

Python

**Parameters**

**Name**

**Description**

`**parameter_type**`

Required*

The parameter type.

`**description**`

Required*

The description of the parameter. Defaults to None.

Default value: None

`**structured_output**`

Required*

Whether the outputs are structured. Defaults to False.

Default value: False

**Returns**

**Type**

**Description**

dict[str, Any]

The JSON schema for the parameter type.

ﾉ

**Expand table**

`handle_complex_type(parameter_type: type, description: str | ``None`` = ``None``, `

`structured_output: bool = ``False``) -> dict[str, Any]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**services Package**

Reference

**Modules**

ai_service_client_base

ai_service_selector

kernel_services_extension

**Classes**

AIServiceSelector

Default service selector, can be subclassed and overridden.

To use a custom service selector, subclass this class and override the

select_ai_service method. Make sure that the function signature stays the

same.

ﾉ

**Expand table**

ﾉ

**Expand table**



**ai_service_client_base Module**

Reference

**Classes**

AIServiceClientBase

Base class for all AI Services.

Has an ai_model_id and service_id, any other fields have to be defined by

the subclasses.

The ai_model_id can refer to a specific model, like 'gpt-35-turbo' for

OpenAI, or can just be a string that is used to identify the model in the

service.

The service_id is used in Semantic Kernel to identify the service, if empty
the

ai_model_id is used.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**AIServiceClientBase Class**

Reference

Base class for all AI Services.

Has an ai_model_id and service_id, any other fields have to be defined by the
subclasses.

The ai_model_id can refer to a specific model, like 'gpt-35-turbo' for OpenAI,
or can just

be a string that is used to identify the model in the service.

The service_id is used in Semantic Kernel to identify the service, if empty
the ai_model_id

is used.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**ai_model_id**`

Required*

`**service_id**`

Required*

`AIServiceClientBase(*, ai_model_id: Annotated[str, `

`StringConstraints(strip_whitespace=``True``, to_upper=``None``,
to_lower=``None``, `

`strict=``None``, min_length=1, max_length=``None``, pattern=``None``)],
service_id: str `

`= ``''``)`

ﾉ

**Expand table**

**Methods**



get_prompt_execution_settings_class

Get the request settings class.

get_prompt_execution_settings_from_settings

Get the request settings from a settings object.

instantiate_prompt_execution_settings

Create a request settings object.

All arguments are passed to the constructor of

the request settings object.

model_post_init

Update the service_id if it is not set.

service_url

Get the URL of the service.

Override this in the subclass to return the proper

URL. If the service does not have a URL, return

None.

**get_prompt_execution_settings_class**

Get the request settings class.

Python

**get_prompt_execution_settings_from_settings**

Get the request settings from a settings object.

Python

**Parameters**

**Name**

**Description**

`**settings**`

Required*

ﾉ

**Expand table**

`get_prompt_execution_settings_class() -> type[PromptExecutionSettings]`

`get_prompt_execution_settings_from_settings(settings: `

`PromptExecutionSettings) -> PromptExecutionSettings`

ﾉ

**Expand table**



**instantiate_prompt_execution_settings**

Create a request settings object.

All arguments are passed to the constructor of the request settings object.

Python

**model_post_init**

Update the service_id if it is not set.

Python

**Parameters**

**Name**

**Description**

`**_AIServiceClientBase__context**`

Default value: None

**service_url**

Get the URL of the service.

Override this in the subclass to return the proper URL. If the service does
not have a

URL, return None.

Python

`instantiate_prompt_execution_settings(**kwargs) -> `

`PromptExecutionSettings`

`model_post_init(_AIServiceClientBase__context: object | ``None`` = ``None``)`

ﾉ

**Expand table**

`service_url() -> str | ``None`

**Attributes**



**ai_model_id**

Python

**service_id**

Python

`ai_model_id: Annotated[str, StringConstraints(strip_whitespace=``True``, `

`to_upper=``None``, to_lower=``None``, strict=``None``, min_length=1,
max_length=``None``, `

`pattern=``None``)]`

`service_id: str`



**ai_service_selector Module**

Reference

**Classes**

AIServiceSelector

Default service selector, can be subclassed and overridden.

To use a custom service selector, subclass this class and override the

select_ai_service method. Make sure that the function signature stays the

same.

ﾉ

**Expand table**



**AIServiceSelector Class**

Reference

Default service selector, can be subclassed and overridden.

To use a custom service selector, subclass this class and override the
select_ai_service

method. Make sure that the function signature stays the same.

**Constructor**

Python

select_ai_service

Select an AI Service on a first come, first served basis.

Starts with execution settings in the arguments, followed by the execution

settings from the function. If the same service_id is in both, the one in the

arguments will be used.

**select_ai_service**

Select an AI Service on a first come, first served basis.

Starts with execution settings in the arguments, followed by the execution
settings

from the function. If the same service_id is in both, the one in the arguments
will be

used.

Python

`AIServiceSelector()`

**Methods**

ﾉ

**Expand table**

`select_ai_service(kernel: KernelServicesExtension, function: `

`KernelFunction | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`type_: type[AI_SERVICE_CLIENT_TYPE] | tuple[type[AI_SERVICE_CLIENT_TYPE], `

`...] | ``None`` = ``None``) -> tuple[AIServiceClientBase, `

`PromptExecutionSettings]`



**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel used.

`**function**`

Required*

The function used. (optional)

Default value: None

`**arguments**`

Required*

The arguments used. (optional)

Default value: None

`**type**`

Required*

The type of service to select. (optional)

`**type_**`

Required*

Default value: None

ﾉ

**Expand table**



**kernel_services_extension Module**

Reference

**Classes**

KernelServicesExtension

Kernel services extension.

Adds all services related entities to the Kernel.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**KernelServicesExtension Class**

Reference

Kernel services extension.

Adds all services related entities to the Kernel.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**services**`

Required*

`**ai_service_selector**`

Required*

add_service

Add a single service to the Kernel.

get_prompt_execution_settings_from_service_id

Get the specific request settings from the

service, instantiated with the service_id and

`KernelServicesExtension(*, services: dict[str, AIServiceClientBase] =
``None``, `

`ai_service_selector: AIServiceSelector = ``None``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



ai_model_id.

get_service

Get a service by service_id and type.

Type is optional and when not supplied, no

checks are done. Type should be

TextCompletionClientBase,

ChatCompletionClientBase,

EmbeddingGeneratorBase or a subclass of one.

You can also check for multiple types in one go,

by using a tuple: (TextCompletionClientBase,

ChatCompletionClientBase).

If type and service_id are both None, the first

service is returned.

get_services_by_type

Get all services of a specific type.

remove_all_services

Removes the services from the Kernel, does not

delete them.

remove_service

Delete a single service from the Kernel.

rewrite_services

Rewrite services to a dictionary.

select_ai_service

Uses the AI service selector to select a service

for the function.

**add_service**

Add a single service to the Kernel.

Python

**Parameters**

**Name**

**Description**

`**service**`

Required*

<xref:semantic_kernel.services.kernel_services_extension.AIServiceClientBase>

The service to add.

`**overwrite**`

<xref:<xref:semantic_kernel.services.kernel_services_extension.bool,
optional>>

`add_service(service: AIServiceClientBase, overwrite: bool = ``False``) -> `

`None`

ﾉ

**Expand table**



**Name**

**Description**

Required*

Whether to overwrite the service if it already exists. Defaults to False.

Default value: False

**get_prompt_execution_settings_from_service_id**

Get the specific request settings from the service, instantiated with the
service_id and

ai_model_id.

Python

**Parameters**

**Name**

**Description**

`**service_id**`

Required*

`**type**`

Required*

Default value: None

**get_service**

Get a service by service_id and type.

Type is optional and when not supplied, no checks are done. Type should be

TextCompletionClientBase, ChatCompletionClientBase, EmbeddingGeneratorBase or

a subclass of one. You can also check for multiple types in one go, by using a
tuple:

(TextCompletionClientBase, ChatCompletionClientBase).

If type and service_id are both None, the first service is returned.

Python

`get_prompt_execution_settings_from_service_id(service_id: str, type: `

`type[AI_SERVICE_CLIENT_TYPE] | ``None`` = ``None``) -> PromptExecutionSettings`

ﾉ

**Expand table**

`get_service(service_id: str | ``None`` = ``None``, type: `

`type[AI_SERVICE_CLIENT_TYPE] | tuple[type[AI_SERVICE_CLIENT_TYPE], ...] | `

`None`` = ``None``) -> AIServiceClientBase`



**Parameters**

**Name**

**Description**

`**service_id**`

<xref:<xref:semantic_kernel.services.kernel_services_extension.str | None>>

The service id, if None, the default service is returned or the first service
is

returned.

Default value: None

`**type**`

<xref:Type>[<xref:AI_SERVICE_CLIENT_TYPE>]<xref: |

tuple>[type[<xref:AI_SERVICE_CLIENT_TYPE>],<xref: ...>]<xref: | None>

The type of the service, if None, no checks are done on service type.

Default value: None

**Returns**

**Type**

**Description**

AIServiceClientBase

The service, should be a class derived from AIServiceClientBase.

**Exceptions**

**Type**

**Description**

KernelServiceNotFoundError

If no service is found that matches the type or id.

**get_services_by_type**

Get all services of a specific type.

Python

**Parameters**

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**

`get_services_by_type(type: type[AI_SERVICE_CLIENT_TYPE] | `

`tuple[type[AI_SERVICE_CLIENT_TYPE], ...] | ``None``) -> dict[str, `

`AIServiceClientBase]`



**Name**

**Description**

`**type**`

Required*

**remove_all_services**

Removes the services from the Kernel, does not delete them.

Python

**remove_service**

Delete a single service from the Kernel.

Python

**Parameters**

**Name**

**Description**

`**service_id**`

Required*

**rewrite_services**

Rewrite services to a dictionary.

Python

ﾉ

**Expand table**

`remove_all_services() -> ``None`

`remove_service(service_id: str) -> ``None`

ﾉ

**Expand table**

`rewrite_services(services: AI_SERVICE_CLIENT_TYPE | `

`list[AI_SERVICE_CLIENT_TYPE] | dict[str, AI_SERVICE_CLIENT_TYPE] | ``None`` = `

`None``) -> dict[str, AI_SERVICE_CLIENT_TYPE]`



**Parameters**

**Name**

**Description**

`**services**`

Default value: None

**select_ai_service**

Uses the AI service selector to select a service for the function.

Python

**Parameters**

**Name**

**Description**

`**function**`

<xref:<xref:semantic_kernel.services.kernel_services_extension.KernelFunction
|

None>>

The function used.

Default value: None

`**arguments**`

<xref:<xref:semantic_kernel.services.kernel_services_extension.KernelArguments

| None>>

The arguments used.

Default value: None

`**type**`

<xref:Type>[<xref:AI_SERVICE_CLIENT_TYPE>]<xref: |

tuple>[type[<xref:AI_SERVICE_CLIENT_TYPE>],<xref: ...>]<xref: | None>

The type of service to select. Defaults to None.

Default value: None

**ai_service_selector**

ﾉ

**Expand table**

`select_ai_service(function: KernelFunction | ``None`` = ``None``, arguments: `

`KernelArguments | ``None`` = ``None``, type: type[AI_SERVICE_CLIENT_TYPE] | `

`tuple[type[AI_SERVICE_CLIENT_TYPE], ...] | ``None`` = ``None``) -> `

`tuple[AIServiceClientBase, PromptExecutionSettings]`

ﾉ

**Expand table**

**Attributes**



Python

**services**

Python

`ai_service_selector: AIServiceSelector`

`services: dict[str, AIServiceClientBase]`



**AIServiceSelector Class**

Reference

Default service selector, can be subclassed and overridden.

To use a custom service selector, subclass this class and override the
select_ai_service

method. Make sure that the function signature stays the same.

**Constructor**

Python

select_ai_service

Select an AI Service on a first come, first served basis.

Starts with execution settings in the arguments, followed by the execution

settings from the function. If the same service_id is in both, the one in the

arguments will be used.

**select_ai_service**

Select an AI Service on a first come, first served basis.

Starts with execution settings in the arguments, followed by the execution
settings

from the function. If the same service_id is in both, the one in the arguments
will be

used.

Python

`AIServiceSelector()`

**Methods**

ﾉ

**Expand table**

`select_ai_service(kernel: KernelServicesExtension, function: `

`KernelFunction | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`type_: type[AI_SERVICE_CLIENT_TYPE] | tuple[type[AI_SERVICE_CLIENT_TYPE], `

`...] | ``None`` = ``None``) -> tuple[AIServiceClientBase, `

`PromptExecutionSettings]`



**Parameters**

**Name**

**Description**

`**kernel**`

Required*

The kernel used.

`**function**`

Required*

The function used. (optional)

Default value: None

`**arguments**`

Required*

The arguments used. (optional)

Default value: None

`**type**`

Required*

The type of service to select. (optional)

`**type_**`

Required*

Default value: None

ﾉ

**Expand table**



**template_engine Package**

Reference

**Packages**

blocks

protocols

**Modules**

code_tokenizer

template_tokenizer

ﾉ

**Expand table**

ﾉ

**Expand table**



**blocks Package**

Reference

**Modules**

block

block_types

code_block

function_id_block

named_arg_block

symbols

text_block

val_block

var_block

ﾉ

**Expand table**



**block Module**

Reference

**Classes**

Block

A block.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**Block Class**

Reference

A block.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**content**`

Required*

content_strip

Strip the content of the block.

**content_strip**

Strip the content of the block.

Python

`Block(*, content: str)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**content**`

Required*

**content**

Python

**type**

Python

`content_strip(content: str)`

ﾉ

**Expand table**

**Attributes**

`content: str`

`type: ClassVar[BlockTypes] = 1`



**block_types Module**

Reference

**Enums**

BlockTypes

Block types.

ﾉ

**Expand table**



**BlockTypes Enum**

Reference

Block types.

CODE

FUNCTION_ID

NAMED_ARG

TEXT

UNDEFINED

VALUE

VARIABLE

**Fields**

ﾉ

**Expand table**



**code_block Module**

Reference

**Classes**

CodeBlock

Create a code block.

A code block is a block that usually contains functions to be executed by the
kernel.

It consists of a list of tokens that can be either a function_id, value, a
variable or a

named argument.

If the first token is not a function_id but a variable or value, the rest of
the tokens will

be ignored. Only the first argument for the function can be a variable or
value, the

rest of the tokens have be named arguments.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**CodeBlock Class**

Reference

Create a code block.

A code block is a block that usually contains functions to be executed by the
kernel. It

consists of a list of tokens that can be either a function_id, value, a
variable or a named

argument.

If the first token is not a function_id but a variable or value, the rest of
the tokens will be

ignored. Only the first argument for the function can be a variable or value,
the rest of

the tokens have be named arguments.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

The content of the code block.

`**tokens**`

Required*

The list of tokens that compose the code block, if empty, will be created by
the

CodeTokenizer.

**Keyword-Only Parameters**

`CodeBlock(*, content: str, tokens: list[Block] = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**content**`

Required*

`**tokens**`

Required*

check_tokens

Check the tokens in the list.

If the first token is a value or variable, the rest of the tokens will be
ignored. If the

first token is a function_id, then the next token can be a value,

variable or named_arg, the rest have to be named_args.

parse_content

Parse the content of the code block and tokenize it.

If tokens are already present, skip the tokenizing.

render_code

Render the code block.

If the first token is a function_id, it will call the function from the plugin
collection.

Otherwise, it is a value or variable and those are then rendered directly.

**check_tokens**

Check the tokens in the list.

If the first token is a value or variable, the rest of the tokens will be
ignored. If the

first token is a function_id, then the next token can be a value,

variable or named_arg, the rest have to be named_args.

Python

**Parameters**

**Methods**

ﾉ

**Expand table**

`check_tokens(tokens: list[Block]) -> list[Block]`

ﾉ

**Expand table**



**Name**

**Description**

`**tokens**`

Required*

**Exceptions**

**Type**

**Description**

CodeBlockTokenError

If the content does not contain at least one token.

CodeBlockTokenError

If the first token is a named argument.

CodeBlockTokenError

If the second token is not a value or variable.

CodeBlockTokenError

If a token is not a named argument after the second token.

**parse_content**

Parse the content of the code block and tokenize it.

If tokens are already present, skip the tokenizing.

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

**Exceptions**

ﾉ

**Expand table**

`parse_content(fields: Any) -> Any`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

CodeBlockTokenError

If the content does not contain at least one token.

CodeBlockTokenError

If the first token is a named argument.

CodeBlockTokenError

If the second token is not a value or variable.

CodeBlockTokenError

If a token is not a named argument after the second token.

**render_code**

Render the code block.

If the first token is a function_id, it will call the function from the plugin
collection.

Otherwise, it is a value or variable and those are then rendered directly.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`**arguments**`

Required*

**Exceptions**

**Type**

**Description**

CodeBlockTokenError

If the content does not contain at least one token.

CodeBlockTokenError

If the first token is a named argument.

CodeBlockTokenError

If the second token is not a value or variable.

`async`` render_code(kernel: Kernel, arguments: KernelArguments) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

CodeBlockTokenError

If a token is not a named argument after the second token.

**tokens**

Python

**type**

Python

**Attributes**

`tokens: list[Block]`

`type: ClassVar[BlockTypes] = 3`



**function_id_block Module**

Reference

**Classes**

FunctionIdBlock

Block to represent a function id. It can be used to call a function from a
plugin.

The content is parsed using a regex, that returns either a plugin and function

name or just a function name, depending on the content.

Anything other than that and a ValueError is raised.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**FunctionIdBlock Class**

Reference

Block to represent a function id. It can be used to call a function from a
plugin.

The content is parsed using a regex, that returns either a plugin and function
name or

just a function name, depending on the content.

Anything other than that and a ValueError is raised.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

str

The content of the block.

`**function_name**`

Required*

<xref:Optional>[str],<xref: optional>

The function name.

`**plugin_name**`

Required*

<xref:Optional>[str],<xref: optional>

The plugin name.

**Keyword-Only Parameters**

`FunctionIdBlock(*, content: str, function_name: str = ``''``, plugin_name: str | `

`None`` = ``None``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**content**`

Required*

`**function_name**`

Required*

`**plugin_name**`

Required*

parse_content

Parse the content of the function id block and extract the plugin and function

name.

If both are present in the fields, return the fields as is. Otherwise, use the
regex to

extract the plugin and function name.

render

Render the function id block.

**parse_content**

Parse the content of the function id block and extract the plugin and function
name.

If both are present in the fields, return the fields as is. Otherwise, use the
regex to

extract the plugin and function name.

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

**Methods**

ﾉ

**Expand table**

`parse_content(fields: Any) -> dict[str, Any]`

ﾉ

**Expand table**



**render**

Render the function id block.

Python

**function_name**

Python

**plugin_name**

Python

**type**

Python

`render(*_: Kernel | KernelArguments | ``None``) -> str`

**Attributes**

`function_name: str`

`plugin_name: str | ``None`

`type: ClassVar[BlockTypes] = 6`



**named_arg_block Module**

Reference

**Classes**

NamedArgBlock

Create a named argument block.

A named arg block is used to add arguments to a function call. It needs to be

combined with a function_id block to be useful. Inside a code block, if the
first

block is a function_id block, the first block can be a variable or value
block,

anything else must be a named arg block.

The value inside the NamedArgBlock can be a ValBlock or a VarBlock.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot

be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**NamedArgBlock Class**

Reference

Create a named argument block.

A named arg block is used to add arguments to a function call. It needs to be
combined

with a function_id block to be useful. Inside a code block, if the first block
is a

function_id block, the first block can be a variable or value block, anything
else must be

a named arg block.

The value inside the NamedArgBlock can be a ValBlock or a VarBlock.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**str**`

Required*

<xref:<xref:name ->>

The content of the named argument block, the name and value separated by an

equal sign, for instance arg1=$var.

`**str**`

Required*

The name of the argument.

`**ValBlock**`

Required*

<xref:<xref:value ->>

The value of the argument.

`**VarBlock**`

<xref:<xref:variable ->>

`NamedArgBlock(*, content: str, name: str | ``None`` = ``None``, value: ValBlock | `

`None`` = ``None``, variable: VarBlock | ``None`` = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

Required*

The variable of the argument. Either the value or variable field is used.

**Keyword-Only Parameters**

**Name**

**Description**

`**content**`

Required*

`**name**`

Required*

`**value**`

Required*

`**variable**`

Required*

**Examples**

{{ plugin.function arg1=$var }} {{ plugin.function arg1='value' }} {{
plugin.function 'value'

arg2=$var }} {{ plugin.function $var arg2='value' }} {{ plugin_function
arg1=$var1

arg2=$var2 arg3='value' }}

parse_content

Parse the content of the named argument block and extract the name and value.

If the name and either value or variable is present the parsing is skipped.

Otherwise, the content is parsed using a regex to extract the name and value.

Those are then turned into Blocks.

render

Render the named argument block.

**parse_content**

Parse the content of the named argument block and extract the name and value.

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



If the name and either value or variable is present the parsing is skipped.
Otherwise,

the content is parsed using a regex to extract the name and value. Those are
then

turned into Blocks.

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

**Exceptions**

**Type**

**Description**

NamedArgBlockSyntaxError

If the content does not match the named argument syntax.

**render**

Render the named argument block.

Python

**Parameters**

**Name**

**Description**

`**kernel**`

Required*

`parse_content(fields: Any) -> Any`

ﾉ

**Expand table**

ﾉ

**Expand table**

`render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

`**arguments**`

Required*

Default value: None

**Exceptions**

**Type**

**Description**

NamedArgBlockSyntaxError

If the content does not match the named argument syntax.

**name**

Python

**type**

Python

**value**

Python

**variable**

Python

ﾉ

**Expand table**

**Attributes**

`name: str | ``None`

`type: ClassVar[BlockTypes] = 7`

`value: ValBlock | ``None`

`variable: VarBlock | ``None`



**symbols Module**

Reference

**Enums**

Symbols

Symbols used in the template engine.

ﾉ

**Expand table**



**Symbols Enum**

Reference

Symbols used in the template engine.

BLOCK_ENDER

BLOCK_STARTER

CARRIAGE_RETURN

DBL_QUOTE

ESCAPE_CHAR

NAMED_ARG_BLOCK_SEPARATOR

NEW_LINE

SGL_QUOTE

SPACE

TAB

VAR_PREFIX

**Fields**

ﾉ

**Expand table**



**text_block Module**

Reference

**Classes**

TextBlock

A block with text content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**TextBlock Class**

Reference

A block with text content.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Keyword-Only Parameters**

**Name**

**Description**

`**content**`

Required*

content_strip

Strip the content of the text block.

Overload strip method, text blocks are not stripped.

from_text

Create a text block from a string.

render

Render the text block.

**content_strip**

`TextBlock(*, content: str)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Strip the content of the text block.

Overload strip method, text blocks are not stripped.

Python

**Parameters**

**Name**

**Description**

`**content**`

Required*

**from_text**

Create a text block from a string.

Python

**Parameters**

**Name**

**Description**

`**text**`

Default value: None

`**start_index**`

Default value: None

`**stop_index**`

Default value: None

**render**

Render the text block.

Python

`content_strip(content: str)`

ﾉ

**Expand table**

`from_text(text: str | ``None`` = ``None``, start_index: int | ``None`` = ``None``, `

`stop_index: int | ``None`` = ``None``)`

ﾉ

**Expand table**



**type**

Python

`render(*_: tuple[Kernel | ``None``, KernelArguments | ``None``]) -> str`

**Attributes**

`type: ClassVar[BlockTypes] = 2`



**val_block Module**

Reference

**Classes**

ValBlock

Create a value block.

A value block is used to represent a value in a template. It can be used to
represent any

characters. It needs to start and end with the same quote character, can be
both single

or double quotes, as long as they are not mixed.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**ValBlock Class**

Reference

Create a value block.

A value block is used to represent a value in a template. It can be used to
represent any

characters. It needs to start and end with the same quote character, can be
both single

or double quotes, as long as they are not mixed.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**str**`

Required*

<xref:<xref:quote ->>

The content of the value block.

`**str**`

Required*

The value of the block.

`**str**`

Required*

The quote used to wrap the value.

**Keyword-Only Parameters**

`ValBlock(*, content: str, value: str | ``None`` = ``''``, quote: str | ``None`` = ``"'"``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

`**content**`

Required*

`**value**`

Required*

`**quote**`

Default value: '

**Examples**

'value' "value" 'value with "quotes"' "value with 'quotes'"

parse_content

Parse the content and extract the value and quote.

The parsing is based on a regex that returns the value and quote. if the
'value' is

already present then the parsing is skipped.

render

Render the value block.

**parse_content**

Parse the content and extract the value and quote.

The parsing is based on a regex that returns the value and quote. if the
'value' is

already present then the parsing is skipped.

Python

**Parameters**

**Name**

**Description**

`**fields**`

**Methods**

ﾉ

**Expand table**

`parse_content(fields: Any) -> Any`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**render**

Render the value block.

Python

**quote**

Python

**type**

Python

**value**

Python

`render(*_: Kernel | KernelArguments | ``None``) -> str`

**Attributes**

`quote: str | ``None`

`type: ClassVar[BlockTypes] = 5`

`value: str | ``None`



**var_block Module**

Reference

**Classes**

VarBlock

Create a variable block.

A variable block is used to add a variable to a template. It gets rendered
from

KernelArguments, if the variable is not found a warning is logged and an empty
string

is returned. The variable must start with $ and be followed by a valid
variable name. A

valid variable name is a string of letters, numbers and underscores.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

ﾉ

**Expand table**



**VarBlock Class**

Reference

Create a variable block.

A variable block is used to add a variable to a template. It gets rendered
from

KernelArguments, if the variable is not found a warning is logged and an empty
string is

returned. The variable must start with $ and be followed by a valid variable
name. A

valid variable name is a string of letters, numbers and underscores.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**str**`

Required*

<xref:<xref:name ->>

The content of the variable block, the name of the variable.

`**str**`

Required*

The name of the variable.

**Keyword-Only Parameters**

**Name**

**Description**

`**content**`

`VarBlock(*, content: str, name: str = ``''``)`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Name**

**Description**

Required*

`**name**`

Required*

**Examples**

$var $test_var

parse_content

Parse the content and extract the name.

The parsing is based on a regex that returns the name. if the 'name' is
already

present then the parsing is skipped.

render

Render the variable block with the given arguments.

If the variable is not found in the arguments, return an empty string.

**parse_content**

Parse the content and extract the name.

The parsing is based on a regex that returns the name. if the 'name' is
already

present then the parsing is skipped.

Python

**Parameters**

**Name**

**Description**

`**fields**`

Required*

**Methods**

ﾉ

**Expand table**

`parse_content(fields: Any) -> Any`

ﾉ

**Expand table**



**render**

Render the variable block with the given arguments.

If the variable is not found in the arguments, return an empty string.

Python

**Parameters**

**Name**

**Description**

`**_**`

Required*

`**arguments**`

Required*

Default value: None

**name**

Python

**type**

Python

`render(_: Kernel, arguments: KernelArguments | ``None`` = ``None``) -> str`

ﾉ

**Expand table**

**Attributes**

`name: str`

`type: ClassVar[BlockTypes] = 4`



**protocols Package**

Reference

**Modules**

code_renderer

text_renderer

ﾉ

**Expand table**



**code_renderer Module**

Reference

**Classes**

CodeRenderer

Protocol for dynamic code blocks that need async IO to be rendered.

ﾉ

**Expand table**



**CodeRenderer Class**

Reference

Protocol for dynamic code blocks that need async IO to be rendered.

**Constructor**

Python

render_code

Render the block using the given context.

**render_code**

Render the block using the given context.

Python

**Parameters**

**Name**

**Description**

`**context**`

Required*

kernel execution context

`**kernel**`

Required*

`**arguments**`

Required*

`CodeRenderer(*args, **kwargs)`

**Methods**

ﾉ

**Expand table**

`abstract ``async`` render_code(kernel: Kernel, arguments: KernelArguments) ->
`

`str`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

Rendered content

ﾉ

**Expand table**



**text_renderer Module**

Reference

**Classes**

TextRenderer

Protocol for static (text) blocks that don't need async rendering.

ﾉ

**Expand table**



**TextRenderer Class**

Reference

Protocol for static (text) blocks that don't need async rendering.

**Constructor**

Python

render

Render the block using only the given variables.

**render**

Render the block using only the given variables.

Python

**Parameters**

**Name**

**Description**

`**variables**`

Required*

Optional variables used to render the block

`**kernel**`

Required*

`**arguments**`

Required*

Default value: None

`TextRenderer(*args, **kwargs)`

**Methods**

ﾉ

**Expand table**

`abstract render(kernel: Kernel, arguments: KernelArguments | ``None`` = ``None``) `

`-> str`

ﾉ

**Expand table**



**Returns**

**Type**

**Description**

Rendered content

ﾉ

**Expand table**



**code_tokenizer Module**

Reference

**Classes**

CodeTokenizer

Tokenize the code text into blocks.

ﾉ

**Expand table**



**CodeTokenizer Class**

Reference

Tokenize the code text into blocks.

**Constructor**

Python

tokenize

Tokenize the code text into blocks.

**tokenize**

Tokenize the code text into blocks.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`CodeTokenizer()`

**Methods**

ﾉ

**Expand table**

`static tokenize(text: str) -> list[Block]`

ﾉ

**Expand table**



**template_tokenizer Module**

Reference

**Classes**

TemplateTokenizer

Tokenize the template text into blocks.

ﾉ

**Expand table**



**TemplateTokenizer Class**

Reference

Tokenize the template text into blocks.

**Constructor**

Python

tokenize

Tokenize the template text into blocks.

**tokenize**

Tokenize the template text into blocks.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`TemplateTokenizer()`

**Methods**

ﾉ

**Expand table**

`static tokenize(text: str) -> list[Block]`

ﾉ

**Expand table**



**text Package**

Reference

**Modules**

function_extension

text_chunker

A Text splitter.

Split text in chunks, attempting to leave meaning intact. For plain text,
split

looking at new lines first, then periods, and so on. For markdown, split

looking at punctuation first, and so on.

**aggregate_chunked_results**

Aggregate the results from the chunked results.

Python

**Parameters**

**Name**

**Description**

`**func**`

Required*

`**chunked_results**`

Required*

`**kernel**`

Required*

`**arguments**`

ﾉ

**Expand table**

**Functions**

`async`` aggregate_chunked_results(func: KernelFunction, chunked_results: `

`list[str], kernel: Kernel, arguments: KernelArguments) -> str`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**split_markdown_lines**

Split markdown into lines.

It will split on punctuation first, and then on space and new lines.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_token_per_line**`

Required*

`**token_counter**`

Required*

**split_markdown_paragraph**

Split markdown into paragraphs.

Python

**Parameters**

`split_markdown_lines(text: str, max_token_per_line: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**

`split_markdown_paragraph(text: list[str], max_tokens: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**



**Name**

**Description**

`**text**`

Required*

`**max_tokens**`

Required*

`**token_counter**`

Required*

**split_plaintext_lines**

Split plain text into lines.

it will split on new lines first, and then on punctuation.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_token_per_line**`

Required*

`**token_counter**`

Required*

**split_plaintext_paragraph**

Split plain text into paragraphs.

Python

`split_plaintext_lines(text: str, max_token_per_line: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**

`split_plaintext_paragraph(text: list[str], max_tokens: int, `

`token_counter: ~collections.abc.Callable = <function _token_counter>) -> `



**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_tokens**`

Required*

`**token_counter**`

Required*

`list[str]`

ﾉ

**Expand table**



**function_extension Module**

Reference

**aggregate_chunked_results**

Aggregate the results from the chunked results.

Python

**Parameters**

**Name**

**Description**

`**func**`

Required*

`**chunked_results**`

Required*

`**kernel**`

Required*

`**arguments**`

Required*

**Functions**

`async`` aggregate_chunked_results(func: KernelFunction, chunked_results: `

`list[str], kernel: Kernel, arguments: KernelArguments) -> str`

ﾉ

**Expand table**



**text_chunker Module**

Reference

A Text splitter.

Split text in chunks, attempting to leave meaning intact. For plain text,
split looking at

new lines first, then periods, and so on. For markdown, split looking at
punctuation first,

and so on.

**split_markdown_lines**

Split markdown into lines.

It will split on punctuation first, and then on space and new lines.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_token_per_line**`

Required*

`**token_counter**`

Required*

**split_markdown_paragraph**

Split markdown into paragraphs.

**Functions**

`split_markdown_lines(text: str, max_token_per_line: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_tokens**`

Required*

`**token_counter**`

Required*

**split_plaintext_lines**

Split plain text into lines.

it will split on new lines first, and then on punctuation.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_token_per_line**`

Required*

`**token_counter**`

`split_markdown_paragraph(text: list[str], max_tokens: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**

`split_plaintext_lines(text: str, max_token_per_line: int, token_counter: `

`~collections.abc.Callable = <function _token_counter>) -> list[str]`

ﾉ

**Expand table**



**Name**

**Description**

Required*

**split_plaintext_paragraph**

Split plain text into paragraphs.

Python

**Parameters**

**Name**

**Description**

`**text**`

Required*

`**max_tokens**`

Required*

`**token_counter**`

Required*

`split_plaintext_paragraph(text: list[str], max_tokens: int, `

`token_counter: ~collections.abc.Callable = <function _token_counter>) -> `

`list[str]`

ﾉ

**Expand table**



**utils Package**

Reference

**Packages**

authentication

telemetry

**Modules**

chat

feature_stage_decorator

list_handler

logging

naming

validation

ﾉ

**Expand table**

ﾉ

**Expand table**



**authentication Package**

Reference

**Modules**

async_default_azure_credential_wrapper

entra_id_authentication

**get_entra_auth_token**

Retrieve a Microsoft Entra Auth Token for a given token endpoint.

The token endpoint may be specified as an environment variable, via the .env
file or

as an argument. If the token endpoint is not provided, the default is None.

Python

**Parameters**

**Name**

**Description**

`**token_endpoint**`

Required*

The token endpoint to use to retrieve the authentication token.

**Returns**

ﾉ

**Expand table**

**Functions**

`get_entra_auth_token(token_endpoint: str) -> str | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The Azure token or None if the token could not be retrieved.



**async_default_azure_credential_wrapper**

**Module**

Reference

**Classes**

AsyncDefaultAzureCredentialWrapper

Wrapper to make sure the async version of the

DefaultAzureCredential is closed properly.

ﾉ

**Expand table**



**AsyncDefaultAzureCredentialWrapper**

**Class**

Reference

Wrapper to make sure the async version of the DefaultAzureCredential is closed

properly.

**Constructor**

Python

`AsyncDefaultAzureCredentialWrapper(**kwargs: Any)`



**entra_id_authentication Module**

Reference

**get_entra_auth_token**

Retrieve a Microsoft Entra Auth Token for a given token endpoint.

The token endpoint may be specified as an environment variable, via the .env
file or

as an argument. If the token endpoint is not provided, the default is None.

Python

**Parameters**

**Name**

**Description**

`**token_endpoint**`

Required*

The token endpoint to use to retrieve the authentication token.

**Returns**

**Type**

**Description**

The Azure token or None if the token could not be retrieved.

**Functions**

`get_entra_auth_token(token_endpoint: str) -> str | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**telemetry Package**

Reference

**Packages**

agent_diagnostics

model_diagnostics

**Modules**

user_agent

ﾉ

**Expand table**

ﾉ

**Expand table**



**agent_diagnostics Package**

Reference

**Modules**

decorators

gen_ai_attributes

ﾉ

**Expand table**



**decorators Module**

Reference

**trace_agent_get_response**

Decorator to trace agent invocation.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**get_response_func**`

Required*

**trace_agent_invocation**

Decorator to trace agent invocation.

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Functions**

`trace_agent_get_response(get_response_func: Callable) -> Callable`

ﾉ

**Expand table**

`trace_agent_invocation(invoke_func: Callable) -> Callable`

ﾉ

**Expand table**



**Name**

**Description**

`**invoke_func**`

Required*



**gen_ai_attributes Module**

Reference



**model_diagnostics Package**

Reference

**Modules**

decorators

gen_ai_attributes

model_diagnostics_settings

**trace_chat_completion**

Decorator to trace chat completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

ﾉ

**Expand table**

**Functions**

`trace_chat_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**



**trace_streaming_chat_completion**

Decorator to trace streaming chat completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

**trace_streaming_text_completion**

Decorator to trace streaming text completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

`trace_streaming_chat_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**

`trace_streaming_text_completion(model_provider: str) -> Callable`



**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

**trace_text_completion**

Decorator to trace text completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

ﾉ

**Expand table**

`trace_text_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**



**decorators Module**

Reference

**Classes**

ChatHistoryMessageTimestampFilter

A filter to increment the timestamp of INFO logs by 1

microsecond.

Initialize a filter.

Initialize with the name of the logger which, together with

its children, will have its events allowed through the filter.

If no name is specified, allow every event.

**are_model_diagnostics_enabled**

Check if model diagnostics are enabled.

Model diagnostics are enabled if either diagnostic is enabled or diagnostic
with

sensitive events is enabled.

Note: This function is marked as 'experimental' and may change in the future.

Python

**are_sensitive_events_enabled**

Check if sensitive events are enabled.

Sensitive events are enabled if the diagnostic with sensitive events is
enabled.

Note: This function is marked as 'experimental' and may change in the future.

Python

ﾉ

**Expand table**

**Functions**

`are_model_diagnostics_enabled() -> bool`



**trace_chat_completion**

Decorator to trace chat completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

**trace_streaming_chat_completion**

Decorator to trace streaming chat completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

`are_sensitive_events_enabled() -> bool`

`trace_chat_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

**trace_streaming_text_completion**

Decorator to trace streaming text completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

**trace_text_completion**

`trace_streaming_chat_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**

`trace_streaming_text_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**



Decorator to trace text completion activities.

Args: model_provider (str): The model provider should describe a family of
GenAI

models with specific model identified by ai_model_id. For example,
model_provider

could be "openai" and ai_model_id could be "gpt-3.5-turbo". Sometimes the
model

provider is unknown at runtime, in which case it can be set to the most
specific

known provider. For example, while using local models hosted by Ollama, the
model

provider could be set to "ollama".

Note: This function is marked as 'experimental' and may change in the future.

Python

**Parameters**

**Name**

**Description**

`**model_provider**`

Required*

`trace_text_completion(model_provider: str) -> Callable`

ﾉ

**Expand table**



**ChatHistoryMessageTimestampFilter**

**Class**

Reference

A filter to increment the timestamp of INFO logs by 1 microsecond.

Initialize a filter.

Initialize with the name of the logger which, together with its children, will
have its

events allowed through the filter. If no name is specified, allow every event.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**name**`

filter

Increment the timestamp of INFO logs by 1 microsecond.

**filter**

Increment the timestamp of INFO logs by 1 microsecond.

Python

`ChatHistoryMessageTimestampFilter(name=``''``)`

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**

`filter(record: LogRecord) -> bool`



**Parameters**

**Name**

**Description**

`**record**`

Required*

**INDEX_KEY**

Python

ﾉ

**Expand table**

**Attributes**

`INDEX_KEY: ClassVar[str] = ``'CHAT_MESSAGE_INDEX'`



**gen_ai_attributes Module**

Reference



**model_diagnostics_settings Module**

Reference

**Classes**

ModelDiagnosticSettings

Settings for model diagnostics.

The settings are first loaded from environment variables with the prefix

'>>SEMANTICKERNEL_EXPERIMENTAL_GENAI_<<'. If the environment variables are not

found, the settings can be loaded from a .env file with the encoding 'utf-8'.
If the settings

are not found in the .env file, the settings are ignored; however, validation
will fail alerting

that the settings are missing.

Required settings for prefix '>>SEMANTICKERNEL_EXPERIMENTAL_GENAI_<<' are:

enable_otel_diagnostics: bool - Enable OpenTelemetry diagnostics. Default is
False.

(Env var SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS)

enable_otel_diagnostics_sensitive: bool - Enable OpenTelemetry sensitive
events.

Default is False.

(Env var

SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIVE)

Note: This class is marked as 'experimental' and may change in the future.

ﾉ

**Expand table**



**ModelDiagnosticSettings Class**

Reference

Settings for model diagnostics.

The settings are first loaded from environment variables with the prefix

'>>SEMANTICKERNEL_EXPERIMENTAL_GENAI_<<'. If the environment variables are not

found, the settings can be loaded from a .env file with the encoding 'utf-8'.
If the

settings are not found in the .env file, the settings are ignored; however,
validation will

fail alerting that the settings are missing.

Required settings for prefix '>>SEMANTICKERNEL_EXPERIMENTAL_GENAI_<<' are:

enable_otel_diagnostics: bool - Enable OpenTelemetry diagnostics. Default is
False.

(Env var SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS)

enable_otel_diagnostics_sensitive: bool - Enable OpenTelemetry sensitive
events.

Default is False.

(Env var

SEMANTICKERNEL_EXPERIMENTAL_GENAI_ENABLE_OTEL_DIAGNOSTICS_SENSITIV

E)

Note: This class is marked as 'experimental' and may change in the future.

**Constructor**

Python

`ModelDiagnosticSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``, `



**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

`enable_otel_diagnostics: bool = ``False``, enable_otel_diagnostics_sensitive:
`

`bool = ``False``)`

ﾉ

**Expand table**



**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

`**env_file_encoding**`

Default value: utf-8

`**enable_otel_diagnostics**`

Required*

`**enable_otel_diagnostics_sensitive**`

Required*

**enable_otel_diagnostics**

Python

**enable_otel_diagnostics_sensitive**

Python

**env_prefix**

Python

**is_experimental**

Python

ﾉ

**Expand table**

**Attributes**

`enable_otel_diagnostics: bool`

`enable_otel_diagnostics_sensitive: bool`

`env_prefix: ClassVar[str] = ``'SEMANTICKERNEL_EXPERIMENTAL_GENAI_'`



**stage_status**

Python

`is_experimental = ``True`

`stage_status = ``'experimental'`



**user_agent Module**

Reference

**prepend_semantic_kernel_to_user_agent**

Prepend "semantic-kernel" to the User-Agent in the headers.

Python

**Parameters**

**Name**

**Description**

`**headers**`

Required*

The existing headers dictionary.

**Returns**

**Type**

**Description**

The modified headers dictionary with "semantic-kernel-python/{version}"
prepended

to the User-Agent.

**Functions**

`prepend_semantic_kernel_to_user_agent(headers: dict[str, Any])`

ﾉ

**Expand table**

ﾉ

**Expand table**



**chat Module**

Reference

**store_results**

Stores specific results in the context and chat prompt.

Python

**Parameters**

**Name**

**Description**

`**chat_history**`

Required*

`**results**`

Required*

**Functions**

`store_results(chat_history: ChatHistory, results: `

`list[ChatMessageContent])`

ﾉ

**Expand table**



**feature_stage_decorator Module**

Reference

**experimental**

Decorator specifically for 'experimental' features.

It uses the general 'stage' decorator but also attaches 'is_experimental =
True'.

Python

**Parameters**

**Name**

**Description**

`**obj**`

Required*

**release_candidate**

Decorator that designates a function/class as being in a 'release candidate'
state.

By default, applies a descriptive note indicating near-completion and possible
minor

refinements before achieving general availability. You may override this with
a

custom 'doc_string' if needed.

Usage:

1\. @release_candidate

2\. @release_candidate()

3\. @release_candidate("1.21.3-rc1")

4\. @release_candidate(version="1.21.3-rc1")

**Functions**

`experimental(obj: T) -> T`

ﾉ

**Expand table**



5\. @release_candidate(doc_string="Custom RC note...")

6\. @release_candidate(version="1.21.3-rc1", doc_string="Custom RC note...")

Python

**Parameters**

**Name**

**Description**

`**func**`

In cases (1) or (2), this is the function/class being decorated.

In cases (3) or (4), this may be a version string or None.

Default value: None

`**version**`

Required*

The RC version string, if provided.

Default value: None

`**doc_string**`

Required*

An optional custom note to append to the docstring, overriding the default RC

descriptive note.

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**version**`

Required*

`**doc_string**`

Required*

**Returns**

`release_candidate(func: T | str | ``None`` = ``None``, *, version: str | ``None`` = `

`None``, doc_string: str | ``None`` = ``None``) -> T`

ﾉ

**Expand table**

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

The decorated object, with an updated docstring and 'is_release_candidate =
True'.

**stage**

A general-purpose decorator for marking a function or a class.

It updates the docstring and attaches 'stage_status' (and optionally
'stage_version')

as metadata. A custom 'note' may be provided to override the default appended

text.

Python

**Parameters**

**Name**

**Description**

`**status**`

The development stage (e.g., 'experimental', 'release_candidate', etc.).

Default value: experimental

`**version**`

Optional version or release info (e.g., '1.21.0-rc4').

Default value: None

`**note**`

A custom note to append to the docstring. If omitted, a default note is used
to

indicate the stage and possible changes.

Default value: None

**Returns**

**Type**

**Description**

A decorator that updates the docstring and metadata of the target
function/class.

**DEFAULT_RC_NOTE**

`stage(status: str = ``'experimental'``, version: str | ``None`` = ``None``, note: str `

`| ``None`` = ``None``) -> Callable[[T], T]`

ﾉ

**Expand table**

ﾉ

**Expand table**



Example usage:

@experimental class MyExperimentalClass:

'''A class that is still evolving rapidly.''' pass

@stage(status="experimental") class MyExperimentalClass:

'''A class that is still evolving rapidly.''' pass

@experimental def my_experimental_function():

'''A function that is still evolving rapidly.''' pass

@release_candidate class MyRCClass:

'''A class that is nearly final, but still in release-candidate stage.''' pass

@release_candidate("1.23.1-rc1") class MyRCClass:

'''A class that is nearly final, but still in release-candidate stage.''' pass

Python

`DEFAULT_RC_NOTE = ``'Features marked with this status are nearing `

`completion and are considered stable for most purposes, but may still `

`incur minor refinements or optimizations before achieving full general `

`availability.'`



**list_handler Module**

Reference

**desync_list**

De synchronize a list of synchronous objects.

Python

**Parameters**

**Name**

**Description**

`**sync_list**`

Required*

**empty_generator**

An empty generator, can be used to return an empty generator.

Python

**Functions**

`async`` desync_list(sync_list: Sequence[_T]) -> AsyncIterable[_T]`

ﾉ

**Expand table**

`async`` empty_generator() -> AsyncGenerator[_T, ``None``]`



**logging Module**

Reference

**setup_logging**

Setup a detailed logging format.

Python

**Functions**

`setup_logging()`



**naming Module**

Reference

**generate_random_ascii_name**

Generate a series of random ASCII characters of the specified length.

As example, plugin/function names can contain upper/lowercase letters, and

underscores

Python

**Parameters**

**Name**

**Description**

`**length**`

int

The length of the string to generate.

Default value: 16

**Returns**

**Type**

**Description**

A string of random ASCII characters of the specified length.

**Functions**

`generate_random_ascii_name(length: int = 16) -> str`

ﾉ

**Expand table**

ﾉ

**Expand table**



**validation Module**

Reference



**const Module**

Reference



**kernel Module**

Reference

**Classes**

Kernel

The Kernel of Semantic Kernel.

This is the main entry point for Semantic Kernel. It provides the ability to
run functions

and manage filters, plugins, and AI services.

Initialize a new instance of the Kernel class.

ﾉ

**Expand table**



**Kernel Class**

Reference

The Kernel of Semantic Kernel.

This is the main entry point for Semantic Kernel. It provides the ability to
run functions

and manage filters, plugins, and AI services.

Initialize a new instance of the Kernel class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**plugins**`

The plugins to be used by the kernel, will be rewritten to a dict with

plugin name as key

Default value: None

`**services**`

The services to be used by the kernel, will be rewritten to a dict with

service_id as key

Default value: None

`**ai_service_selector**`

The AI service selector to be used by the kernel, default is based on

order of execution settings.

Default value: None

`Kernel(plugins: KernelPlugin | dict[str, KernelPlugin] | list[KernelPlugin] `

`| ``None`` = ``None``, services: AI_SERVICE_CLIENT_TYPE | `

`list[AI_SERVICE_CLIENT_TYPE] | dict[str, AI_SERVICE_CLIENT_TYPE] | ``None`` = `

`None``, ai_service_selector: AIServiceSelector | ``None`` = ``None``, *, `

`retry_mechanism: RetryMechanismBase = ``None``, function_invocation_filters:
`

`list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]], Awaitable[``None``]]]]
= `

`None``, prompt_rendering_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``, `

`auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

Additional fields to be passed to the Kernel model, these are limited to

filters.

**Keyword-Only Parameters**

**Name**

**Description**

`**retry_mechanism**`

Required*

`**function_invocation_filters**`

Required*

`**prompt_rendering_filters**`

Required*

`**auto_function_invocation_filters**`

Required*

add_embedding_to_object

Gather all fields to embed, batch the embedding generation and

store.

invoke

Execute a function and return the FunctionResult.

invoke_function_call

Processes the provided FunctionCallContent and updates the chat

history.

invoke_prompt

Invoke a function from the provided prompt.

invoke_prompt_stream

Invoke a function from the provided prompt and stream the results.

invoke_stream

Execute one or more stream functions.

This will execute the functions in the order they are provided, if a list

of functions is provided. When multiple functions are provided only

the last one is streamed, the rest is executed as a pipeline.

**add_embedding_to_object**

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Gather all fields to embed, batch the embedding generation and store.

Python

**Parameters**

**Name**

**Description**

`**inputs**`

Required*

`**field_to_embed**`

Required*

`**field_to_store**`

Required*

`**execution_settings**`

Required*

`**container_mode**`

Required*

Default value: False

`**cast_function**`

Required*

Default value: None

**invoke**

Execute a function and return the FunctionResult.

Python

**Parameters**

`async`` add_embedding_to_object(inputs: TDataModel | Sequence[TDataModel], `

`field_to_embed: str, field_to_store: str, execution_settings: dict[str, `

`PromptExecutionSettings], container_mode: bool = ``False``, cast_function: `

`Callable[[list[float]], Any] | ``None`` = ``None``, **kwargs: Any)`

ﾉ

**Expand table**

`async`` invoke(function: KernelFunction | ``None`` = ``None``, arguments: `

`KernelArguments | ``None`` = ``None``, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, metadata: dict[str, Any] = {}, **kwargs: `

`Any) -> FunctionResult | ``None`



**Name**

**Description**

`**function**`

<xref:semantic_kernel.kernel.KernelFunction>

The function or functions to execute, this value has precedence when

supplying both this and using function_name and plugin_name, if this is

none, function_name and plugin_name are used and cannot be None.

Default value: None

`**arguments**`

<xref:semantic_kernel.kernel.KernelArguments>

The arguments to pass to the function(s), optional

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.kernel.str | None>>

The name of the function to execute

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.kernel.str | None>>

The name of the plugin to execute

Default value: None

`**metadata**`

dict[str,<xref: Any>]

The metadata to pass to the function(s)

Default value: {}

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying KernelArguments

Default value: None

**Exceptions**

**Type**

**Description**

KernelInvokeException

If an error occurs during function invocation

**invoke_function_call**

Processes the provided FunctionCallContent and updates the chat history.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_function_call(function_call: FunctionCallContent, `

`chat_history: ChatHistory, *, arguments: KernelArguments | ``None`` = ``None``, `

`execution_settings: PromptExecutionSettings | ``None`` = ``None``, `

`function_call_count: int | ``None`` = ``None``, request_index: int | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**function_call**`

Required*

`**chat_history**`

Required*

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**execution_settings**`

Required*

`**function_call_count**`

Required*

`**request_index**`

Required*

`**is_streaming**`

Required*

`**function_behavior**`

Required*

**invoke_prompt**

Invoke a function from the provided prompt.

Python

`is_streaming: bool = ``False``, function_behavior: FunctionChoiceBehavior = `

`None``) -> AutoFunctionInvocationContext | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to use

`**function_name**`

Required*

str

The name of the function, optional

Default value: None

`**plugin_name**`

Required*

str

The name of the plugin, optional

Default value: None

`**arguments**`

Required*

<xref:<xref:semantic_kernel.kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**template_format**`

Required*

<xref:<xref:semantic_kernel.kernel.str | None>>

The format of the prompt template

Default value: semantic-kernel

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying KernelArguments

**Returns**

**Type**

**Description**

FunctionResult | list[FunctionResult] | None

The result of the function(s)

**invoke_prompt_stream**

Invoke a function from the provided prompt and stream the results.

`async`` invoke_prompt(prompt: str, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``] = `

`'semantic-kernel'``, **kwargs: Any) -> FunctionResult | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to use

`**function_name**`

Required*

str

The name of the function, optional

Default value: None

`**plugin_name**`

Required*

str

The name of the plugin, optional

Default value: None

`**arguments**`

Required*

<xref:<xref:semantic_kernel.kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**template_format**`

Required*

<xref:<xref:semantic_kernel.kernel.str | None>>

The format of the prompt template

Default value: semantic-kernel

`**return_function_results**`

Required*

bool

If True, the function results are yielded as a list[FunctionResult]

Default value: False

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying

KernelArguments

**Returns**

`async`` invoke_prompt_stream(prompt: str, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``] = `

`'semantic-kernel'``, return_function_results: bool | ``None`` = ``False``, `

`**kwargs: Any) -> AsyncIterable[list[StreamingContentMixin] | `

`FunctionResult | list[FunctionResult]]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

AsyncIterable[StreamingContentMixi

n]

The content of the stream of the last function

provided.

**invoke_stream**

Execute one or more stream functions.

This will execute the functions in the order they are provided, if a list of
functions is

provided. When multiple functions are provided only the last one is streamed,
the

rest is executed as a pipeline.

Python

**Parameters**

**Name**

**Description**

`**function**`

<xref:semantic_kernel.kernel.KernelFunction>

The function to execute, this value has precedence when

supplying both this and using function_name and

plugin_name, if this is none, function_name and plugin_name

are used and cannot be None.

Default value: None

`**arguments**`

<xref:<xref:semantic_kernel.kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.kernel.str | None>>

The name of the function to execute

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.kernel.str | None>>

The name of the plugin to execute

Default value: None

`async`` invoke_stream(function: KernelFunction | ``None`` = ``None``, arguments: `

`KernelArguments | ``None`` = ``None``, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, metadata: dict[str, Any] = {}, `

`return_function_results: bool = ``False``, **kwargs: Any) -> `

`AsyncGenerator[list[StreamingContentMixin] | FunctionResult | `

`list[FunctionResult], Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**metadata**`

dict[str,<xref: Any>]

The metadata to pass to the function(s)

Default value: {}

`**return_function_results**`

bool

If True, the function results are yielded as a list[FunctionResult]

Default value: False

`**content**`

Required*

**content** (<xref:in addition to the streaming>)

Default value: None

`**yielded.**`

Required*

**yielded.** (<xref:otherwise only the streaming content is>)

Default value: None

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying

KernelArguments

Default value: None

**retry_mechanism**

Data descriptor used to emit a runtime deprecation warning before accessing a

deprecated field.

Python

**function_invocation_filters**

Filters applied during function invocation, from KernelFilterExtension.

Python

**prompt_rendering_filters**

**Attributes**

`retry_mechanism: RetryMechanismBase`

`function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`



Filters applied during prompt rendering, from KernelFilterExtension.

Python

**auto_function_invocation_filters**

Filters applied during auto function invocation, from KernelFilterExtension.

Python

**plugins**

A dict with the plugins registered with the Kernel, from
KernelFunctionExtension.

Python

**services**

A dict with the services registered with the Kernel, from
KernelServicesExtension.

Python

**ai_service_selector**

The AI service selector to be used by the kernel, from
KernelServicesExtension.

Python

`prompt_rendering_filters: list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]],
Awaitable[``None``]]]]`

`auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`

`plugins: dict[str, KernelPlugin]`

`services: dict[str, AIServiceClientBase]`

`ai_service_selector: AIServiceSelector`



**msg**

The deprecation message to be emitted.

**wrapped_property**

The property instance if the deprecated field is a computed field, or _None_.

**field_name**

The name of the field being deprecated.



**kernel_pydantic Module**

Reference

**Classes**

KernelBaseModel

Base class for all pydantic models in the SK.

Create a new model by parsing and validating input data from keyword

arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data

cannot be validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

KernelBaseSettings

Base class for all settings classes in the SK.

A subclass creates it's fields and overrides the env_prefix class variable
with

the prefix for the environment variables.

In the case where a value is specified for the same Settings field in multiple

ways, the selected value is determined as follows (in descending order of

priority):

Arguments passed to the Settings class initializer.

Environment variables, e.g. my_prefix_special_function as described

above.

Variables loaded from a dotenv (.env) file.

Variables loaded from the secrets directory.

The default field values for the Settings model.

ﾉ

**Expand table**



**KernelBaseModel Class**

Reference

Base class for all pydantic models in the SK.

Create a new model by parsing and validating input data from keyword
arguments.

Raises [_ValidationError_][pydantic_core.ValidationError] if the input data
cannot be

validated to form a valid model.

_self_ is explicitly positional-only to allow _self_ as a field name.

**Constructor**

Python

`KernelBaseModel()`



**KernelBaseSettings Class**

Reference

Base class for all settings classes in the SK.

A subclass creates it's fields and overrides the env_prefix class variable
with the prefix for

the environment variables.

In the case where a value is specified for the same Settings field in multiple
ways, the

selected value is determined as follows (in descending order of priority):

Arguments passed to the Settings class initializer.

Environment variables, e.g. my_prefix_special_function as described above.

Variables loaded from a dotenv (.env) file.

Variables loaded from the secrets directory.

The default field values for the Settings model.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**_case_sensitive**`

Default value: None

`KernelBaseSettings(_case_sensitive: bool | ``None`` = ``None``, `

`_nested_model_default_partial_update: bool | ``None`` = ``None``, _env_prefix: str | `

`None`` = ``None``, _env_file: DotenvType | ``None`` = PosixPath(``'.'``), `

`_env_file_encoding: str | ``None`` = ``None``, _env_ignore_empty: bool | ``None`` = `

`None``, _env_nested_delimiter: str | ``None`` = ``None``, _env_parse_none_str: str | `

`None`` = ``None``, _env_parse_enums: bool | ``None`` = ``None``, _cli_prog_name: str | `

`None`` = ``None``, _cli_parse_args: bool | list[str] | tuple[str, ...] | ``None`` = `

`None``, _cli_settings_source: CliSettingsSource[Any] | ``None`` = ``None``, `

`_cli_parse_none_str: str | ``None`` = ``None``, _cli_hide_none_type: bool | ``None`` = `

`None``, _cli_avoid_json: bool | ``None`` = ``None``, _cli_enforce_required: bool | `

`None`` = ``None``, _cli_use_class_docs_for_groups: bool | ``None`` = ``None``, `

`_cli_exit_on_error: bool | ``None`` = ``None``, _cli_prefix: str | ``None`` = ``None``, `

`_cli_implicit_flags: bool | ``None`` = ``None``, _secrets_dir: PathType | ``None`` = `

`None``, *, env_file_path: str | ``None`` = ``None``, env_file_encoding: str = ``'utf-8'``)`

ﾉ

**Expand table**



**Name**

**Description**

`**_nested_model_default_partial_update**`

Default value: None

`**_env_prefix**`

Default value: None

`**_env_file**`

Default value: .

`**_env_file_encoding**`

Default value: None

`**_env_ignore_empty**`

Default value: None

`**_env_nested_delimiter**`

Default value: None

`**_env_parse_none_str**`

Default value: None

`**_env_parse_enums**`

Default value: None

`**_cli_prog_name**`

Default value: None

`**_cli_parse_args**`

Default value: None

`**_cli_settings_source**`

Default value: None

`**_cli_parse_none_str**`

Default value: None

`**_cli_hide_none_type**`

Default value: None

`**_cli_avoid_json**`

Default value: None

`**_cli_enforce_required**`

Default value: None

`**_cli_use_class_docs_for_groups**`

Default value: None

`**_cli_exit_on_error**`

Default value: None

`**_cli_prefix**`

Default value: None

`**_cli_implicit_flags**`

Default value: None

`**_secrets_dir**`

Default value: None

**Keyword-Only Parameters**

**Name**

**Description**

`**env_file_path**`

Required*

ﾉ

**Expand table**



**Name**

**Description**

`**env_file_encoding**`

Default value: utf-8

create

Update the model_config with the prefix.

**create**

Update the model_config with the prefix.

Python

**env_file_encoding**

Python

**env_file_path**

Python

**env_prefix**

Python

**Methods**

ﾉ

**Expand table**

`create(**data: Any) -> T`

**Attributes**

`env_file_encoding: str`

`env_file_path: str | ``None`

`env_prefix: ClassVar[str] = ``''`



**kernel_types Module**

Reference



**Kernel Class**

Reference

The Kernel of Semantic Kernel.

This is the main entry point for Semantic Kernel. It provides the ability to
run functions

and manage filters, plugins, and AI services.

Initialize a new instance of the Kernel class.

**Constructor**

Python

**Parameters**

**Name**

**Description**

`**plugins**`

The plugins to be used by the kernel, will be rewritten to a dict with

plugin name as key

Default value: None

`**services**`

The services to be used by the kernel, will be rewritten to a dict with

service_id as key

Default value: None

`**ai_service_selector**`

The AI service selector to be used by the kernel, default is based on

order of execution settings.

Default value: None

`Kernel(plugins: KernelPlugin | dict[str, KernelPlugin] | list[KernelPlugin] `

`| ``None`` = ``None``, services: AI_SERVICE_CLIENT_TYPE | `

`list[AI_SERVICE_CLIENT_TYPE] | dict[str, AI_SERVICE_CLIENT_TYPE] | ``None`` = `

`None``, ai_service_selector: AIServiceSelector | ``None`` = ``None``, *, `

`retry_mechanism: RetryMechanismBase = ``None``, function_invocation_filters:
`

`list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]], Awaitable[``None``]]]]
= `

`None``, prompt_rendering_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``, `

`auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]] = ``None``)`

ﾉ

**Expand table**



**Name**

**Description**

`****kwargs**`

Required*

Additional fields to be passed to the Kernel model, these are limited to

filters.

**Keyword-Only Parameters**

**Name**

**Description**

`**retry_mechanism**`

Required*

`**function_invocation_filters**`

Required*

`**prompt_rendering_filters**`

Required*

`**auto_function_invocation_filters**`

Required*

add_embedding_to_object

Gather all fields to embed, batch the embedding generation and

store.

invoke

Execute a function and return the FunctionResult.

invoke_function_call

Processes the provided FunctionCallContent and updates the chat

history.

invoke_prompt

Invoke a function from the provided prompt.

invoke_prompt_stream

Invoke a function from the provided prompt and stream the results.

invoke_stream

Execute one or more stream functions.

This will execute the functions in the order they are provided, if a list

of functions is provided. When multiple functions are provided only

the last one is streamed, the rest is executed as a pipeline.

**add_embedding_to_object**

ﾉ

**Expand table**

**Methods**

ﾉ

**Expand table**



Gather all fields to embed, batch the embedding generation and store.

Python

**Parameters**

**Name**

**Description**

`**inputs**`

Required*

`**field_to_embed**`

Required*

`**field_to_store**`

Required*

`**execution_settings**`

Required*

`**container_mode**`

Required*

Default value: False

`**cast_function**`

Required*

Default value: None

**invoke**

Execute a function and return the FunctionResult.

Python

**Parameters**

`async`` add_embedding_to_object(inputs: TDataModel | Sequence[TDataModel], `

`field_to_embed: str, field_to_store: str, execution_settings: dict[str, `

`PromptExecutionSettings], container_mode: bool = ``False``, cast_function: `

`Callable[[list[float]], Any] | ``None`` = ``None``, **kwargs: Any)`

ﾉ

**Expand table**

`async`` invoke(function: KernelFunction | ``None`` = ``None``, arguments: `

`KernelArguments | ``None`` = ``None``, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, metadata: dict[str, Any] = {}, **kwargs: `

`Any) -> FunctionResult | ``None`



**Name**

**Description**

`**function**`

<xref:semantic_kernel.KernelFunction>

The function or functions to execute, this value has precedence when

supplying both this and using function_name and plugin_name, if this is

none, function_name and plugin_name are used and cannot be None.

Default value: None

`**arguments**`

<xref:semantic_kernel.KernelArguments>

The arguments to pass to the function(s), optional

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.str | None>>

The name of the function to execute

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.str | None>>

The name of the plugin to execute

Default value: None

`**metadata**`

dict[str,<xref: Any>]

The metadata to pass to the function(s)

Default value: {}

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying KernelArguments

Default value: None

**Exceptions**

**Type**

**Description**

KernelInvokeException

If an error occurs during function invocation

**invoke_function_call**

Processes the provided FunctionCallContent and updates the chat history.

Python

ﾉ

**Expand table**

ﾉ

**Expand table**

`async`` invoke_function_call(function_call: FunctionCallContent, `

`chat_history: ChatHistory, *, arguments: KernelArguments | ``None`` = ``None``, `

`execution_settings: PromptExecutionSettings | ``None`` = ``None``, `

`function_call_count: int | ``None`` = ``None``, request_index: int | ``None`` = ``None``, `



**Parameters**

**Name**

**Description**

`**function_call**`

Required*

`**chat_history**`

Required*

**Keyword-Only Parameters**

**Name**

**Description**

`**arguments**`

Required*

`**execution_settings**`

Required*

`**function_call_count**`

Required*

`**request_index**`

Required*

`**is_streaming**`

Required*

`**function_behavior**`

Required*

**invoke_prompt**

Invoke a function from the provided prompt.

Python

`is_streaming: bool = ``False``, function_behavior: FunctionChoiceBehavior = `

`None``) -> AutoFunctionInvocationContext | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to use

`**function_name**`

Required*

str

The name of the function, optional

Default value: None

`**plugin_name**`

Required*

str

The name of the plugin, optional

Default value: None

`**arguments**`

Required*

<xref:<xref:semantic_kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**template_format**`

Required*

<xref:<xref:semantic_kernel.str | None>>

The format of the prompt template

Default value: semantic-kernel

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying KernelArguments

**Returns**

**Type**

**Description**

FunctionResult | list[FunctionResult] | None

The result of the function(s)

**invoke_prompt_stream**

Invoke a function from the provided prompt and stream the results.

`async`` invoke_prompt(prompt: str, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``] = `

`'semantic-kernel'``, **kwargs: Any) -> FunctionResult | ``None`

ﾉ

**Expand table**

ﾉ

**Expand table**



Python

**Parameters**

**Name**

**Description**

`**prompt**`

Required*

str

The prompt to use

`**function_name**`

Required*

str

The name of the function, optional

Default value: None

`**plugin_name**`

Required*

str

The name of the plugin, optional

Default value: None

`**arguments**`

Required*

<xref:<xref:semantic_kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**template_format**`

Required*

<xref:<xref:semantic_kernel.str | None>>

The format of the prompt template

Default value: semantic-kernel

`**return_function_results**`

Required*

bool

If True, the function results are yielded as a list[FunctionResult]

Default value: False

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying

KernelArguments

**Returns**

`async`` invoke_prompt_stream(prompt: str, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, arguments: KernelArguments | ``None`` = ``None``, `

`template_format: Literal[``'semantic-kernel'``, ``'handlebars'``,
``'jinja2'``] = `

`'semantic-kernel'``, return_function_results: bool | ``None`` = ``False``, `

`**kwargs: Any) -> AsyncIterable[list[StreamingContentMixin] | `

`FunctionResult | list[FunctionResult]]`

ﾉ

**Expand table**

ﾉ

**Expand table**



**Type**

**Description**

AsyncIterable[StreamingContentMixi

n]

The content of the stream of the last function

provided.

**invoke_stream**

Execute one or more stream functions.

This will execute the functions in the order they are provided, if a list of
functions is

provided. When multiple functions are provided only the last one is streamed,
the

rest is executed as a pipeline.

Python

**Parameters**

**Name**

**Description**

`**function**`

<xref:semantic_kernel.KernelFunction>

The function to execute, this value has precedence when

supplying both this and using function_name and

plugin_name, if this is none, function_name and plugin_name

are used and cannot be None.

Default value: None

`**arguments**`

<xref:<xref:semantic_kernel.KernelArguments | None>>

The arguments to pass to the function(s), optional

Default value: None

`**function_name**`

<xref:<xref:semantic_kernel.str | None>>

The name of the function to execute

Default value: None

`**plugin_name**`

<xref:<xref:semantic_kernel.str | None>>

The name of the plugin to execute

Default value: None

`async`` invoke_stream(function: KernelFunction | ``None`` = ``None``, arguments: `

`KernelArguments | ``None`` = ``None``, function_name: str | ``None`` = ``None``, `

`plugin_name: str | ``None`` = ``None``, metadata: dict[str, Any] = {}, `

`return_function_results: bool = ``False``, **kwargs: Any) -> `

`AsyncGenerator[list[StreamingContentMixin] | FunctionResult | `

`list[FunctionResult], Any]`

ﾉ

**Expand table**



**Name**

**Description**

`**metadata**`

dict[str,<xref: Any>]

The metadata to pass to the function(s)

Default value: {}

`**return_function_results**`

bool

If True, the function results are yielded as a list[FunctionResult]

Default value: False

`**content**`

Required*

**content** (<xref:in addition to the streaming>)

Default value: None

`**yielded.**`

Required*

**yielded.** (<xref:otherwise only the streaming content is>)

Default value: None

`**kwargs**`

Required*

dict[str,<xref: Any>]

arguments that can be used instead of supplying

KernelArguments

Default value: None

**retry_mechanism**

Data descriptor used to emit a runtime deprecation warning before accessing a

deprecated field.

Python

**function_invocation_filters**

Filters applied during function invocation, from KernelFilterExtension.

Python

**prompt_rendering_filters**

**Attributes**

`retry_mechanism: RetryMechanismBase`

`function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`



Filters applied during prompt rendering, from KernelFilterExtension.

Python

**auto_function_invocation_filters**

Filters applied during auto function invocation, from KernelFilterExtension.

Python

**plugins**

A dict with the plugins registered with the Kernel, from
KernelFunctionExtension.

**services**

A dict with the services registered with the Kernel, from
KernelServicesExtension.

**ai_service_selector**

The AI service selector to be used by the kernel, from
KernelServicesExtension.

**msg**

The deprecation message to be emitted.

**wrapped_property**

The property instance if the deprecated field is a computed field, or _None_.

**field_name**

The name of the field being deprecated.

`prompt_rendering_filters: list[tuple[int, Callable[[FILTER_CONTEXT_TYPE, `

`Callable[[FILTER_CONTEXT_TYPE], Awaitable[``None``]]],
Awaitable[``None``]]]]`

`auto_function_invocation_filters: list[tuple[int, `

`Callable[[FILTER_CONTEXT_TYPE, Callable[[FILTER_CONTEXT_TYPE], `

`Awaitable[``None``]]], Awaitable[``None``]]]]`



